<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">


  <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({
      google_ad_client: "ca-pub-5653382914441020",
      enable_page_level_ads: true
    });
  </script>

  <meta name="google-site-verification" content="WSpJFvfcmIPBX_iK8uPbmCHIy_0f1lvd7ZJpbyad2sA">
  <meta name="yandex-verification" content="808eb057eb8396cc">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"kezunlin.me","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Solving in Python with LeNetIn this example, we’ll explore learning with Caffe in Python, using the fully-exposed Solver interface. Setup Set up the Python environment: we’ll use the pylab import for">
<meta property="og:type" content="article">
<meta property="og:title" content="learning lenet with caffe and python">
<meta property="og:url" content="https://kezunlin.me/blog/learning-lenet/index.html">
<meta property="og:site_name" content="Kezunlin&#39;s Blog">
<meta property="og:description" content="Solving in Python with LeNetIn this example, we’ll explore learning with Caffe in Python, using the fully-exposed Solver interface. Setup Set up the Python environment: we’ll use the pylab import for">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://kezunlin.me/images/posts/635233-20180808094007919-402598205.png">
<meta property="og:image" content="https://kezunlin.me/images/posts/635233-20180808094011687-911149908.png">
<meta property="og:image" content="https://kezunlin.me/images/posts/635233-20180808094015260-1011851543.png">
<meta property="og:image" content="https://kezunlin.me/images/posts/635233-20180808094018680-398130560.png">
<meta property="og:image" content="https://kezunlin.me/images/posts/635233-20180808094024721-1595689994.png">
<meta property="og:image" content="https://kezunlin.me/images/posts/635233-20180808094029552-416345653.png">
<meta property="og:image" content="https://kezunlin.me/images/posts/635233-20180808094034649-822966977.png">
<meta property="og:image" content="https://kezunlin.me/images/posts/635233-20180808094311497-1449576622.png">
<meta property="og:image" content="https://kezunlin.me/images/posts/635233-20180808094314887-1994925148.png">
<meta property="og:image" content="https://kezunlin.me/images/posts/635233-20180808094317536-26566385.png">
<meta property="og:image" content="https://kezunlin.me/images/posts/635233-20180808094335122-926095188.png">
<meta property="og:image" content="https://kezunlin.me/images/posts/635233-20180808094337980-697149050.png">
<meta property="og:image" content="https://kezunlin.me/images/posts/635233-20180808094432007-708264326.png">
<meta property="og:image" content="https://kezunlin.me/images/posts/635233-20180808094434566-1643487577.png">
<meta property="og:image" content="https://kezunlin.me/images/posts/635233-20180808094437895-820163270.png">
<meta property="og:image" content="https://kezunlin.me/images/posts/635233-20180808094440686-873638921.png">
<meta property="og:image" content="https://kezunlin.me/images/posts/635233-20180808094443597-1649048206.png">
<meta property="og:image" content="https://kezunlin.me/images/posts/635233-20180808094538529-1155732317.png">
<meta property="og:image" content="https://kezunlin.me/images/posts/635233-20180808094541236-1763457969.png">
<meta property="og:image" content="https://kezunlin.me/images/posts/635233-20180808094544775-920829401.png">
<meta property="og:image" content="https://kezunlin.me/images/posts/635233-20180808094547389-452464521.png">
<meta property="og:image" content="https://kezunlin.me/images/posts/635233-20180808094550416-1980646715.png">
<meta property="og:image" content="https://kezunlin.me/images/posts/635233-20180808094643633-1555377796.png">
<meta property="og:image" content="https://kezunlin.me/images/posts/635233-20180808094646089-423286443.png">
<meta property="og:image" content="https://kezunlin.me/images/posts/635233-20180808094648806-227199948.png">
<meta property="og:image" content="https://kezunlin.me/images/posts/635233-20180808094652998-1285471219.png">
<meta property="og:image" content="https://kezunlin.me/images/posts/635233-20180808094655636-2019663924.png">
<meta property="og:image" content="https://kezunlin.me/images/posts/635233-20180808094801120-1382644570.png">
<meta property="og:image" content="https://kezunlin.me/images/posts/635233-20180808094803553-433970977.png">
<meta property="og:image" content="https://kezunlin.me/images/posts/635233-20180808094806318-876599599.png">
<meta property="og:image" content="https://kezunlin.me/images/posts/635233-20180808094808993-1657424066.png">
<meta property="og:image" content="https://kezunlin.me/images/posts/635233-20180808094812363-210748808.png">
<meta property="og:image" content="https://kezunlin.me/images/posts/635233-20180808094910532-638562904.png">
<meta property="og:image" content="https://kezunlin.me/images/posts/635233-20180808094913556-1116095260.png">
<meta property="og:image" content="https://kezunlin.me/images/posts/635233-20180808094916138-1679934767.png">
<meta property="og:image" content="https://kezunlin.me/images/posts/635233-20180808094919214-1696881500.png">
<meta property="og:image" content="https://kezunlin.me/images/posts/635233-20180808094922779-1511591853.png">
<meta property="article:published_time" content="2018-08-08T01:37:00.000Z">
<meta property="article:modified_time" content="2024-10-14T05:39:28.656Z">
<meta property="article:author" content="kezunlin">
<meta property="article:tag" content="caffe">
<meta property="article:tag" content="LeNet">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://kezunlin.me/images/posts/635233-20180808094007919-402598205.png">

<link rel="canonical" href="https://kezunlin.me/blog/learning-lenet/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>learning lenet with caffe and python | Kezunlin's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Kezunlin's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Kezunlin's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Live and Learn</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kezunlin.me/blog/learning-lenet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="kezunlin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kezunlin's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          learning lenet with caffe and python
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-08-08 09:37:00" itemprop="dateCreated datePublished" datetime="2018-08-08T09:37:00+08:00">2018-08-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-10-14 13:39:28" itemprop="dateModified" datetime="2024-10-14T13:39:28+08:00">2024-10-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="Solving-in-Python-with-LeNet"><a href="#Solving-in-Python-with-LeNet" class="headerlink" title="Solving in Python with LeNet"></a>Solving in Python with LeNet</h2><p>In this example, we’ll explore learning with Caffe in Python, using the fully-exposed <code>Solver</code> interface.</p>
<h3 id="Setup"><a href="#Setup" class="headerlink" title="Setup"></a>Setup</h3><ul>
<li>Set up the Python environment: we’ll use the <code>pylab</code> import for numpy and plot inline.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pylab <span class="keyword">import</span> *</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>

<ul>
<li>Import <code>caffe</code>, adding it to <code>sys.path</code> if needed. Make sure you’ve built pycaffe.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">caffe_root = <span class="string">&#x27;../&#x27;</span>  <span class="comment"># this file should be run from &#123;caffe_root&#125;/examples (otherwise change this line)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.insert(<span class="number">0</span>, caffe_root + <span class="string">&#x27;python&#x27;</span>)</span><br><span class="line"><span class="keyword">import</span> caffe</span><br></pre></td></tr></table></figure>

<ul>
<li>We’ll be using the provided LeNet example data and networks (make sure you’ve downloaded the data and created the databases, as below).</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># run scripts from caffe root</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.chdir(caffe_root)</span><br><span class="line"><span class="comment"># Download data</span></span><br><span class="line">!data/mnist/get_mnist.sh</span><br><span class="line"><span class="comment"># Prepare data</span></span><br><span class="line">!examples/mnist/create_mnist.sh</span><br><span class="line"><span class="comment"># back to examples</span></span><br><span class="line">os.chdir(<span class="string">&#x27;examples&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Downloading...
Creating lmdb...
Done.
</code></pre>
<h3 id="Creating-the-net"><a href="#Creating-the-net" class="headerlink" title="Creating the net"></a>Creating the net</h3><p>Now let’s make a variant of LeNet, the classic 1989 convnet architecture.</p>
<p>We’ll need two external files to help out:</p>
<ul>
<li>the net <code>prototxt</code>, defining the architecture and pointing to the train&#x2F;test data</li>
<li>the solver <code>prototxt</code>, defining the learning parameters</li>
</ul>
<p>We start by creating the net. We’ll write the net in a succinct and natural way as Python code that serializes to Caffe’s protobuf model format.</p>
<p>This network expects to read from pregenerated LMDBs, but reading directly from <code>ndarray</code>s is also possible using <code>MemoryDataLayer</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> caffe <span class="keyword">import</span> layers <span class="keyword">as</span> L, params <span class="keyword">as</span> P</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">lenet</span>(<span class="params">lmdb, batch_size</span>):</span><br><span class="line">    <span class="comment"># our version of LeNet: a series of linear and simple nonlinear transformations</span></span><br><span class="line">    n = caffe.NetSpec()</span><br><span class="line">    </span><br><span class="line">    n.data, n.label = L.Data(batch_size=batch_size, backend=P.Data.LMDB, source=lmdb,</span><br><span class="line">                             transform_param=<span class="built_in">dict</span>(scale=<span class="number">1.</span>/<span class="number">255</span>), ntop=<span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">    n.conv1 = L.Convolution(n.data, kernel_size=<span class="number">5</span>, num_output=<span class="number">20</span>, weight_filler=<span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">&#x27;xavier&#x27;</span>))</span><br><span class="line">    n.pool1 = L.Pooling(n.conv1, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, pool=P.Pooling.MAX)</span><br><span class="line">    n.conv2 = L.Convolution(n.pool1, kernel_size=<span class="number">5</span>, num_output=<span class="number">50</span>, weight_filler=<span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">&#x27;xavier&#x27;</span>))</span><br><span class="line">    n.pool2 = L.Pooling(n.conv2, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, pool=P.Pooling.MAX)</span><br><span class="line">    n.fc1 =   L.InnerProduct(n.pool2, num_output=<span class="number">500</span>, weight_filler=<span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">&#x27;xavier&#x27;</span>))</span><br><span class="line">    n.relu1 = L.ReLU(n.fc1, in_place=<span class="literal">True</span>)</span><br><span class="line">    n.score = L.InnerProduct(n.relu1, num_output=<span class="number">10</span>, weight_filler=<span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">&#x27;xavier&#x27;</span>))</span><br><span class="line">    n.loss =  L.SoftmaxWithLoss(n.score, n.label)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> n.to_proto()</span><br><span class="line">    </span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;mnist/lenet_auto_train.prototxt&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="built_in">str</span>(lenet(<span class="string">&#x27;mnist/mnist_train_lmdb&#x27;</span>, <span class="number">64</span>)))</span><br><span class="line">    </span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;mnist/lenet_auto_test.prototxt&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="built_in">str</span>(lenet(<span class="string">&#x27;mnist/mnist_test_lmdb&#x27;</span>, <span class="number">100</span>)))</span><br></pre></td></tr></table></figure>

<p>The net has been written to disk in a more verbose but human-readable serialization format using Google’s protobuf library. You can read, write, and modify this description directly. Let’s take a look at the train net.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!cat mnist/lenet_auto_train.prototxt</span><br></pre></td></tr></table></figure>

<pre><code>layer &#123;
  name: &quot;data&quot;
  type: &quot;Data&quot;
  top: &quot;data&quot;
  top: &quot;label&quot;
  transform_param &#123;
    scale: 0.00392156862745
  &#125;
  data_param &#123;
    source: &quot;mnist/mnist_train_lmdb&quot;
    batch_size: 64
    backend: LMDB
  &#125;
&#125;
layer &#123;
  name: &quot;conv1&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;data&quot;
  top: &quot;conv1&quot;
  convolution_param &#123;
    num_output: 20
    kernel_size: 5
    weight_filler &#123;
      type: &quot;xavier&quot;
    &#125;
  &#125;
&#125;
layer &#123;
  name: &quot;pool1&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv1&quot;
  top: &quot;pool1&quot;
  pooling_param &#123;
    pool: MAX
    kernel_size: 2
    stride: 2
  &#125;
&#125;
layer &#123;
  name: &quot;conv2&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;pool1&quot;
  top: &quot;conv2&quot;
  convolution_param &#123;
    num_output: 50
    kernel_size: 5
    weight_filler &#123;
      type: &quot;xavier&quot;
    &#125;
  &#125;
&#125;
layer &#123;
  name: &quot;pool2&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv2&quot;
  top: &quot;pool2&quot;
  pooling_param &#123;
    pool: MAX
    kernel_size: 2
    stride: 2
  &#125;
&#125;
layer &#123;
  name: &quot;fc1&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;pool2&quot;
  top: &quot;fc1&quot;
  inner_product_param &#123;
    num_output: 500
    weight_filler &#123;
      type: &quot;xavier&quot;
    &#125;
  &#125;
&#125;
layer &#123;
  name: &quot;relu1&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;fc1&quot;
  top: &quot;fc1&quot;
&#125;
layer &#123;
  name: &quot;score&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;fc1&quot;
  top: &quot;score&quot;
  inner_product_param &#123;
    num_output: 10
    weight_filler &#123;
      type: &quot;xavier&quot;
    &#125;
  &#125;
&#125;
layer &#123;
  name: &quot;loss&quot;
  type: &quot;SoftmaxWithLoss&quot;
  bottom: &quot;score&quot;
  bottom: &quot;label&quot;
  top: &quot;loss&quot;
&#125;
</code></pre>
<p>Now let’s see the learning parameters, which are also written as a <code>prototxt</code> file (already provided on disk). We’re using SGD with momentum, weight decay, and a specific learning rate schedule.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!cat mnist/lenet_auto_solver.prototxt</span><br></pre></td></tr></table></figure>

<pre><code># The train/test net protocol buffer definition
train_net: &quot;mnist/lenet_auto_train.prototxt&quot;
test_net: &quot;mnist/lenet_auto_test.prototxt&quot;
# test_iter specifies how many forward passes the test should carry out.
# In the case of MNIST, we have test batch size 100 and 100 test iterations,
# covering the full 10,000 testing images.
test_iter: 100
# Carry out testing every 500 training iterations.
test_interval: 500
# The base learning rate, momentum and the weight decay of the network.
base_lr: 0.01
momentum: 0.9
weight_decay: 0.0005
# The learning rate policy
lr_policy: &quot;inv&quot;
gamma: 0.0001
power: 0.75
# Display every 100 iterations
display: 100
# The maximum number of iterations
max_iter: 10000
# snapshot intermediate results
snapshot: 5000
snapshot_prefix: &quot;mnist/lenet&quot;
</code></pre>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- kzl in-article ad -->
<p><ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-5653382914441020"
     data-ad-slot="7925631830"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>




<h3 id="Loading-and-checking-the-solver"><a href="#Loading-and-checking-the-solver" class="headerlink" title="Loading and checking the solver"></a>Loading and checking the solver</h3><ul>
<li>Let’s pick a device and load the solver. We’ll use SGD (with momentum), but other methods (such as Adagrad and Nesterov’s accelerated gradient) are also available.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">caffe.set_device(<span class="number">0</span>)</span><br><span class="line">caffe.set_mode_gpu()</span><br><span class="line"></span><br><span class="line"><span class="comment">### load the solver and create train and test nets</span></span><br><span class="line">solver = <span class="literal">None</span>  <span class="comment"># ignore this workaround for lmdb data (can&#x27;t instantiate two solvers on the same data)</span></span><br><span class="line">solver = caffe.SGDSolver(<span class="string">&#x27;mnist/lenet_auto_solver.prototxt&#x27;</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>To get an idea of the architecture of our net, we can check the dimensions of the intermediate features (blobs) and parameters (these will also be useful to refer to when manipulating data later).</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># each output is (batch size, feature dim, spatial dim)</span></span><br><span class="line">[(k, v.data.shape) <span class="keyword">for</span> k, v <span class="keyword">in</span> solver.net.blobs.items()]</span><br></pre></td></tr></table></figure>




<pre><code>[(&#39;data&#39;, (64, 1, 28, 28)),
 (&#39;label&#39;, (64,)),
 (&#39;conv1&#39;, (64, 20, 24, 24)),
 (&#39;pool1&#39;, (64, 20, 12, 12)),
 (&#39;conv2&#39;, (64, 50, 8, 8)),
 (&#39;pool2&#39;, (64, 50, 4, 4)),
 (&#39;fc1&#39;, (64, 500)),
 (&#39;score&#39;, (64, 10)),
 (&#39;loss&#39;, ())]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># just print the weight sizes (we&#x27;ll omit the biases)</span></span><br><span class="line">[(k, v[<span class="number">0</span>].data.shape) <span class="keyword">for</span> k, v <span class="keyword">in</span> solver.net.params.items()]</span><br></pre></td></tr></table></figure>




<pre><code>[(&#39;conv1&#39;, (20, 1, 5, 5)),
 (&#39;conv2&#39;, (50, 20, 5, 5)),
 (&#39;fc1&#39;, (500, 800)),
 (&#39;score&#39;, (10, 500))]
</code></pre>
<ul>
<li>Before taking off, let’s check that everything is loaded as we expect. We’ll run a forward pass on the train and test nets and check that they contain our data.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">solver.net.forward()  <span class="comment"># train net</span></span><br><span class="line">solver.test_nets[<span class="number">0</span>].forward()  <span class="comment"># test net (there can be more than one)</span></span><br></pre></td></tr></table></figure>




<pre><code>&#123;&#39;loss&#39;: array(2.365971088409424, dtype=float32)&#125;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># we use a little trick to tile the first eight images</span></span><br><span class="line">imshow(solver.net.blobs[<span class="string">&#x27;data&#x27;</span>].data[:<span class="number">8</span>, <span class="number">0</span>].transpose(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>).reshape(<span class="number">28</span>, <span class="number">8</span>*<span class="number">28</span>), cmap=<span class="string">&#x27;gray&#x27;</span>); axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span> <span class="string">&#x27;train labels:&#x27;</span>, solver.net.blobs[<span class="string">&#x27;label&#x27;</span>].data[:<span class="number">8</span>]</span><br></pre></td></tr></table></figure>

<pre><code>train labels: [ 5.  0.  4.  1.  9.  2.  1.  3.]
</code></pre>
<p><img src="https://kezunlin.me/images/posts/635233-20180808094007919-402598205.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">imshow(solver.test_nets[<span class="number">0</span>].blobs[<span class="string">&#x27;data&#x27;</span>].data[:<span class="number">8</span>, <span class="number">0</span>].transpose(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>).reshape(<span class="number">28</span>, <span class="number">8</span>*<span class="number">28</span>), cmap=<span class="string">&#x27;gray&#x27;</span>); axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span> <span class="string">&#x27;test labels:&#x27;</span>, solver.test_nets[<span class="number">0</span>].blobs[<span class="string">&#x27;label&#x27;</span>].data[:<span class="number">8</span>]</span><br></pre></td></tr></table></figure>

<pre><code>test labels: [ 7.  2.  1.  0.  4.  1.  4.  9.]
</code></pre>
<p><img src="https://kezunlin.me/images/posts/635233-20180808094011687-911149908.png" alt="png"></p>
<h3 id="Stepping-the-solver"><a href="#Stepping-the-solver" class="headerlink" title="Stepping the solver"></a>Stepping the solver</h3><p>Both train and test nets seem to be loading data, and to have correct labels.</p>
<ul>
<li>Let’s take one step of (minibatch) SGD and see what happens.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">solver.step(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>Do we have gradients propagating through our filters? Let’s see the updates to the first layer, shown here as a $4 \times 5$ grid of $5 \times 5$ filters.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">imshow(solver.net.params[<span class="string">&#x27;conv1&#x27;</span>][<span class="number">0</span>].diff[:, <span class="number">0</span>].reshape(<span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line">       .transpose(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>).reshape(<span class="number">4</span>*<span class="number">5</span>, <span class="number">5</span>*<span class="number">5</span>), cmap=<span class="string">&#x27;gray&#x27;</span>); axis(<span class="string">&#x27;off&#x27;</span>)</span><br></pre></td></tr></table></figure>




<pre><code>(-0.5, 24.5, 19.5, -0.5)
</code></pre>
<p><img src="https://kezunlin.me/images/posts/635233-20180808094015260-1011851543.png" alt="png"></p>
<h3 id="Writing-a-custom-training-loop"><a href="#Writing-a-custom-training-loop" class="headerlink" title="Writing a custom training loop"></a>Writing a custom training loop</h3><p>Something is happening. Let’s run the net for a while, keeping track of a few things as it goes.<br>Note that this process will be the same as if training through the <code>caffe</code> binary. In particular:</p>
<ul>
<li>logging will continue to happen as normal</li>
<li>snapshots will be taken at the interval specified in the solver prototxt (here, every 5000 iterations)</li>
<li>testing will happen at the interval specified (here, every 500 iterations)</li>
</ul>
<p>Since we have control of the loop in Python, we’re free to compute additional things as we go, as we show below. We can do many other things as well, for example:</p>
<ul>
<li>write a custom stopping criterion</li>
<li>change the solving process by updating the net in the loop</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">niter = <span class="number">200</span></span><br><span class="line">test_interval = <span class="number">25</span></span><br><span class="line"><span class="comment"># losses will also be stored in the log</span></span><br><span class="line">train_loss = zeros(niter)</span><br><span class="line">test_acc = zeros(<span class="built_in">int</span>(np.ceil(niter / test_interval)))</span><br><span class="line">output = zeros((niter, <span class="number">8</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># the main solver loop</span></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> <span class="built_in">range</span>(niter):</span><br><span class="line">    solver.step(<span class="number">1</span>)  <span class="comment"># SGD by Caffe</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># store the train loss</span></span><br><span class="line">    train_loss[it] = solver.net.blobs[<span class="string">&#x27;loss&#x27;</span>].data</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># store the output on the first test batch</span></span><br><span class="line">    <span class="comment"># (start the forward pass at conv1 to avoid loading new data)</span></span><br><span class="line">    solver.test_nets[<span class="number">0</span>].forward(start=<span class="string">&#x27;conv1&#x27;</span>)</span><br><span class="line">    output[it] = solver.test_nets[<span class="number">0</span>].blobs[<span class="string">&#x27;score&#x27;</span>].data[:<span class="number">8</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># run a full test every so often</span></span><br><span class="line">    <span class="comment"># (Caffe can also do this for us and write to a log, but we show here</span></span><br><span class="line">    <span class="comment">#  how to do it directly in Python, where more complicated things are easier.)</span></span><br><span class="line">    <span class="keyword">if</span> it % test_interval == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span> <span class="string">&#x27;Iteration&#x27;</span>, it, <span class="string">&#x27;testing...&#x27;</span></span><br><span class="line">        correct = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> test_it <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">            solver.test_nets[<span class="number">0</span>].forward()</span><br><span class="line">            correct += <span class="built_in">sum</span>(solver.test_nets[<span class="number">0</span>].blobs[<span class="string">&#x27;score&#x27;</span>].data.argmax(<span class="number">1</span>)</span><br><span class="line">                           == solver.test_nets[<span class="number">0</span>].blobs[<span class="string">&#x27;label&#x27;</span>].data)</span><br><span class="line">        test_acc[it // test_interval] = correct / <span class="number">1e4</span></span><br></pre></td></tr></table></figure>

<pre><code>Iteration 0 testing...
Iteration 25 testing...
Iteration 50 testing...
Iteration 75 testing...
Iteration 100 testing...
Iteration 125 testing...
Iteration 150 testing...
Iteration 175 testing...
CPU times: user 12.6 s, sys: 2.4 s, total: 15 s
Wall time: 14.4 s
</code></pre>
<ul>
<li>Let’s plot the train loss and test accuracy.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">_, ax1 = subplots()</span><br><span class="line">ax2 = ax1.twinx()</span><br><span class="line">ax1.plot(arange(niter), train_loss)</span><br><span class="line">ax2.plot(test_interval * arange(<span class="built_in">len</span>(test_acc)), test_acc, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">ax1.set_xlabel(<span class="string">&#x27;iteration&#x27;</span>)</span><br><span class="line">ax1.set_ylabel(<span class="string">&#x27;train loss&#x27;</span>)</span><br><span class="line">ax2.set_ylabel(<span class="string">&#x27;test accuracy&#x27;</span>)</span><br><span class="line">ax2.set_title(<span class="string">&#x27;Test Accuracy: &#123;:.2f&#125;&#x27;</span>.<span class="built_in">format</span>(test_acc[-<span class="number">1</span>]))</span><br></pre></td></tr></table></figure>




<pre><code>&lt;matplotlib.text.Text at 0x7f5199b33610&gt;
</code></pre>
<p><img src="https://kezunlin.me/images/posts/635233-20180808094018680-398130560.png" alt="png"></p>
<p>The loss seems to have dropped quickly and coverged (except for stochasticity), while the accuracy rose correspondingly. Hooray!</p>
<ul>
<li>Since we saved the results on the first test batch, we can watch how our prediction scores evolved. We’ll plot time on the $x$ axis and each possible label on the $y$, with lightness indicating confidence.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    figure(figsize=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">    imshow(solver.test_nets[<span class="number">0</span>].blobs[<span class="string">&#x27;data&#x27;</span>].data[i, <span class="number">0</span>], cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    figure(figsize=(<span class="number">10</span>, <span class="number">2</span>))</span><br><span class="line">    imshow(output[:<span class="number">50</span>, i].T, interpolation=<span class="string">&#x27;nearest&#x27;</span>, cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    xlabel(<span class="string">&#x27;iteration&#x27;</span>)</span><br><span class="line">    ylabel(<span class="string">&#x27;label&#x27;</span>)</span><br></pre></td></tr></table></figure>


<p><img src="https://kezunlin.me/images/posts/635233-20180808094024721-1595689994.png" alt="png"></p>
<p><img src="https://kezunlin.me/images/posts/635233-20180808094029552-416345653.png" alt="png"></p>
<p><img src="https://kezunlin.me/images/posts/635233-20180808094034649-822966977.png" alt="png"></p>
<p><img src="https://kezunlin.me/images/posts/635233-20180808094311497-1449576622.png" alt="png"></p>
<p><img src="https://kezunlin.me/images/posts/635233-20180808094314887-1994925148.png" alt="png"></p>
<p><img src="https://kezunlin.me/images/posts/635233-20180808094317536-26566385.png" alt="png"></p>
<p><img src="https://kezunlin.me/images/posts/635233-20180808094335122-926095188.png" alt="png"></p>
<p><img src="https://kezunlin.me/images/posts/635233-20180808094337980-697149050.png" alt="png"></p>
<p><img src="https://kezunlin.me/images/posts/635233-20180808094432007-708264326.png" alt="png"></p>
<p><img src="https://kezunlin.me/images/posts/635233-20180808094434566-1643487577.png" alt="png"></p>
<p><img src="https://kezunlin.me/images/posts/635233-20180808094437895-820163270.png" alt="png"></p>
<p><img src="https://kezunlin.me/images/posts/635233-20180808094440686-873638921.png" alt="png"></p>
<p><img src="https://kezunlin.me/images/posts/635233-20180808094443597-1649048206.png" alt="png"></p>
<p><img src="https://kezunlin.me/images/posts/635233-20180808094538529-1155732317.png" alt="png"></p>
<p><img src="https://kezunlin.me/images/posts/635233-20180808094541236-1763457969.png" alt="png"></p>
<p><img src="https://kezunlin.me/images/posts/635233-20180808094544775-920829401.png" alt="png"></p>
<p>We started with little idea about any of these digits, and ended up with correct classifications for each. If you’ve been following along, you’ll see the last digit is the most difficult, a slanted “9” that’s (understandably) most confused with “4”.</p>
<ul>
<li>Note that these are the “raw” output scores rather than the softmax-computed probability vectors. The latter, shown below, make it easier to see the confidence of our net (but harder to see the scores for less likely digits).</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    figure(figsize=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">    imshow(solver.test_nets[<span class="number">0</span>].blobs[<span class="string">&#x27;data&#x27;</span>].data[i, <span class="number">0</span>], cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    figure(figsize=(<span class="number">10</span>, <span class="number">2</span>))</span><br><span class="line">    imshow(exp(output[:<span class="number">50</span>, i].T) / exp(output[:<span class="number">50</span>, i].T).<span class="built_in">sum</span>(<span class="number">0</span>), interpolation=<span class="string">&#x27;nearest&#x27;</span>, cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    xlabel(<span class="string">&#x27;iteration&#x27;</span>)</span><br><span class="line">    ylabel(<span class="string">&#x27;label&#x27;</span>)</span><br></pre></td></tr></table></figure>


<p><img src="https://kezunlin.me/images/posts/635233-20180808094547389-452464521.png" alt="png"></p>
<p><img src="https://kezunlin.me/images/posts/635233-20180808094550416-1980646715.png" alt="png"></p>
<p><img src="https://kezunlin.me/images/posts/635233-20180808094643633-1555377796.png" alt="png"></p>
<p><img src="https://kezunlin.me/images/posts/635233-20180808094646089-423286443.png" alt="png"></p>
<p><img src="https://kezunlin.me/images/posts/635233-20180808094648806-227199948.png" alt="png"></p>
<p><img src="https://kezunlin.me/images/posts/635233-20180808094652998-1285471219.png" alt="png"></p>
<p><img src="https://kezunlin.me/images/posts/635233-20180808094655636-2019663924.png" alt="png"></p>
<p><img src="https://kezunlin.me/images/posts/635233-20180808094801120-1382644570.png" alt="png"></p>
<p><img src="https://kezunlin.me/images/posts/635233-20180808094803553-433970977.png" alt="png"></p>
<p><img src="https://kezunlin.me/images/posts/635233-20180808094806318-876599599.png" alt="png"></p>
<p><img src="https://kezunlin.me/images/posts/635233-20180808094808993-1657424066.png" alt="png"></p>
<p><img src="https://kezunlin.me/images/posts/635233-20180808094812363-210748808.png" alt="png"></p>
<p><img src="https://kezunlin.me/images/posts/635233-20180808094910532-638562904.png" alt="png"></p>
<p><img src="https://kezunlin.me/images/posts/635233-20180808094913556-1116095260.png" alt="png"></p>
<p><img src="https://kezunlin.me/images/posts/635233-20180808094916138-1679934767.png" alt="png"></p>
<p><img src="https://kezunlin.me/images/posts/635233-20180808094919214-1696881500.png" alt="png"></p>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- kzl in-article ad -->
<p><ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-5653382914441020"
     data-ad-slot="7925631830"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>



<h3 id="Experiment-with-architecture-and-optimization"><a href="#Experiment-with-architecture-and-optimization" class="headerlink" title="Experiment with architecture and optimization"></a>Experiment with architecture and optimization</h3><p>Now that we’ve defined, trained, and tested LeNet there are many possible next steps:</p>
<ul>
<li>Define new architectures for comparison</li>
<li>Tune optimization by setting <code>base_lr</code> and the like or simply training longer</li>
<li>Switching the solver type from <code>SGD</code> to an adaptive method like <code>AdaDelta</code> or <code>Adam</code></li>
</ul>
<p>Feel free to explore these directions by editing the all-in-one example that follows.<br>Look for “<code>EDIT HERE</code>“ comments for suggested choice points.</p>
<p>By default this defines a simple linear classifier as a baseline.</p>
<p>In case your coffee hasn’t kicked in and you’d like inspiration, try out</p>
<ol>
<li>Switch the nonlinearity from <code>ReLU</code> to <code>ELU</code> or a saturing nonlinearity like <code>Sigmoid</code></li>
<li>Stack more fully connected and nonlinear layers</li>
<li>Search over learning rate 10x at a time (trying <code>0.1</code> and <code>0.001</code>)</li>
<li>Switch the solver type to <code>Adam</code> (this adaptive solver type should be less sensitive to hyperparameters, but no guarantees…)</li>
<li>Solve for longer by setting <code>niter</code> higher (to 500 or 1,000 for instance) to better show training differences</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br></pre></td><td class="code"><pre><span class="line">train_net_path = <span class="string">&#x27;mnist/custom_auto_train.prototxt&#x27;</span></span><br><span class="line">test_net_path = <span class="string">&#x27;mnist/custom_auto_test.prototxt&#x27;</span></span><br><span class="line">solver_config_path = <span class="string">&#x27;mnist/custom_auto_solver.prototxt&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### define net</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">custom_net</span>(<span class="params">lmdb, batch_size</span>):</span><br><span class="line">    <span class="comment"># define your own net!</span></span><br><span class="line">    n = caffe.NetSpec()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># keep this data layer for all networks</span></span><br><span class="line">    n.data, n.label = L.Data(batch_size=batch_size, backend=P.Data.LMDB, source=lmdb,</span><br><span class="line">                             transform_param=<span class="built_in">dict</span>(scale=<span class="number">1.</span>/<span class="number">255</span>), ntop=<span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># EDIT HERE to try different networks</span></span><br><span class="line">    <span class="comment"># this single layer defines a simple linear classifier</span></span><br><span class="line">    <span class="comment"># (in particular this defines a multiway logistic regression)</span></span><br><span class="line">    n.score =   L.InnerProduct(n.data, num_output=<span class="number">10</span>, weight_filler=<span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">&#x27;xavier&#x27;</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># EDIT HERE this is the LeNet variant we have already tried</span></span><br><span class="line">    <span class="comment"># n.conv1 = L.Convolution(n.data, kernel_size=5, num_output=20, weight_filler=dict(type=&#x27;xavier&#x27;))</span></span><br><span class="line">    <span class="comment"># n.pool1 = L.Pooling(n.conv1, kernel_size=2, stride=2, pool=P.Pooling.MAX)</span></span><br><span class="line">    <span class="comment"># n.conv2 = L.Convolution(n.pool1, kernel_size=5, num_output=50, weight_filler=dict(type=&#x27;xavier&#x27;))</span></span><br><span class="line">    <span class="comment"># n.pool2 = L.Pooling(n.conv2, kernel_size=2, stride=2, pool=P.Pooling.MAX)</span></span><br><span class="line">    <span class="comment"># n.fc1 =   L.InnerProduct(n.pool2, num_output=500, weight_filler=dict(type=&#x27;xavier&#x27;))</span></span><br><span class="line">    <span class="comment"># EDIT HERE consider L.ELU or L.Sigmoid for the nonlinearity</span></span><br><span class="line">    <span class="comment"># n.relu1 = L.ReLU(n.fc1, in_place=True)</span></span><br><span class="line">    <span class="comment"># n.score =   L.InnerProduct(n.fc1, num_output=10, weight_filler=dict(type=&#x27;xavier&#x27;))</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># keep this loss layer for all networks</span></span><br><span class="line">    n.loss =  L.SoftmaxWithLoss(n.score, n.label)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> n.to_proto()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(train_net_path, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="built_in">str</span>(custom_net(<span class="string">&#x27;mnist/mnist_train_lmdb&#x27;</span>, <span class="number">64</span>)))    </span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(test_net_path, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="built_in">str</span>(custom_net(<span class="string">&#x27;mnist/mnist_test_lmdb&#x27;</span>, <span class="number">100</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment">### define solver</span></span><br><span class="line"><span class="keyword">from</span> caffe.proto <span class="keyword">import</span> caffe_pb2</span><br><span class="line">s = caffe_pb2.SolverParameter()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set a seed for reproducible experiments:</span></span><br><span class="line"><span class="comment"># this controls for randomization in training.</span></span><br><span class="line">s.random_seed = <span class="number">0xCAFFE</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Specify locations of the train and (maybe) test networks.</span></span><br><span class="line">s.train_net = train_net_path</span><br><span class="line">s.test_net.append(test_net_path)</span><br><span class="line">s.test_interval = <span class="number">500</span>  <span class="comment"># Test after every 500 training iterations.</span></span><br><span class="line">s.test_iter.append(<span class="number">100</span>) <span class="comment"># Test on 100 batches each time we test.</span></span><br><span class="line"></span><br><span class="line">s.max_iter = <span class="number">10000</span>     <span class="comment"># no. of times to update the net (training iterations)</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># EDIT HERE to try different solvers</span></span><br><span class="line"><span class="comment"># solver types include &quot;SGD&quot;, &quot;Adam&quot;, and &quot;Nesterov&quot; among others.</span></span><br><span class="line">s.<span class="built_in">type</span> = <span class="string">&quot;SGD&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the initial learning rate for SGD.</span></span><br><span class="line">s.base_lr = <span class="number">0.01</span>  <span class="comment"># EDIT HERE to try different learning rates</span></span><br><span class="line"><span class="comment"># Set momentum to accelerate learning by</span></span><br><span class="line"><span class="comment"># taking weighted average of current and previous updates.</span></span><br><span class="line">s.momentum = <span class="number">0.9</span></span><br><span class="line"><span class="comment"># Set weight decay to regularize and prevent overfitting</span></span><br><span class="line">s.weight_decay = <span class="number">5e-4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set `lr_policy` to define how the learning rate changes during training.</span></span><br><span class="line"><span class="comment"># This is the same policy as our default LeNet.</span></span><br><span class="line">s.lr_policy = <span class="string">&#x27;inv&#x27;</span></span><br><span class="line">s.gamma = <span class="number">0.0001</span></span><br><span class="line">s.power = <span class="number">0.75</span></span><br><span class="line"><span class="comment"># EDIT HERE to try the fixed rate (and compare with adaptive solvers)</span></span><br><span class="line"><span class="comment"># `fixed` is the simplest policy that keeps the learning rate constant.</span></span><br><span class="line"><span class="comment"># s.lr_policy = &#x27;fixed&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Display the current training loss and accuracy every 1000 iterations.</span></span><br><span class="line">s.display = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Snapshots are files used to store networks we&#x27;ve trained.</span></span><br><span class="line"><span class="comment"># We&#x27;ll snapshot every 5K iterations -- twice during training.</span></span><br><span class="line">s.snapshot = <span class="number">5000</span></span><br><span class="line">s.snapshot_prefix = <span class="string">&#x27;mnist/custom_net&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Train on the GPU</span></span><br><span class="line">s.solver_mode = caffe_pb2.SolverParameter.GPU</span><br><span class="line"></span><br><span class="line"><span class="comment"># Write the solver to a temporary file and return its filename.</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(solver_config_path, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="built_in">str</span>(s))</span><br><span class="line"></span><br><span class="line"><span class="comment">### load the solver and create train and test nets</span></span><br><span class="line">solver = <span class="literal">None</span>  <span class="comment"># ignore this workaround for lmdb data (can&#x27;t instantiate two solvers on the same data)</span></span><br><span class="line">solver = caffe.get_solver(solver_config_path)</span><br><span class="line"></span><br><span class="line"><span class="comment">### solve</span></span><br><span class="line">niter = <span class="number">250</span>  <span class="comment"># EDIT HERE increase to train for longer</span></span><br><span class="line">test_interval = niter / <span class="number">10</span></span><br><span class="line"><span class="comment"># losses will also be stored in the log</span></span><br><span class="line">train_loss = zeros(niter)</span><br><span class="line">test_acc = zeros(<span class="built_in">int</span>(np.ceil(niter / test_interval)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># the main solver loop</span></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> <span class="built_in">range</span>(niter):</span><br><span class="line">    solver.step(<span class="number">1</span>)  <span class="comment"># SGD by Caffe</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># store the train loss</span></span><br><span class="line">    train_loss[it] = solver.net.blobs[<span class="string">&#x27;loss&#x27;</span>].data</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># run a full test every so often</span></span><br><span class="line">    <span class="comment"># (Caffe can also do this for us and write to a log, but we show here</span></span><br><span class="line">    <span class="comment">#  how to do it directly in Python, where more complicated things are easier.)</span></span><br><span class="line">    <span class="keyword">if</span> it % test_interval == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span> <span class="string">&#x27;Iteration&#x27;</span>, it, <span class="string">&#x27;testing...&#x27;</span></span><br><span class="line">        correct = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> test_it <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">            solver.test_nets[<span class="number">0</span>].forward()</span><br><span class="line">            correct += <span class="built_in">sum</span>(solver.test_nets[<span class="number">0</span>].blobs[<span class="string">&#x27;score&#x27;</span>].data.argmax(<span class="number">1</span>)</span><br><span class="line">                           == solver.test_nets[<span class="number">0</span>].blobs[<span class="string">&#x27;label&#x27;</span>].data)</span><br><span class="line">        test_acc[it // test_interval] = correct / <span class="number">1e4</span></span><br><span class="line"></span><br><span class="line">_, ax1 = subplots()</span><br><span class="line">ax2 = ax1.twinx()</span><br><span class="line">ax1.plot(arange(niter), train_loss)</span><br><span class="line">ax2.plot(test_interval * arange(<span class="built_in">len</span>(test_acc)), test_acc, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">ax1.set_xlabel(<span class="string">&#x27;iteration&#x27;</span>)</span><br><span class="line">ax1.set_ylabel(<span class="string">&#x27;train loss&#x27;</span>)</span><br><span class="line">ax2.set_ylabel(<span class="string">&#x27;test accuracy&#x27;</span>)</span><br><span class="line">ax2.set_title(<span class="string">&#x27;Custom Test Accuracy: &#123;:.2f&#125;&#x27;</span>.<span class="built_in">format</span>(test_acc[-<span class="number">1</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>Iteration 0 testing...
Iteration 25 testing...
Iteration 50 testing...
Iteration 75 testing...
Iteration 100 testing...
Iteration 125 testing...
Iteration 150 testing...
Iteration 175 testing...
Iteration 200 testing...
Iteration 225 testing...





&lt;matplotlib.text.Text at 0x7f5199af9f50&gt;
</code></pre>
<p><img src="https://kezunlin.me/images/posts/635233-20180808094922779-1511591853.png" alt="png"></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a target="_blank" rel="noopener" href="http://demo.vislab.berkeleyvision.org/">demo</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/BVLC/caffe">caffe git</a></li>
</ul>
<h2 id="History"><a href="#History" class="headerlink" title="History"></a>History</h2><ul>
<li>20180808: created.</li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/caffe/" rel="tag"># caffe</a>
              <a href="/tags/LeNet/" rel="tag"># LeNet</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/blog/Classification-Instant-Recognition-with-Caffe/" rel="prev" title="Classification: Instant Recognition with Caffe">
      <i class="fa fa-chevron-left"></i> Classification: Instant Recognition with Caffe
    </a></div>
      <div class="post-nav-item">
    <a href="/blog/Fine-tuning-a-Pretrained-Network-for-Style-Recognition/" rel="next" title="Fine-tuning a Pretrained Network for Style Recognition">
      Fine-tuning a Pretrained Network for Style Recognition <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Solving-in-Python-with-LeNet"><span class="nav-number">1.</span> <span class="nav-text">Solving in Python with LeNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Setup"><span class="nav-number">1.1.</span> <span class="nav-text">Setup</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Creating-the-net"><span class="nav-number">1.2.</span> <span class="nav-text">Creating the net</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Loading-and-checking-the-solver"><span class="nav-number">1.3.</span> <span class="nav-text">Loading and checking the solver</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Stepping-the-solver"><span class="nav-number">1.4.</span> <span class="nav-text">Stepping the solver</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Writing-a-custom-training-loop"><span class="nav-number">1.5.</span> <span class="nav-text">Writing a custom training loop</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Experiment-with-architecture-and-optimization"><span class="nav-number">1.6.</span> <span class="nav-text">Experiment with architecture and optimization</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reference"><span class="nav-number">2.</span> <span class="nav-text">Reference</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#History"><span class="nav-number">3.</span> <span class="nav-text">History</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">kezunlin</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">181</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">213</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">kezunlin</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
