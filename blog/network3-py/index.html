<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"kezunlin.me","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="network3.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899">
<meta property="og:type" content="article">
<meta property="og:title" content="network3.py">
<meta property="og:url" content="https://kezunlin.me/blog/network3-py/index.html">
<meta property="og:site_name" content="Kezunlin&#39;s Blog">
<meta property="og:description" content="network3.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2018-08-07T02:36:00.000Z">
<meta property="article:modified_time" content="2024-10-14T05:39:28.656Z">
<meta property="article:author" content="kezunlin">
<meta property="article:tag" content="sgd">
<meta property="article:tag" content="neural networks and deep learning">
<meta property="article:tag" content="mlp">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://kezunlin.me/blog/network3-py/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>network3.py | Kezunlin's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Kezunlin's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Kezunlin's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Live and Learn</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kezunlin.me/blog/network3-py/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="kezunlin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kezunlin's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          network3.py
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-08-07 10:36:00" itemprop="dateCreated datePublished" datetime="2018-08-07T10:36:00+08:00">2018-08-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-10-14 13:39:28" itemprop="dateModified" datetime="2024-10-14T13:39:28+08:00">2024-10-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="network3-py"><a href="#network3-py" class="headerlink" title="network3.py"></a>network3.py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br><span class="line">715</span><br><span class="line">716</span><br><span class="line">717</span><br><span class="line">718</span><br><span class="line">719</span><br><span class="line">720</span><br><span class="line">721</span><br><span class="line">722</span><br><span class="line">723</span><br><span class="line">724</span><br><span class="line">725</span><br><span class="line">726</span><br><span class="line">727</span><br><span class="line">728</span><br><span class="line">729</span><br><span class="line">730</span><br><span class="line">731</span><br><span class="line">732</span><br><span class="line">733</span><br><span class="line">734</span><br><span class="line">735</span><br><span class="line">736</span><br><span class="line">737</span><br><span class="line">738</span><br><span class="line">739</span><br><span class="line">740</span><br><span class="line">741</span><br><span class="line">742</span><br><span class="line">743</span><br><span class="line">744</span><br><span class="line">745</span><br><span class="line">746</span><br><span class="line">747</span><br><span class="line">748</span><br><span class="line">749</span><br><span class="line">750</span><br><span class="line">751</span><br><span class="line">752</span><br><span class="line">753</span><br><span class="line">754</span><br><span class="line">755</span><br><span class="line">756</span><br><span class="line">757</span><br><span class="line">758</span><br><span class="line">759</span><br><span class="line">760</span><br><span class="line">761</span><br><span class="line">762</span><br><span class="line">763</span><br><span class="line">764</span><br><span class="line">765</span><br><span class="line">766</span><br><span class="line">767</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;network3.py</span></span><br><span class="line"><span class="string">~~~~~~~~~~~~~~</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">A Theano-based program for training and running simple neural</span></span><br><span class="line"><span class="string">networks.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Supports several layer types (fully connected, convolutional, max</span></span><br><span class="line"><span class="string">pooling, softmax), and activation functions (sigmoid, tanh, and</span></span><br><span class="line"><span class="string">rectified linear units, with more easily added).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">When run on a CPU, this program is much faster than network.py and</span></span><br><span class="line"><span class="string">network2.py.  However, unlike network.py and network2.py it can also</span></span><br><span class="line"><span class="string">be run on a GPU, which makes it faster still.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Because the code is based on Theano, the code is different in many</span></span><br><span class="line"><span class="string">ways from network.py and network2.py.  However, where possible I have</span></span><br><span class="line"><span class="string">tried to maintain consistency with the earlier programs.  In</span></span><br><span class="line"><span class="string">particular, the API is similar to network2.py.  Note that I have</span></span><br><span class="line"><span class="string">focused on making the code simple, easily readable, and easily</span></span><br><span class="line"><span class="string">modifiable.  It is not optimized, and omits many desirable features.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">This program incorporates ideas from the Theano documentation on</span></span><br><span class="line"><span class="string">convolutional neural nets (notably,</span></span><br><span class="line"><span class="string">http://deeplearning.net/tutorial/lenet.html ), from Misha Denil&#x27;s</span></span><br><span class="line"><span class="string">implementation of dropout (https://github.com/mdenil/dropout ), and</span></span><br><span class="line"><span class="string">from Chris Olah (http://colah.github.io ).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Written for Theano 0.6 and 0.7, needs some changes for more recent</span></span><br><span class="line"><span class="string">versions of Theano.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">对于N=50000数据全部参与训练，time(python) = 7分钟; time(theano) = 1分钟。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">But the big win is the ability to do fast symbolic differentiation, </span></span><br><span class="line"><span class="string">using a very general form of the backpropagation algorithm. </span></span><br><span class="line"><span class="string">This is extremely useful for applying stochastic gradient</span></span><br><span class="line"><span class="string">descent to a wide variety of network architectures.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#### Libraries</span></span><br><span class="line"><span class="comment"># Standard library</span></span><br><span class="line"><span class="keyword">import</span> cPickle</span><br><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"></span><br><span class="line"><span class="comment"># Third-party libraries</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> theano</span><br><span class="line"><span class="keyword">import</span> theano.tensor <span class="keyword">as</span> T</span><br><span class="line"><span class="keyword">from</span> theano.tensor.nnet <span class="keyword">import</span> conv</span><br><span class="line"><span class="keyword">from</span> theano.tensor.nnet <span class="keyword">import</span> softmax</span><br><span class="line"><span class="keyword">from</span> theano.tensor <span class="keyword">import</span> shared_randomstreams</span><br><span class="line"><span class="comment">#from theano.tensor.signal.downsample import max_pool_2d  # for version theano-0.7</span></span><br><span class="line"><span class="keyword">from</span> theano.tensor.signal.pool <span class="keyword">import</span> pool_2d <span class="comment"># for version theano-0.9</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Activation functions for neurons</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">linear</span>(<span class="params">z</span>): <span class="keyword">return</span> z</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ReLU</span>(<span class="params">z</span>): <span class="keyword">return</span> T.maximum(<span class="number">0.0</span>, z)</span><br><span class="line"><span class="keyword">from</span> theano.tensor.nnet <span class="keyword">import</span> sigmoid</span><br><span class="line"><span class="keyword">from</span> theano.tensor <span class="keyword">import</span> tanh</span><br><span class="line"></span><br><span class="line"><span class="comment">#### Load the MNIST data</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_shared</span>(<span class="params">filename=<span class="string">&quot;../data/mnist.pkl.gz&quot;</span>,training_set_size=<span class="number">1000</span></span>):</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&#x27;loading data from &#123;0&#125; of #&#123;1&#125;&#x27;</span>.<span class="built_in">format</span>(filename,training_set_size)</span><br><span class="line">    f = gzip.<span class="built_in">open</span>(filename, <span class="string">&#x27;rb&#x27;</span>)</span><br><span class="line">    training_data, validation_data, test_data = cPickle.load(f) <span class="comment"># float32(N,784); int64(N,)</span></span><br><span class="line">    f.close()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">shared</span>(<span class="params">data</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Place the data into shared variables.  This allows Theano to copy</span></span><br><span class="line"><span class="string">        the data to the GPU, if one is available.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        shared_x.get_value().shape   float32(50000, 784)</span></span><br><span class="line"><span class="string">        shared_y.get_value().shape   float32(50000,)   </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        y_cast = T.cast(shared_y, &quot;int8&quot;) # float32---&gt;int8</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        shared_x.type  TensorType(float32, matrix)  theano.tensor.sharedvar.TensorSharedVariable</span></span><br><span class="line"><span class="string">        shared_y.type  TensorType(float32, vector)  theano.tensor.sharedvar.TensorSharedVariable</span></span><br><span class="line"><span class="string">        y_cast.type    TensorType(int32, vector)    theano.tensor.var.TensorVariable  (y_cast不是shared变量)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 默认floatX = float64,在运行的时候需要设置floatX = float32</span></span><br><span class="line">        <span class="comment"># 取x[N,784],y[N]的前training_set_size个样本参与训练</span></span><br><span class="line">        shared_x = theano.shared(np.asarray(data[<span class="number">0</span>][:training_set_size,],dtype=theano.config.floatX), borrow=<span class="literal">True</span>)  </span><br><span class="line">        shared_y = theano.shared(np.asarray(data[<span class="number">1</span>][:training_set_size], dtype=theano.config.floatX), borrow=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># shared变量中的数据在GPU上必须是float32类型，但是计算阶段可能需要int类型(y)，所以需要将float32---&gt;int.</span></span><br><span class="line">        <span class="comment"># 并且int8类型需要和　self.y = T.bvector(&quot;y&quot;)的b类型一样。</span></span><br><span class="line">        <span class="comment"># When storing data on the GPU it has to be stored as floats</span></span><br><span class="line">        <span class="comment"># therefore we will store the labels as ``floatX`` as well</span></span><br><span class="line">        <span class="comment"># (``shared_y`` does exactly that). But during our computations</span></span><br><span class="line">        <span class="comment"># we need them as ints (we use labels as index, and if they are</span></span><br><span class="line">        <span class="comment"># floats it doesn&#x27;t make sense) therefore instead of returning</span></span><br><span class="line">        <span class="comment"># ``shared_y`` we will have to cast it to int. This little hack</span></span><br><span class="line">        <span class="comment"># lets us get around this issue</span></span><br><span class="line">        <span class="keyword">return</span> shared_x, T.cast(shared_y, <span class="string">&#x27;int8&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> [shared(training_data), shared(validation_data), shared(test_data)]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_expanded</span>(<span class="params">filename=<span class="string">&quot;../data/mnist_expanded.pkl.gz&quot;</span>,training_set_size=<span class="number">1000</span></span>):</span><br><span class="line">    <span class="keyword">return</span> load_data_shared(filename=filename,training_set_size=training_set_size)</span><br><span class="line"></span><br><span class="line"><span class="comment">#### Main class used to construct and train networks</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Network</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layers, mini_batch_size</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Takes a list of `layers`, describing the network architecture, and</span></span><br><span class="line"><span class="string">        a value for the `mini_batch_size` to be used during training</span></span><br><span class="line"><span class="string">        by stochastic gradient descent.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.layers = layers</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.layers)&gt;=<span class="number">2</span></span><br><span class="line">        <span class="variable language_">self</span>.mini_batch_size = mini_batch_size</span><br><span class="line">        <span class="variable language_">self</span>.params = [param <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="variable language_">self</span>.layers <span class="keyword">for</span> param <span class="keyword">in</span> layer.params]</span><br><span class="line">        <span class="variable language_">self</span>.x = T.matrix(<span class="string">&quot;x&quot;</span>)  <span class="comment"># batch x  float32,(m,784) 不需要指定fmatrix</span></span><br><span class="line">        <span class="variable language_">self</span>.y = T.bvector(<span class="string">&quot;y&quot;</span>)  <span class="comment"># batch y   int8,(m,)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># first layer init with inpt=x,inpt_dropout=x</span></span><br><span class="line">        init_layer = <span class="variable language_">self</span>.layers[<span class="number">0</span>]</span><br><span class="line">        init_layer.set_inpt(<span class="variable language_">self</span>.x, <span class="variable language_">self</span>.x, <span class="variable language_">self</span>.mini_batch_size)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> xrange(<span class="number">1</span>, <span class="built_in">len</span>(<span class="variable language_">self</span>.layers)):</span><br><span class="line">            prev_layer, layer  = <span class="variable language_">self</span>.layers[j-<span class="number">1</span>], <span class="variable language_">self</span>.layers[j]</span><br><span class="line">            layer.set_inpt(prev_layer.output, prev_layer.output_dropout, <span class="variable language_">self</span>.mini_batch_size)</span><br><span class="line">            </span><br><span class="line">        <span class="variable language_">self</span>.output = <span class="variable language_">self</span>.layers[-<span class="number">1</span>].output</span><br><span class="line">        <span class="variable language_">self</span>.output_dropout = <span class="variable language_">self</span>.layers[-<span class="number">1</span>].output_dropout</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">SGD</span>(<span class="params">self, training_data, epochs, mini_batch_size, eta,</span></span><br><span class="line"><span class="params">            validation_data, test_data, lmbda=<span class="number">0.0</span>,</span></span><br><span class="line"><span class="params">            no_improvement_in_n=<span class="number">20</span>,use_constant_eta=<span class="literal">True</span>, <span class="comment"># default not vary eta because accuracy not imporved too much</span></span></span><br><span class="line"><span class="params">            eta_shrink_times=<span class="number">10</span>,eta_descrease_factor = <span class="number">0.0001</span></span>):</span><br><span class="line">        </span><br><span class="line">        <span class="string">&quot;&quot;&quot;Train the network using mini-batch stochastic gradient descent.&quot;&quot;&quot;</span></span><br><span class="line">        training_x, training_y = training_data       <span class="comment"># (N,784) (N,)</span></span><br><span class="line">        validation_x, validation_y = validation_data</span><br><span class="line">        test_x, test_y = test_data</span><br><span class="line"></span><br><span class="line">        <span class="comment"># compute number of minibatches for training, validation and testing</span></span><br><span class="line">        num_training_batches = size(training_data)/mini_batch_size</span><br><span class="line">        num_validation_batches = size(validation_data)/mini_batch_size</span><br><span class="line">        num_test_batches = size(test_data)/mini_batch_size</span><br><span class="line"></span><br><span class="line">        <span class="comment"># define the (regularized) cost function, symbolic gradients, and updates</span></span><br><span class="line">        l2_norm_squared = <span class="built_in">sum</span>([(layer.w**<span class="number">2</span>).<span class="built_in">sum</span>() <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="variable language_">self</span>.layers])</span><br><span class="line">        cost0 = <span class="variable language_">self</span>.layers[-<span class="number">1</span>].cost(<span class="variable language_">self</span>) <span class="comment"># 计算最后一层的输出代价，传递Network作为net参数</span></span><br><span class="line">        </span><br><span class="line">        cost = cost0 + <span class="number">0.5</span>*lmbda*l2_norm_squared/size(training_data)  <span class="comment"># ??? N instead of num_training_batches</span></span><br><span class="line">        grads = T.grad(cost, <span class="variable language_">self</span>.params)</span><br><span class="line">        </span><br><span class="line">        shared_eta = theano.shared(eta,borrow=<span class="literal">True</span>) <span class="comment">#(same as shared_b) use SharedVariable instead of value</span></span><br><span class="line">        </span><br><span class="line">        updates = [(param, param-T.cast(shared_eta*grad,dtype=theano.config.floatX)) <span class="keyword">for</span> param, grad <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="variable language_">self</span>.params, grads)] </span><br><span class="line">        </span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        grad(float32),没有指定floatX=float32,则eta*grad(float64),指定之后eta*grad(float32)，无需cast</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        #for param, grad in zip(self.params, grads):</span></span><br><span class="line"><span class="string">        #    print param.type,grad.type,(eta*grad).type</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        # updates = [(param, T.cast(param-eta*grad,&#x27;float32&#x27;) ) for param, grad in zip(self.params, grads)]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># define functions to train a mini-batch, and to compute the</span></span><br><span class="line">        <span class="comment"># accuracy in validation and test mini-batches.</span></span><br><span class="line">        i = T.lscalar() <span class="comment"># mini-batch index</span></span><br><span class="line">        train_mb = theano.function(</span><br><span class="line">            [i], cost, updates=updates, <span class="comment"># 给定i,===&gt;x,y===&gt;cost中的x,y被替换掉，从而计算mini-batch的代价，最后updates</span></span><br><span class="line">            givens=&#123;</span><br><span class="line">                <span class="variable language_">self</span>.x:</span><br><span class="line">                training_x[i*<span class="variable language_">self</span>.mini_batch_size: (i+<span class="number">1</span>)*<span class="variable language_">self</span>.mini_batch_size],</span><br><span class="line">                <span class="variable language_">self</span>.y:</span><br><span class="line">                training_y[i*<span class="variable language_">self</span>.mini_batch_size: (i+<span class="number">1</span>)*<span class="variable language_">self</span>.mini_batch_size]</span><br><span class="line">            &#125;)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># cost and accuracy for train,val,test</span></span><br><span class="line">        <span class="comment"># (1) train</span></span><br><span class="line">        train_mb_cost = theano.function(</span><br><span class="line">            [i], cost,</span><br><span class="line">            givens=&#123;</span><br><span class="line">                <span class="variable language_">self</span>.x:</span><br><span class="line">                training_x[i*<span class="variable language_">self</span>.mini_batch_size: (i+<span class="number">1</span>)*<span class="variable language_">self</span>.mini_batch_size],</span><br><span class="line">                <span class="variable language_">self</span>.y:</span><br><span class="line">                training_y[i*<span class="variable language_">self</span>.mini_batch_size: (i+<span class="number">1</span>)*<span class="variable language_">self</span>.mini_batch_size]</span><br><span class="line">            &#125;) </span><br><span class="line">        train_mb_accuracy = theano.function(</span><br><span class="line">            [i], <span class="variable language_">self</span>.layers[-<span class="number">1</span>].accuracy(<span class="variable language_">self</span>.y), <span class="comment"># y(m,)</span></span><br><span class="line">            givens=&#123;</span><br><span class="line">                <span class="variable language_">self</span>.x:</span><br><span class="line">                training_x[i*<span class="variable language_">self</span>.mini_batch_size: (i+<span class="number">1</span>)*<span class="variable language_">self</span>.mini_batch_size],</span><br><span class="line">                <span class="variable language_">self</span>.y:</span><br><span class="line">                training_y[i*<span class="variable language_">self</span>.mini_batch_size: (i+<span class="number">1</span>)*<span class="variable language_">self</span>.mini_batch_size]</span><br><span class="line">            &#125;) </span><br><span class="line">        <span class="comment"># (2) val</span></span><br><span class="line">        validate_mb_cost = theano.function(</span><br><span class="line">            [i], cost,</span><br><span class="line">            givens=&#123;</span><br><span class="line">                <span class="variable language_">self</span>.x:</span><br><span class="line">                validation_x[i*<span class="variable language_">self</span>.mini_batch_size: (i+<span class="number">1</span>)*<span class="variable language_">self</span>.mini_batch_size],</span><br><span class="line">                <span class="variable language_">self</span>.y:</span><br><span class="line">                validation_y[i*<span class="variable language_">self</span>.mini_batch_size: (i+<span class="number">1</span>)*<span class="variable language_">self</span>.mini_batch_size]</span><br><span class="line">            &#125;)</span><br><span class="line">        validate_mb_accuracy = theano.function(</span><br><span class="line">            [i], <span class="variable language_">self</span>.layers[-<span class="number">1</span>].accuracy(<span class="variable language_">self</span>.y), <span class="comment"># y(m,)</span></span><br><span class="line">            givens=&#123;</span><br><span class="line">                <span class="variable language_">self</span>.x:</span><br><span class="line">                validation_x[i*<span class="variable language_">self</span>.mini_batch_size: (i+<span class="number">1</span>)*<span class="variable language_">self</span>.mini_batch_size],</span><br><span class="line">                <span class="variable language_">self</span>.y:</span><br><span class="line">                validation_y[i*<span class="variable language_">self</span>.mini_batch_size: (i+<span class="number">1</span>)*<span class="variable language_">self</span>.mini_batch_size]</span><br><span class="line">            &#125;)</span><br><span class="line">        <span class="comment"># (3) test</span></span><br><span class="line">        test_mb_cost = theano.function(</span><br><span class="line">            [i], cost,</span><br><span class="line">            givens=&#123;</span><br><span class="line">                <span class="variable language_">self</span>.x:</span><br><span class="line">                test_x[i*<span class="variable language_">self</span>.mini_batch_size: (i+<span class="number">1</span>)*<span class="variable language_">self</span>.mini_batch_size],</span><br><span class="line">                <span class="variable language_">self</span>.y:</span><br><span class="line">                test_y[i*<span class="variable language_">self</span>.mini_batch_size: (i+<span class="number">1</span>)*<span class="variable language_">self</span>.mini_batch_size]</span><br><span class="line">            &#125;)</span><br><span class="line">        test_mb_accuracy = theano.function(</span><br><span class="line">            [i], <span class="variable language_">self</span>.layers[-<span class="number">1</span>].accuracy(<span class="variable language_">self</span>.y), <span class="comment"># y(m,)</span></span><br><span class="line">            givens=&#123;</span><br><span class="line">                <span class="variable language_">self</span>.x:</span><br><span class="line">                test_x[i*<span class="variable language_">self</span>.mini_batch_size: (i+<span class="number">1</span>)*<span class="variable language_">self</span>.mini_batch_size],</span><br><span class="line">                <span class="variable language_">self</span>.y:</span><br><span class="line">                test_y[i*<span class="variable language_">self</span>.mini_batch_size: (i+<span class="number">1</span>)*<span class="variable language_">self</span>.mini_batch_size]</span><br><span class="line">            &#125;)</span><br><span class="line">        <span class="variable language_">self</span>.test_mb_predictions = theano.function(</span><br><span class="line">            [i], <span class="variable language_">self</span>.layers[-<span class="number">1</span>].y_out,   <span class="comment"># y(m,)　m个样本的预测结果</span></span><br><span class="line">            givens=&#123;</span><br><span class="line">                <span class="variable language_">self</span>.x:</span><br><span class="line">                test_x[i*<span class="variable language_">self</span>.mini_batch_size: (i+<span class="number">1</span>)*<span class="variable language_">self</span>.mini_batch_size]</span><br><span class="line">            &#125;)</span><br><span class="line">        </span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">         def shuffle_data(x,y):</span></span><br><span class="line"><span class="string">            seed = int(time.time()) </span></span><br><span class="line"><span class="string">            np.random.seed(seed)</span></span><br><span class="line"><span class="string">            np.random.shuffle(x)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            np.random.seed(seed)</span></span><br><span class="line"><span class="string">            np.random.shuffle(y)</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        def shuffle_training_data(training_x,training_y):</span></span><br><span class="line"><span class="string">            # CPU, OK; GPU, FAILED (在GPU中borrow失效)</span></span><br><span class="line"><span class="string">            originX = training_x.get_value(borrow=True) # shared---&gt; nparray</span></span><br><span class="line"><span class="string">            originY = training_y.get_value(borrow=True) # shared---&gt; nparray</span></span><br><span class="line"><span class="string">            shuffle_data(originX,originY)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        evaluation_costs, evaluation_accuracys = [], []</span><br><span class="line">        training_costs, training_accuracys = [], []</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># use no-improvement-in-n early stopping</span></span><br><span class="line">        <span class="comment"># 记录best_validation_accuracy,best_epoch，如果epoch-best_epoch&gt;=no_improvement_in_n,stop</span></span><br><span class="line">        best_epoch = <span class="number">0</span></span><br><span class="line">        cur_eta_shrink_times = <span class="number">0</span> <span class="comment"># if cur_eta_shrink_times&gt;=eta_shrink_times,stop</span></span><br><span class="line">        best_validation_accuracy = <span class="number">0.0</span> <span class="comment"># with gpu, numpy.float64</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> xrange(epochs):</span><br><span class="line">            <span class="comment">#random.shuffle(training_data) # for list[(x1,y1),(x2,y2),...] 此处training_data是(X,Y)</span></span><br><span class="line">            <span class="comment"># shuffle_training_data(training_x,training_y) # FAILED on GPU</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> minibatch_index <span class="keyword">in</span> xrange(num_training_batches):</span><br><span class="line">                <span class="comment"># iteration记录训练次数，每训练1000次输出一次</span></span><br><span class="line">                iteration = num_training_batches*epoch+minibatch_index</span><br><span class="line">                <span class="keyword">if</span> iteration % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&quot;Training mini-batch number &#123;0&#125;&quot;</span>.<span class="built_in">format</span>(iteration))</span><br><span class="line">                cost_ij = train_mb(minibatch_index)</span><br><span class="line">           </span><br><span class="line">            <span class="comment"># 一个epoch训练结束，训练了num_training_batches次，iterration=4999。利用w,b计算一次验证accuracy</span></span><br><span class="line">            <span class="comment">#if (iteration+1) % num_training_batches == 0:</span></span><br><span class="line">            validation_cost = np.mean( [validate_mb_cost(j) <span class="keyword">for</span> j <span class="keyword">in</span> xrange(num_validation_batches)] )</span><br><span class="line">            validation_accuracy = np.mean( [validate_mb_accuracy(j) <span class="keyword">for</span> j <span class="keyword">in</span> xrange(num_validation_batches)] )</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;\nEpoch &#123;0&#125;: validation accuracy &#123;1:.2%&#125;&quot;</span>.<span class="built_in">format</span>(epoch, validation_accuracy))</span><br><span class="line"></span><br><span class="line">            train_cost = np.mean( [train_mb_cost(j) <span class="keyword">for</span> j <span class="keyword">in</span> xrange(num_training_batches)] )</span><br><span class="line">            train_accuracy = np.mean( [train_mb_accuracy(j) <span class="keyword">for</span> j <span class="keyword">in</span> xrange(num_training_batches)] )</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># save 4 return lists</span></span><br><span class="line">            evaluation_costs.append(validation_cost)</span><br><span class="line">            evaluation_accuracys.append(validation_accuracy)</span><br><span class="line">            training_costs.append(train_cost)</span><br><span class="line">            training_accuracys.append(train_accuracy)</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#记录best_validation_accuracy</span></span><br><span class="line">            <span class="comment"># 关键在于&lt;,满足足够多的NIIN,才能满足eta_shrink_times&gt;=10</span></span><br><span class="line">            <span class="keyword">if</span> best_validation_accuracy - validation_accuracy &lt; <span class="number">0.0</span>:  <span class="comment"># &lt;=</span></span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;This is the best validation accuracy to date.&quot;</span>)</span><br><span class="line">                best_validation_accuracy = validation_accuracy</span><br><span class="line">                best_epoch = epoch</span><br><span class="line">                best_iteration = iteration</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># save best network</span></span><br><span class="line">                best_net = copy.deepcopy(<span class="variable language_">self</span>)</span><br><span class="line">                </span><br><span class="line">                <span class="comment">#计算在val取得最佳accuracy情况下，test数据集的accuracy</span></span><br><span class="line">                <span class="keyword">if</span> test_data:</span><br><span class="line">                    test_accuracy = np.mean( [test_mb_accuracy(j) <span class="keyword">for</span> j <span class="keyword">in</span> xrange(num_test_batches)] )</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&#x27;The corresponding test accuracy is &#123;0:.2%&#125;&#x27;</span>.<span class="built_in">format</span>(test_accuracy))</span><br><span class="line">                    </span><br><span class="line">            <span class="comment">#============================================================================================</span></span><br><span class="line">            <span class="comment"># early stopping with variable learning rate</span></span><br><span class="line">            <span class="comment"># (1) (epoch - best_epoch) &gt;= no_improvement_in_n: stop   NIIN = 20</span></span><br><span class="line">            <span class="comment"># (2) new_eta = 1/2*eta until new_eta&lt;=1/1024*eta         ETA_SHRINK_TIME = 10 </span></span><br><span class="line">            <span class="comment">#============================================================================================</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># check in last epoch of NIIN stage</span></span><br><span class="line">            <span class="keyword">if</span> (epoch+<span class="number">1</span>) % no_improvement_in_n == <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># (1) check NIIN</span></span><br><span class="line">                <span class="keyword">if</span> (epoch - best_epoch) &gt;= no_improvement_in_n:</span><br><span class="line">                    <span class="comment"># stop learning</span></span><br><span class="line">                    <span class="built_in">print</span> <span class="string">&#x27;!&#x27;</span>*<span class="number">100</span></span><br><span class="line">                    <span class="built_in">print</span> <span class="string">&#x27;[HIT] Early stopping at epoch #&#123;0&#125;,best_epoch #&#123;1&#125;,iteration #&#123;2&#125;,validation accuracy &#123;3:.2%&#125;&#x27;</span>.<span class="built_in">format</span>(epoch,best_epoch,best_iteration,best_validation_accuracy)</span><br><span class="line">                    <span class="built_in">print</span> <span class="string">&#x27;!&#x27;</span>*<span class="number">100</span></span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment">#******************************************************************************</span></span><br><span class="line">                    <span class="keyword">if</span> use_constant_eta:</span><br><span class="line">                        <span class="keyword">break</span> <span class="comment"># goto (2) instead of break</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        <span class="comment"># (2) shrink eta to 1/2*eta　　(accuracy not improved too much)</span></span><br><span class="line">                        <span class="built_in">print</span> <span class="string">&#x27;cur_eta_shrink_times = &#123;0&#125;&#x27;</span>.<span class="built_in">format</span>(cur_eta_shrink_times)</span><br><span class="line">                        <span class="keyword">if</span> cur_eta_shrink_times &gt;= eta_shrink_times:</span><br><span class="line">                            <span class="built_in">print</span> <span class="string">&#x27;+&#x27;</span>*<span class="number">100</span></span><br><span class="line">                            <span class="built_in">print</span> <span class="string">&#x27;[HIT] Eta shrink OK. at epoch #&#123;0&#125;,best_epoch #&#123;1&#125;,iteration #&#123;2&#125;,validation accuracy &#123;3:.2%&#125;&#x27;</span>.<span class="built_in">format</span>(epoch,best_epoch,best_iteration,best_validation_accuracy)</span><br><span class="line">                            <span class="built_in">print</span> <span class="string">&#x27;+&#x27;</span>*<span class="number">100</span></span><br><span class="line">                            <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">                        cur_eta_shrink_times +=<span class="number">1</span> </span><br><span class="line"></span><br><span class="line">                        <span class="comment"># update eta every epoch</span></span><br><span class="line">                        eta_descrease_factor = <span class="number">0.0001</span></span><br><span class="line">                        new_eta = eta/(<span class="number">1.0</span>+eta_descrease_factor*(epoch+<span class="number">1</span>))</span><br><span class="line">                        shared_eta.set_value(np.asarray(new_eta,dtype=theano.config.floatX),borrow=<span class="literal">True</span>) <span class="comment"># update eta</span></span><br><span class="line"></span><br><span class="line">                        <span class="comment">#eta = eta/2.0 </span></span><br><span class="line">                        <span class="comment">#shared_eta.set_value(np.asarray(eta,dtype=theano.config.floatX),borrow=True) # update eta</span></span><br><span class="line">                    <span class="comment">#******************************************************************************</span></span><br><span class="line">            <span class="comment">#============================================================================================</span></span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">        <span class="comment"># once early stopping, we save the best model to file</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;best_model.pkl&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            <span class="built_in">print</span> <span class="string">&#x27;Saving best mode to best_model.pkl...&#x27;</span></span><br><span class="line">            cPickle.dump(best_net, fp)</span><br><span class="line">                    </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\nFinished training network.&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Best validation accuracy of &#123;0:.2%&#125; obtained at best_epoch &#123;1&#125;&quot;</span>.<span class="built_in">format</span>(best_validation_accuracy, best_epoch))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Corresponding test accuracy of &#123;0:.2%&#125;&quot;</span>.<span class="built_in">format</span>(test_accuracy))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> evaluation_costs, evaluation_accuracys, training_costs, training_accuracys,best_epoch <span class="comment"># for plot</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#********************************************************</span></span><br><span class="line"><span class="comment"># load model and predict on test data</span></span><br><span class="line"><span class="comment">#********************************************************</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_network_and_predict</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    An example of how to load a trained model and use it</span></span><br><span class="line"><span class="string">    to predict labels.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># load the saved model</span></span><br><span class="line">    net = cPickle.load(<span class="built_in">open</span>(<span class="string">&#x27;best_model.pkl&#x27;</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># predict</span></span><br><span class="line">    training_set_size = <span class="number">50000</span></span><br><span class="line">    train_data,val_data,test_data = load_data_shared(training_set_size=training_set_size)</span><br><span class="line">    test_x,test_y = test_data</span><br><span class="line">    </span><br><span class="line">    mini_batch_size = <span class="number">10</span></span><br><span class="line">    num_test_batches = size(test_data)/mini_batch_size </span><br><span class="line">    </span><br><span class="line">    i = T.lscalar()</span><br><span class="line">    <span class="comment"># test predict</span></span><br><span class="line">    test_mb_predictions = theano.function(</span><br><span class="line">        [i], net.layers[-<span class="number">1</span>].y_out,   <span class="comment"># y(m,)　m个样本的预测结果</span></span><br><span class="line">        givens=&#123;</span><br><span class="line">            net.x:</span><br><span class="line">            test_x[i*mini_batch_size: (i+<span class="number">1</span>)*mini_batch_size]</span><br><span class="line">        &#125;)</span><br><span class="line">    <span class="comment"># test accuracy</span></span><br><span class="line">    test_mb_accuracy = theano.function(</span><br><span class="line">        [i], net.layers[-<span class="number">1</span>].accuracy(net.y), <span class="comment"># y(m,)</span></span><br><span class="line">        givens=&#123;</span><br><span class="line">            net.x:</span><br><span class="line">            test_x[i*mini_batch_size: (i+<span class="number">1</span>)*mini_batch_size],</span><br><span class="line">            net.y:</span><br><span class="line">            test_y[i*mini_batch_size: (i+<span class="number">1</span>)*mini_batch_size]</span><br><span class="line">        &#125;)</span><br><span class="line">    </span><br><span class="line">    test_predictions = test_mb_predictions(<span class="number">0</span>)</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&#x27;real values of first 10: &#x27;</span>,test_y[:<span class="number">10</span>].<span class="built_in">eval</span>()</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&#x27;predictions of first 10: &#x27;</span>,test_predictions</span><br><span class="line">    </span><br><span class="line">    test_accuracy = np.mean( [test_mb_accuracy(j) <span class="keyword">for</span> j <span class="keyword">in</span> xrange(num_test_batches)] )</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&#x27;test_accuracy &#x27;</span>,test_accuracy</span><br><span class="line">    </span><br><span class="line"><span class="comment">#********************************************************</span></span><br><span class="line"><span class="comment"># end of predict</span></span><br><span class="line"><span class="comment">#********************************************************</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#### Define layer types</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ConvPoolLayer</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Used to create a combination of a convolutional and a max-pooling</span></span><br><span class="line"><span class="string">    layer.  A more sophisticated implementation would separate the</span></span><br><span class="line"><span class="string">    two, but for our purposes we&#x27;ll always use them together, and it</span></span><br><span class="line"><span class="string">    simplifies the code, so it makes sense to combine them.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, filter_shape, image_shape, poolsize=(<span class="params"><span class="number">2</span>, <span class="number">2</span></span>),</span></span><br><span class="line"><span class="params">                 activation_fn=sigmoid</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;`filter_shape` is a tuple of length 4, whose entries are the number</span></span><br><span class="line"><span class="string">        of filters, the number of input feature maps, the filter height, and the</span></span><br><span class="line"><span class="string">        filter width.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        `image_shape` is a tuple of length 4, whose entries are the</span></span><br><span class="line"><span class="string">        mini-batch size, the number of input feature maps, the image</span></span><br><span class="line"><span class="string">        height, and the image width.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `poolsize` is a tuple of length 2, whose entries are the y and</span></span><br><span class="line"><span class="string">        x pooling sizes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        np.prod((2,2)) = 4 # int64</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        ConvPoolLayer1</span></span><br><span class="line"><span class="string">        image_shape=(m,1,28,28)  1*28*28   (1 input feature map)</span></span><br><span class="line"><span class="string">        filter_shape=(20,1,5,5)  20*24*24 </span></span><br><span class="line"><span class="string">        poolsize=(2,2)           20*12*12 </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        ConvPoolLayer2</span></span><br><span class="line"><span class="string">        image_shape=(m,20,12,12) 20*12*12  (20 input feature map)</span></span><br><span class="line"><span class="string">        filter_shape=(40,20,5,5) 40*8*8</span></span><br><span class="line"><span class="string">        poolsize=(2,2)           40*4*4</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        ConvPoolLayer1</span></span><br><span class="line"><span class="string">        (20,1,5,5) </span></span><br><span class="line"><span class="string">        20指定当前ConvLayer1的features的数量: c1_f1,c1_f2,....c1_f19,c1_f20。</span></span><br><span class="line"><span class="string">        (1,5,5)指定feature的一个pixel所对应的local receptive field(LRF),此处对应1个input feature的5*5区域。</span></span><br><span class="line"><span class="string">        对应的w: w1,w2,...w19,w20 of size(1,5,5)===&gt;w(20,1,5,5) filter_shape</span></span><br><span class="line"><span class="string">        对应的b: b1,b2,...b19,b20 of size()     ===&gt;b(20,)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        ConvPoolLayer2</span></span><br><span class="line"><span class="string">        (40,20,5,5) </span></span><br><span class="line"><span class="string">        40指定当前ConvLayer2的features的数量: c2_f1,c2_f2,....c2_f39,c2_f40。</span></span><br><span class="line"><span class="string">        (20,5,5)指定feature的一个pixel所对应的local receptive field(LRF),此处对应20个input feature的5*5区域。</span></span><br><span class="line"><span class="string">        对应的w: w1,w2,...w39,w40 of size(20,5,5)===&gt;w(40,20,5,5) filter_shape</span></span><br><span class="line"><span class="string">        对应的b: b1,b2,...b39,b40 of size()     ===&gt;b(40,)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">assert</span> image_shape[<span class="number">1</span>] == filter_shape[<span class="number">1</span>] <span class="comment"># input feature maps</span></span><br><span class="line">        <span class="variable language_">self</span>.filter_shape = filter_shape</span><br><span class="line">        <span class="variable language_">self</span>.image_shape = image_shape</span><br><span class="line">        <span class="variable language_">self</span>.poolsize = poolsize</span><br><span class="line">        <span class="variable language_">self</span>.activation_fn=activation_fn</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># initialize weights and biases</span></span><br><span class="line">        <span class="comment"># 20*(5*5)/(2*2) = 500/4 = 125</span></span><br><span class="line">        <span class="comment"># 40*(5*5)/(2*2) = 1000/4 = 250</span></span><br><span class="line">        <span class="comment">#n_out = (filter_shape[0]*np.prod(filter_shape[2:])/np.prod(poolsize)) # 125  250 (why???)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># for tanh: w_bound = numpy.sqrt(6./(n_in+n_out))</span></span><br><span class="line">        <span class="comment"># for sigmoid: w_bound = 4*w_bound(tanh)</span></span><br><span class="line">        <span class="comment"># for ReLU: w = 0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># there are &quot;num input feature maps * filter height * filter width&quot; inputs to each hidden unit</span></span><br><span class="line">        n_in = np.prod(filter_shape[<span class="number">1</span>:]) <span class="comment"># LRF</span></span><br><span class="line">        <span class="comment"># each unit in the lower layer receives a gradient from:</span></span><br><span class="line">        <span class="comment"># &quot;num output feature maps * filter height * filter width&quot; / pooling size</span></span><br><span class="line">        n_out = (filter_shape[<span class="number">0</span>] * np.prod(filter_shape[<span class="number">2</span>:]) // np.prod(poolsize))</span><br><span class="line">        </span><br><span class="line">        w_bound = np.sqrt(<span class="number">6.</span>/(n_in+n_out))</span><br><span class="line">        <span class="keyword">if</span> activation_fn == sigmoid:</span><br><span class="line">            w_bound = <span class="number">4</span>*w_bound</span><br><span class="line">            </span><br><span class="line">        <span class="variable language_">self</span>.w = theano.shared(</span><br><span class="line">            np.asarray(</span><br><span class="line">                <span class="comment">#np.random.normal(loc=0, scale=np.sqrt(1.0/n_out), </span></span><br><span class="line">                np.random.uniform(low=-w_bound,high=w_bound, </span><br><span class="line">                                 size=filter_shape), </span><br><span class="line">                <span class="comment"># w(20,1,5,5) w(40,20,5,5)</span></span><br><span class="line">                dtype=theano.config.floatX),</span><br><span class="line">            borrow=<span class="literal">True</span>) </span><br><span class="line">        <span class="variable language_">self</span>.b = theano.shared(</span><br><span class="line">            np.asarray(</span><br><span class="line">                np.random.normal(loc=<span class="number">0</span>, scale=<span class="number">1.0</span>, size=(filter_shape[<span class="number">0</span>],)), </span><br><span class="line">                <span class="comment"># b(20,) b(40,)</span></span><br><span class="line">                dtype=theano.config.floatX),</span><br><span class="line">            borrow=<span class="literal">True</span>)</span><br><span class="line">        <span class="variable language_">self</span>.params = [<span class="variable language_">self</span>.w, <span class="variable language_">self</span>.b]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">set_inpt</span>(<span class="params">self, inpt, inpt_dropout, mini_batch_size</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        inpt = x:  fmatrix(m,784)</span></span><br><span class="line"><span class="string">        ConvPoolLayer1</span></span><br><span class="line"><span class="string">        image_shape=(m,1,28,28)  m,1*28*28   (1 input feature map)</span></span><br><span class="line"><span class="string">        filter_shape=(20,1,5,5)  m,20*24*24   w(20,1,5,5) b(20,)</span></span><br><span class="line"><span class="string">        poolsize=(2,2)           m,20*12*12 </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        ConvPoolLayer2</span></span><br><span class="line"><span class="string">        image_shape=(m,20,12,12) m,20*12*12  (20 input feature map)</span></span><br><span class="line"><span class="string">        filter_shape=(40,20,5,5) m,40*8*8     w(40,20,5,5) b(40,)</span></span><br><span class="line"><span class="string">        poolsize=(2,2)           m,40*4*4</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        ConvPoolLayer1</span></span><br><span class="line"><span class="string">        inpt(m,784)---&gt;inpt(m,1,28,28)</span></span><br><span class="line"><span class="string">        conv_out(m,20,24,24)</span></span><br><span class="line"><span class="string">        pooled_out(m,20,12,12)</span></span><br><span class="line"><span class="string">        output(m,20,12,12)</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        ConvPoolLayer2</span></span><br><span class="line"><span class="string">        inpt(m,20,12,12)</span></span><br><span class="line"><span class="string">        conv_out(m,40,8,8)</span></span><br><span class="line"><span class="string">        pooled_out(m,40,4,4)</span></span><br><span class="line"><span class="string">        output(m,40,4,4)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.inpt = inpt.reshape(<span class="variable language_">self</span>.image_shape)</span><br><span class="line">        conv_out = conv.conv2d( <span class="built_in">input</span>=<span class="variable language_">self</span>.inpt, image_shape=<span class="variable language_">self</span>.image_shape, </span><br><span class="line">                               filters=<span class="variable language_">self</span>.w, filter_shape=<span class="variable language_">self</span>.filter_shape) </span><br><span class="line">        </span><br><span class="line">        <span class="comment">#conv_out = conv.conv2d(input=self.inpt,filters=self.w) </span></span><br><span class="line">        <span class="comment">#theano.tensor.var.TensorVariable float32 TensorType(float32, 4D)</span></span><br><span class="line">        </span><br><span class="line">        pooled_out = pool_2d( <span class="built_in">input</span>=conv_out, ws=<span class="variable language_">self</span>.poolsize, ignore_border=<span class="literal">True</span>) </span><br><span class="line">        <span class="comment">#theano.tensor.var.TensorVariable float32 TensorType(float32, 4D)</span></span><br><span class="line">        </span><br><span class="line">        b_shuffle = <span class="variable language_">self</span>.b.dimshuffle(<span class="string">&#x27;x&#x27;</span>, <span class="number">0</span>, <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;x&#x27;</span>)  </span><br><span class="line">        <span class="comment"># TensorVariable TensorType(float32, (True, False, True, True))</span></span><br><span class="line">        <span class="comment"># ConvPoolLayer1: b(20,) 20个feature map分别增加b0,b1,...b19,b20</span></span><br><span class="line">        <span class="comment"># 对于pooled_out=(m,20,12,12)而言，(&#x27;x&#x27;, 0, &#x27;x&#x27;, &#x27;x&#x27;)的dim2=0，其他为x</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ConvPoolLayer2: b(40,) 40个feature map分别增加b0,b1,...b39,b40</span></span><br><span class="line">        <span class="comment"># 对于pooled_out=(m,40,4,4)而言，(&#x27;x&#x27;, 0, &#x27;x&#x27;, &#x27;x&#x27;)的dim2=0，其他为x</span></span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.output = <span class="variable language_">self</span>.activation_fn( pooled_out + b_shuffle )</span><br><span class="line">        <span class="comment">#theano.tensor.var.TensorVariable float32 TensorType(float32, 4D)</span></span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.output_dropout = <span class="variable language_">self</span>.output <span class="comment"># no dropout in the convolutional layers</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FullyConnectedLayer</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_in, n_out, activation_fn=sigmoid, p_dropout=<span class="number">0.0</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.n_in = n_in</span><br><span class="line">        <span class="variable language_">self</span>.n_out = n_out</span><br><span class="line">        <span class="variable language_">self</span>.activation_fn = activation_fn</span><br><span class="line">        <span class="variable language_">self</span>.p_dropout = p_dropout</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#rng = numpy.random.RandomState(1234) # for w initialization</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># for tanh: w_bound = numpy.sqrt(6./(n_in+n_out))</span></span><br><span class="line">        <span class="comment"># for sigmoid: w_bound = 4*w_bound(tanh)</span></span><br><span class="line">        <span class="comment"># for ReLU: w = 0</span></span><br><span class="line">        </span><br><span class="line">        w_bound = np.sqrt(<span class="number">6.</span>/(n_in+n_out))</span><br><span class="line">        <span class="keyword">if</span> activation_fn == sigmoid:</span><br><span class="line">            w_bound = <span class="number">4</span>*w_bound</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Initialize weights and biases</span></span><br><span class="line">        <span class="variable language_">self</span>.w = theano.shared(</span><br><span class="line">            np.asarray(</span><br><span class="line">                <span class="comment">#np.random.normal(loc=0.0, scale=np.sqrt(1.0/n_in),</span></span><br><span class="line">                np.random.uniform(low=-w_bound,high=w_bound,          </span><br><span class="line">                size=(n_in, n_out)),</span><br><span class="line">                dtype=theano.config.floatX),</span><br><span class="line">            name=<span class="string">&#x27;w&#x27;</span>, borrow=<span class="literal">True</span>)</span><br><span class="line">        <span class="variable language_">self</span>.b = theano.shared(</span><br><span class="line">            np.asarray(np.random.normal(loc=<span class="number">0.0</span>, scale=<span class="number">1.0</span>, size=(n_out,)),</span><br><span class="line">                       dtype=theano.config.floatX),</span><br><span class="line">            name=<span class="string">&#x27;b&#x27;</span>, borrow=<span class="literal">True</span>)</span><br><span class="line">        <span class="variable language_">self</span>.params = [<span class="variable language_">self</span>.w, <span class="variable language_">self</span>.b]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">set_inpt</span>(<span class="params">self, inpt, inpt_dropout, mini_batch_size</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        (1) inpt,output for validating and testing</span></span><br><span class="line"><span class="string">        (2) inpt_dropout,output_dropout for training (output_dropout---&gt;[cost]---&gt;grad---&gt;params)</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        以 ConvPoolLayer1(m,20,12,12),ConvPoolLayer2(m,40,4,4),[640,30,10]网络结构为例说明：</span></span><br><span class="line"><span class="string">        ************************************************************************************************</span></span><br><span class="line"><span class="string">        X(m,784),Y(m,)</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        ConvPoolLayer1:</span></span><br><span class="line"><span class="string">        当前层的inpt是前一层的output，因为是第一层，所以初始化为inpt = X(m,784)</span></span><br><span class="line"><span class="string">        inpt(m,784)---&gt;inpt(m,1,28,28)</span></span><br><span class="line"><span class="string">        conv_out(m,20,24,24)</span></span><br><span class="line"><span class="string">        pooled_out(m,20,12,12)</span></span><br><span class="line"><span class="string">        output(m,20,12,12)</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        ConvPoolLayer2:</span></span><br><span class="line"><span class="string">        inpt(m,20,12,12)</span></span><br><span class="line"><span class="string">        conv_out(m,40,8,8)</span></span><br><span class="line"><span class="string">        pooled_out(m,40,4,4)</span></span><br><span class="line"><span class="string">        output(m,40,4,4)</span></span><br><span class="line"><span class="string">        ************************************************************************************************</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        对于FullyConnectedLayer而言，inpt是ConvPoolLayer2的output=(m,40,4,4) </span></span><br><span class="line"><span class="string">        ================================================================================================</span></span><br><span class="line"><span class="string">        Layer1:</span></span><br><span class="line"><span class="string">        inpt=(m,40,4,4)---&gt;inpt(m,640)    a1(m,640)即：m个样本，每个样本640个neurons</span></span><br><span class="line"><span class="string">        output = sigmoid(input*w+b) ===&gt; a2 = sigmoid(a1*w+b)</span></span><br><span class="line"><span class="string">        a2(m,30) = sigmoid(  a1(m,640)* w(640,30)+ b(30,) ) </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Layer2:</span></span><br><span class="line"><span class="string">        当前层的inpt是前一层的output，即是FullyConnectedLayer1的output，包含30个hidden neurons输出 a2(m,30)</span></span><br><span class="line"><span class="string">        output = SOFTMAX(input*w+b) ===&gt; a3 = SOFTMAX(a2*w+b)</span></span><br><span class="line"><span class="string">        a3(m,10) = SOFTMAX(  a2(m,30)* w(30,10)+ b(10,) ) </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        output是m个样本对应的10个概率,y_out是m个样本对应的真实数值。</span></span><br><span class="line"><span class="string">        ================================================================================================</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.inpt = inpt.reshape((mini_batch_size, <span class="variable language_">self</span>.n_in))</span><br><span class="line">        <span class="comment">#self.output = self.activation_fn((1-self.p_dropout)*T.dot(self.inpt, self.w) + self.b)  </span></span><br><span class="line">        <span class="variable language_">self</span>.output = <span class="variable language_">self</span>.activation_fn(T.dot(<span class="variable language_">self</span>.inpt, <span class="variable language_">self</span>.w) + <span class="variable language_">self</span>.b)</span><br><span class="line">       </span><br><span class="line">        <span class="comment">#self.y_out = T.argmax(self.output, axis=1) # 暂时不用，只是用最后一层的y_out作为输出结果</span></span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.inpt_dropout = dropout_layer( inpt_dropout.reshape((mini_batch_size, <span class="variable language_">self</span>.n_in)), <span class="variable language_">self</span>.p_dropout)</span><br><span class="line">        <span class="variable language_">self</span>.output_dropout = <span class="variable language_">self</span>.activation_fn(T.dot(<span class="variable language_">self</span>.inpt_dropout, <span class="variable language_">self</span>.w) + <span class="variable language_">self</span>.b)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#def accuracy(self, y):</span></span><br><span class="line">    <span class="comment">#    &quot;Return the accuracy for the mini-batch.&quot;</span></span><br><span class="line">    <span class="comment">#    # 暂时不用，只是用最后一层</span></span><br><span class="line">    <span class="comment">#    return T.mean(T.eq(y, self.y_out))</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SoftmaxLayer</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_in, n_out, p_dropout=<span class="number">0.0</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.n_in = n_in</span><br><span class="line">        <span class="variable language_">self</span>.n_out = n_out</span><br><span class="line">        <span class="variable language_">self</span>.activation_fn = softmax <span class="comment"># default to softmax</span></span><br><span class="line">        <span class="variable language_">self</span>.p_dropout = p_dropout</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Initialize weights and biases</span></span><br><span class="line">        <span class="comment"># for sigmoid neurons,w---&gt;(0, 1/sqrt(n_in)) b---&gt;(0,1)</span></span><br><span class="line">        <span class="comment"># for softmax neurons,w = 0,b = 0, no need using suitably parameteried normal random variables</span></span><br><span class="line">        <span class="variable language_">self</span>.w = theano.shared(</span><br><span class="line">            np.zeros((n_in, n_out), dtype=theano.config.floatX),</span><br><span class="line">            name=<span class="string">&#x27;w&#x27;</span>, borrow=<span class="literal">True</span>)</span><br><span class="line">        <span class="variable language_">self</span>.b = theano.shared(</span><br><span class="line">            np.zeros((n_out,), dtype=theano.config.floatX),</span><br><span class="line">            name=<span class="string">&#x27;b&#x27;</span>, borrow=<span class="literal">True</span>)</span><br><span class="line">        <span class="variable language_">self</span>.params = [<span class="variable language_">self</span>.w, <span class="variable language_">self</span>.b]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">set_inpt</span>(<span class="params">self, inpt, inpt_dropout, mini_batch_size</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        (1) inpt,output for validating and testing</span></span><br><span class="line"><span class="string">        (2) inpt_dropout,output_dropout for training (output_dropout---&gt;[cost]---&gt;grad---&gt;params)</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        在Python中，a = sigmoid(w*a+b), w=(30,784),a=(784,1)一次使用一个样本参与计算。</span></span><br><span class="line"><span class="string">        在Theano中修改为,a = sigmoid(a*w+b) a=(m,784),w=(784,30)一次使用m个样本参与计算。</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        以[784,30,10]网络结构为例说明：</span></span><br><span class="line"><span class="string">        Layer1:</span></span><br><span class="line"><span class="string">        当前层的inpt是前一层的output，因为是第一层，所以初始化为a1 = X(m,784) Matrix，每一个样本包含784个输入neurons</span></span><br><span class="line"><span class="string">        output = sigmoid(input*w+b) ===&gt; a2 = sigmoid(a1*w+b)</span></span><br><span class="line"><span class="string">        a2(m,30) = sigmoid(  a1(m,784)* w(784,30)+ b(30,) ) </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Layer2:</span></span><br><span class="line"><span class="string">        当前层的inpt是前一层的output，即是FullyConnectedLayer的output，包含30个hidden neurons输出 a2(m,30)</span></span><br><span class="line"><span class="string">        output = SOFTMAX(input*w+b) ===&gt; a3 = SOFTMAX(a2*w+b)</span></span><br><span class="line"><span class="string">        a3(m,10) = SOFTMAX(  a2(m,30)* w(30,10)+ b(10,) ) </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        output是m个样本对应的10个概率,y_out是m个样本对应的真实数值。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.inpt = inpt.reshape((mini_batch_size, <span class="variable language_">self</span>.n_in))  <span class="comment"># tesorvariable Matrix(m,n_in)</span></span><br><span class="line">        <span class="comment">#self.output = self.activation_fn((1-self.p_dropout)*T.dot(self.inpt, self.w) + self.b)</span></span><br><span class="line">        <span class="variable language_">self</span>.output = <span class="variable language_">self</span>.activation_fn(T.dot(<span class="variable language_">self</span>.inpt, <span class="variable language_">self</span>.w) + <span class="variable language_">self</span>.b)</span><br><span class="line">        </span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        input--&gt;    output   ---&gt; y_out</span></span><br><span class="line"><span class="string">        X1---&gt; [y0,y1,...y9] ---&gt;  1</span></span><br><span class="line"><span class="string">        X2---&gt; [y0,y1,...y9] ---&gt;  0</span></span><br><span class="line"><span class="string">        ...</span></span><br><span class="line"><span class="string">        Xm---&gt; [y0,y1,...y9] ---&gt;  2</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        axis沿着row作为一个整体进行，y_out作为最终的输出=vector(m,)。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.y_out = T.argmax(<span class="variable language_">self</span>.output, axis=<span class="number">1</span>) <span class="comment"># 对应的数值 [2,1,...7]</span></span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.inpt_dropout = dropout_layer( inpt_dropout.reshape((mini_batch_size, <span class="variable language_">self</span>.n_in)), <span class="variable language_">self</span>.p_dropout)</span><br><span class="line">        <span class="variable language_">self</span>.output_dropout = <span class="variable language_">self</span>.activation_fn(T.dot(<span class="variable language_">self</span>.inpt_dropout, <span class="variable language_">self</span>.w) + <span class="variable language_">self</span>.b)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">cost</span>(<span class="params">self, net</span>):</span><br><span class="line">        <span class="string">&quot;Return the log-likelihood cost.&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        使用output_dropout用于train</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        (1) 一个样本对应的代价Cx</span></span><br><span class="line"><span class="string">        C = -log(a[i])</span></span><br><span class="line"><span class="string">        i = np.argmax(y)  # a(10,1) y(10,1)</span></span><br><span class="line"><span class="string">        return -np.log(a[i,0])</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        (2) m个样本的平均代价</span></span><br><span class="line"><span class="string">        计算代价的时候，传递Network作为参数，方便获取net.y</span></span><br><span class="line"><span class="string">       </span></span><br><span class="line"><span class="string">                output(m,10)     net.y   cost</span></span><br><span class="line"><span class="string">        X1---&gt; [y0,y1,...y9] ---&gt;  1     -log a[1,1]</span></span><br><span class="line"><span class="string">        X2---&gt; [y0,y1,...y9] ---&gt;  0     -log a[2,0]</span></span><br><span class="line"><span class="string">        Xm---&gt; [y0,y1,...y9] ---&gt;  2     -log a[m,2]</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        a = np.array([[0,   0.8, 0,   0,...],</span></span><br><span class="line"><span class="string">                      [0.9, 0,   0,   0,...],</span></span><br><span class="line"><span class="string">                      [0,   0,   0.7, 0...]])</span></span><br><span class="line"><span class="string">        y = [1,0,2]</span></span><br><span class="line"><span class="string">        a[[0,1,2],y]</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        &gt; array([ 0.8,  0.9,  0.7])</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        m = net.y.shape[<span class="number">0</span>]</span><br><span class="line">        rows = T.arange(m)</span><br><span class="line">        <span class="keyword">return</span> -T.mean(T.log( <span class="variable language_">self</span>.output_dropout[rows, net.y] ))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">self, y</span>):</span><br><span class="line">        <span class="string">&quot;Return the accuracy for the mini-batch.&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        使用output,y_out用于test</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        y(m,) 对应m个样本的真实结果</span></span><br><span class="line"><span class="string">        y_out(m,)　对应m个样本的预测结果</span></span><br><span class="line"><span class="string">        如果mini_batch_size = 5</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        y = np.array([2,1,7,8,9])</span></span><br><span class="line"><span class="string">        y_out = np.array([2,1,7,6,9])</span></span><br><span class="line"><span class="string">        np.mean(np.equal(y,y_out))  # [1,1,1,0,1] 0.80</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> T.mean(T.eq(y, <span class="variable language_">self</span>.y_out))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#### Miscellanea</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">size</span>(<span class="params">data</span>):</span><br><span class="line">    <span class="string">&quot;Return the size of the dataset `data`.&quot;</span></span><br><span class="line">    <span class="keyword">return</span> data[<span class="number">0</span>].get_value(borrow=<span class="literal">True</span>).shape[<span class="number">0</span>]  <span class="comment"># N = 50000</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dropout_layer</span>(<span class="params">layer, p_dropout</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    对于[784,30,10]</span></span><br><span class="line"><span class="string">    Layer1:</span></span><br><span class="line"><span class="string">    layer= float32 (m,784), p_dropout = 0.2,对每个节点以一定的概率进行drop</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    参考：http://www.jianshu.com/p/ba9ca3b07922</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Inverted Dropout </span></span><br><span class="line"><span class="string">    我们稍微将 Dropout 方法改进一下，使得我们只需要在训练阶段缩放激活函数的输出值，而不用在测试阶段改变什么。</span></span><br><span class="line"><span class="string">    这个改进的 Dropout 方法就被称之为 Inverted Dropout 。</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    在各种深度学习框架的实现中，我们都是用 Inverted Dropout 来代替 Dropout，因为这种方式有助于模型的完整性，</span></span><br><span class="line"><span class="string">    我们只需要修改一个参数（保留/丢弃概率），而整个模型都不用修改。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    srng = shared_randomstreams.RandomStreams( np.random.RandomState(<span class="number">0</span>).randint(<span class="number">999999</span>) )</span><br><span class="line">    retain_prob = <span class="number">1.</span> - p_dropout <span class="comment"># retain probility  theano.config.floatX</span></span><br><span class="line">    <span class="comment">#mask = srng.binomial(n=1, p=retain_prob, size=layer.shape,dtype=&#x27;int8&#x27;) # int8</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#mask: &lt;class &#x27;theano.tensor.var.TensorVariable&#x27;&gt; TensorType(float32, vector)</span></span><br><span class="line">    mask = srng.binomial(n=<span class="number">1</span>, p=retain_prob, size=layer.shape,dtype=theano.config.floatX)</span><br><span class="line">    mask_layer = layer*mask</span><br><span class="line">    <span class="keyword">return</span> mask_layer/retain_prob <span class="comment">#在train阶段除以retain_prob，以便test阶段每一个Layer的output形式保持不变。</span></span><br></pre></td></tr></table></figure>

<h2 id="Test-Network3"><a href="#Test-Network3" class="headerlink" title="Test Network3"></a>Test Network3</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">random.seed(<span class="number">12345678</span>)</span><br><span class="line">np.random.seed(<span class="number">12345678</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#from ke_network3 import *</span></span><br><span class="line">epochs = <span class="number">3</span></span><br><span class="line">training_set_size = <span class="number">100</span></span><br><span class="line">mini_batch_size = <span class="number">10</span></span><br><span class="line">train_data,val_data,test_data = load_data_shared(training_set_size=training_set_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># for conv pool layer</span></span><br><span class="line">image_shape=(mini_batch_size,<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line">filter_shape=(<span class="number">20</span>,<span class="number">1</span>,<span class="number">5</span>,<span class="number">5</span>)</span><br><span class="line">poolsize=(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">convpool_layer1 = ConvPoolLayer(image_shape=image_shape,filter_shape=filter_shape, poolsize=poolsize)</span><br><span class="line">n_in = <span class="number">20</span>*<span class="number">12</span>*<span class="number">12</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">image_shape=(mini_batch_size,<span class="number">20</span>,<span class="number">12</span>,<span class="number">12</span>)</span><br><span class="line">filter_shape=(<span class="number">40</span>,<span class="number">20</span>,<span class="number">5</span>,<span class="number">5</span>)</span><br><span class="line">poolsize=(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">n_in = <span class="number">40</span>*<span class="number">4</span>*<span class="number">4</span></span><br><span class="line">convpool_layer2 = ConvPoolLayer(image_shape=image_shape,filter_shape=filter_shape, poolsize=poolsize)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">full_layer = FullyConnectedLayer(n_in=n_in,n_out=<span class="number">30</span>)</span><br><span class="line">softmax_layer = SoftmaxLayer(n_in=<span class="number">30</span>,n_out=<span class="number">10</span>)</span><br><span class="line"><span class="comment">#net = Network([convpool_layer1,full_layer,softmax_layer],10)</span></span><br><span class="line">net = Network([convpool_layer1,convpool_layer2,full_layer,softmax_layer],<span class="number">10</span>)</span><br><span class="line">net.SGD(train_data,epochs,mini_batch_size,<span class="number">0.3</span>,val_data,test_data,lmbda=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<pre><code>updates TensorType(float32, 4D) TensorType(float32, 4D)
updates TensorType(float32, vector) TensorType(float32, vector)
updates TensorType(float32, 4D) TensorType(float32, 4D)
updates TensorType(float32, vector) TensorType(float32, vector)
updates TensorType(float32, matrix) TensorType(float32, matrix)
updates TensorType(float32, vector) TensorType(float32, vector)
updates TensorType(float32, matrix) TensorType(float32, matrix)
updates TensorType(float32, vector) TensorType(float32, vector)
Training mini-batch number 0
Epoch 0: validation accuracy 10.00%

This is the best validation accuracy to date.
The corresponding test accuracy is 8.00%
Epoch 1: validation accuracy 10.00%

This is the best validation accuracy to date.
The corresponding test accuracy is 8.00%
Epoch 2: validation accuracy 10.00%

This is the best validation accuracy to date.
The corresponding test accuracy is 8.00%

Finished training network.
Best validation accuracy of 10.00% obtained at iteration 29
Corresponding test accuracy of 8.00%





([2.2949765, 2.2951121, 2.2958748],
 [0.10000000000000001, 0.10000000000000001, 0.10000000000000001],
 [2.2682509, 2.2655275, 2.2644706],
 [0.13, 0.13, 0.13])
</code></pre>
<h2 id="Basic-Test-of-Network3-py"><a href="#Basic-Test-of-Network3-py" class="headerlink" title="Basic Test of Network3.py"></a>Basic Test of Network3.py</h2><h3 id="1-load-data"><a href="#1-load-data" class="headerlink" title="(1) load data"></a>(1) load data</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ke_network3 <span class="keyword">import</span> *</span><br><span class="line">filename=<span class="string">&quot;../data/mnist.pkl.gz&quot;</span></span><br><span class="line">filename=<span class="string">&quot;../data/mnist_expanded.pkl.gz&quot;</span></span><br><span class="line">f = gzip.<span class="built_in">open</span>(filename, <span class="string">&#x27;rb&#x27;</span>)</span><br><span class="line">training_data, validation_data, test_data = cPickle.load(f)</span><br><span class="line">f.close()</span><br><span class="line"></span><br><span class="line">x = training_data[<span class="number">0</span>] <span class="comment"># (m,784)</span></span><br><span class="line">y = training_data[<span class="number">1</span>] <span class="comment"># (m,)</span></span><br><span class="line"><span class="built_in">print</span> <span class="built_in">type</span>(x),<span class="built_in">type</span>(y)</span><br><span class="line"><span class="built_in">print</span> <span class="built_in">type</span>(x[<span class="number">0</span>]),<span class="built_in">type</span>(y[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span> x.shape,y.shape</span><br><span class="line"><span class="built_in">print</span> x[<span class="number">0</span>].shape,y[<span class="number">0</span>].shape</span><br><span class="line">x2 = x[:<span class="number">10</span>,]</span><br><span class="line"></span><br><span class="line">set_size = <span class="number">10</span></span><br><span class="line">x = training_data[<span class="number">0</span>] <span class="comment"># float32  (50000, 784)</span></span><br><span class="line">y = training_data[<span class="number">1</span>] <span class="comment"># int64  (50000,)</span></span><br><span class="line"></span><br><span class="line">training_x = theano.shared( training_data[<span class="number">0</span>][:set_size,],  borrow=<span class="literal">True</span>) <span class="comment">#float32</span></span><br><span class="line">training_y = theano.shared( np.asarray(training_data[<span class="number">0</span>][:set_size,],dtype=<span class="string">&#x27;int8&#x27;</span>),  borrow=<span class="literal">True</span>) <span class="comment"># int8</span></span><br><span class="line"><span class="comment">#training_x2 = theano.shared(np.asarray(training_data[0], dtype=theano.config.floatX), borrow=True) # float64</span></span><br><span class="line"><span class="built_in">print</span> training_x.<span class="built_in">type</span></span><br><span class="line"><span class="built_in">print</span> training_y.<span class="built_in">type</span></span><br><span class="line"><span class="comment">#print training_x2.type</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 乘法可能会改变TensorVariable的类型</span></span><br><span class="line">new_x = training_x*<span class="number">0.1</span> <span class="comment"># float32---&gt;float64</span></span><br><span class="line"><span class="built_in">print</span> training_x.<span class="built_in">type</span>,new_x.<span class="built_in">type</span></span><br></pre></td></tr></table></figure>

<pre><code>&lt;type &#39;numpy.ndarray&#39;&gt; &lt;type &#39;numpy.ndarray&#39;&gt;
&lt;type &#39;numpy.ndarray&#39;&gt; &lt;type &#39;numpy.int64&#39;&gt;
(50, 784) (50,)
(784,) ()
TensorType(float32, matrix)
TensorType(int8, matrix)
TensorType(float32, matrix) TensorType(float64, matrix)
</code></pre>
<h3 id="2-dimshuffle-b-to-match-pooled-out"><a href="#2-dimshuffle-b-to-match-pooled-out" class="headerlink" title="(2) dimshuffle b to match pooled_out"></a>(2) dimshuffle b to match pooled_out</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">pooled_out = np.arange(<span class="number">18</span>).reshape(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span> pooled_out</span><br><span class="line">b = np.array([<span class="number">0.0</span>,<span class="number">1.0</span>],dtype=<span class="string">&#x27;float32&#x27;</span>) <span class="comment"># [0,1]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># shuffle b to match pooled_out</span></span><br><span class="line">sb = theano.shared(np.asarray(b,dtype=<span class="string">&#x27;float32&#x27;</span>)) </span><br><span class="line">y = sb.dimshuffle(<span class="string">&#x27;x&#x27;</span>, <span class="number">0</span>, <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;x&#x27;</span>)  <span class="comment"># TensorVariable TensorType(float32, (True, False, True, True))</span></span><br><span class="line"><span class="comment"># 2个feature map分别增加b0,b1</span></span><br><span class="line"><span class="built_in">print</span> <span class="built_in">type</span>(y),y.<span class="built_in">type</span>,y.shape.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">b_value = y.<span class="built_in">eval</span>()</span><br><span class="line"><span class="built_in">print</span> b_value</span><br><span class="line">pooled_out + b_value</span><br></pre></td></tr></table></figure>

<pre><code>[[[[ 0  1  2]
   [ 3  4  5]
   [ 6  7  8]]

  [[ 9 10 11]
   [12 13 14]
   [15 16 17]]]]
&lt;class &#39;theano.tensor.var.TensorVariable&#39;&gt; TensorType(float32, (True, False, True, True)) [1 2 1 1]
[[[[ 0.]]

  [[ 1.]]]]





array([[[[  0.,   1.,   2.],
         [  3.,   4.,   5.],
         [  6.,   7.,   8.]],

        [[ 10.,  11.,  12.],
         [ 13.,  14.,  15.],
         [ 16.,  17.,  18.]]]])
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><h2 id="History"><a href="#History" class="headerlink" title="History"></a>History</h2><ul>
<li>20180807: created.</li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/sgd/" rel="tag"># sgd</a>
              <a href="/tags/neural-networks-and-deep-learning/" rel="tag"># neural networks and deep learning</a>
              <a href="/tags/mlp/" rel="tag"># mlp</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/blog/network2-py/" rel="prev" title="network2.py">
      <i class="fa fa-chevron-left"></i> network2.py
    </a></div>
      <div class="post-nav-item">
    <a href="/blog/theano-conv-pool-example/" rel="next" title="theano conv pool example">
      theano conv pool example <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#network3-py"><span class="nav-number">1.</span> <span class="nav-text">network3.py</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Test-Network3"><span class="nav-number">2.</span> <span class="nav-text">Test Network3</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Basic-Test-of-Network3-py"><span class="nav-number">3.</span> <span class="nav-text">Basic Test of Network3.py</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-load-data"><span class="nav-number">3.1.</span> <span class="nav-text">(1) load data</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-dimshuffle-b-to-match-pooled-out"><span class="nav-number">3.2.</span> <span class="nav-text">(2) dimshuffle b to match pooled_out</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reference"><span class="nav-number">4.</span> <span class="nav-text">Reference</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#History"><span class="nav-number">5.</span> <span class="nav-text">History</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">kezunlin</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">181</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">213</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">kezunlin</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
