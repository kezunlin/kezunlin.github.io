<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">


  <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({
      google_ad_client: "ca-pub-5653382914441020",
      enable_page_level_ads: true
    });
  </script>

  <meta name="google-site-verification" content="WSpJFvfcmIPBX_iK8uPbmCHIy_0f1lvd7ZJpbyad2sA">
  <meta name="yandex-verification" content="808eb057eb8396cc">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"kezunlin.me","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Kezunlin&#39;s Blog">
<meta property="og:url" content="https://kezunlin.me/page/11/index.html">
<meta property="og:site_name" content="Kezunlin&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="kezunlin">
<meta property="article:tag" content="linux, c++, python, AI, LLM">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://kezunlin.me/page/11/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Kezunlin's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Kezunlin's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Kezunlin's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Live and Learn</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kezunlin.me/blog/OpenCV-OCR-and-text-recognition-with-Tesseract-on-ubuntu-16-04/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="kezunlin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kezunlin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/OpenCV-OCR-and-text-recognition-with-Tesseract-on-ubuntu-16-04/" class="post-title-link" itemprop="url">OpenCV OCR and text recognition with Tesseract on ubuntu 16.04</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-09-20 14:50:00" itemprop="dateCreated datePublished" datetime="2018-09-20T14:50:00+08:00">2018-09-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-10-14 13:39:28" itemprop="dateModified" datetime="2024-10-14T13:39:28+08:00">2024-10-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/linux/" itemprop="url" rel="index"><span itemprop="name">linux</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Guide"><a href="#Guide" class="headerlink" title="Guide"></a>Guide</h2><p>requirements:</p>
<ul>
<li>ubuntu: 16.04</li>
<li>python: 3.5.2</li>
<li>opencv: 3.4.2+</li>
<li>tesseract: v4 (binary)</li>
<li>pytesseract: 0.2.4 (python bindings)</li>
</ul>
<h3 id="install-python"><a href="#install-python" class="headerlink" title="install python"></a>install python</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">apt-get install python3-dev python3-pip </span><br><span class="line">```  </span><br><span class="line"></span><br><span class="line"><span class="comment">### install opencv </span></span><br><span class="line">```bash</span><br><span class="line">workon py3 </span><br><span class="line">pip install opencv-contrib-python</span><br><span class="line">```   </span><br><span class="line"></span><br><span class="line"><span class="comment">### install tesseract</span></span><br><span class="line">```bash</span><br><span class="line"><span class="built_in">sudo</span> add-apt-repository ppa:alex-p/tesseract-ocr</span><br><span class="line"><span class="built_in">sudo</span> apt-get update</span><br><span class="line"><span class="built_in">sudo</span> apt install tesseract-ocr</span><br></pre></td></tr></table></figure>

<blockquote>
<p>The latest release of <code>Tesseract (v4)</code> supports deep learning-based OCR that is significantly more accurate.</p>
</blockquote>
<blockquote>
<p>The underlying OCR engine itself utilizes a <code>Long Short-Term Memory (LSTM) network</code>, a kind of <code>Recurrent Neural Network (RNN)</code>.</p>
</blockquote>
<p>check </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tesseract -v</span><br><span class="line">tesseract 4.0.0-beta.4-138-g2093</span><br><span class="line">  leptonica-1.76.0</span><br><span class="line">  libgif 5.1.4 : libjpeg 8d (libjpeg-turbo 1.4.2) : libpng 1.2.54 : libtiff 4.0.6 : zlib 1.2.8 : libwebp 0.4.4 : libopenjp2 2.1.2</span><br><span class="line">  Found AVX2</span><br><span class="line">  Found AVX</span><br><span class="line">  Found SSE</span><br></pre></td></tr></table></figure>

<h3 id="install-Tesseract-Python-bindings"><a href="#install-Tesseract-Python-bindings" class="headerlink" title="install Tesseract + Python bindings"></a>install Tesseract + Python bindings</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">workon py3</span><br><span class="line">pip install pytesseract</span><br><span class="line">pip install pillow imutils</span><br></pre></td></tr></table></figure>

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- kzl in-article ad -->
<p><ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-5653382914441020"
     data-ad-slot="7925631830"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h2 id="tesseract"><a href="#tesseract" class="headerlink" title="tesseract"></a>tesseract</h2><h3 id="help"><a href="#help" class="headerlink" title="help"></a>help</h3><pre><code>tesseract --help
Usage:
  tesseract --help | --help-extra | --version
  tesseract --list-langs
  tesseract imagename outputbase [options...] [configfile...]

OCR options:
  -l LANG[+LANG]        Specify language(s) used for OCR.
NOTE: These options must occur before any configfile.

Single options:
  --help                Show this help message.
  --help-extra          Show extra help for advanced users.
  --version             Show version information.
  --list-langs          List available languages for tesseract engine.
</code></pre>
<h3 id="help-extra"><a href="#help-extra" class="headerlink" title="help-extra"></a>help-extra</h3><pre><code>tesseract --help-extra
Usage:
  tesseract --help | --help-extra | --help-psm | --help-oem | --version
  tesseract --list-langs [--tessdata-dir PATH]
  tesseract --print-parameters [options...] [configfile...]
  tesseract imagename|imagelist|stdin outputbase|stdout [options...] [configfile...]

OCR options:
  --tessdata-dir PATH   Specify the location of tessdata path.
  --user-words PATH     Specify the location of user words file.
  --user-patterns PATH  Specify the location of user patterns file.
  -l LANG[+LANG]        Specify language(s) used for OCR.
  -c VAR=VALUE          Set value for config variables.
                        Multiple -c arguments are allowed.
  --psm NUM             Specify page segmentation mode.
  --oem NUM             Specify OCR Engine mode.
NOTE: These options must occur before any configfile.

Page segmentation modes:
  0    Orientation and script detection (OSD) only.
  1    Automatic page segmentation with OSD.
  2    Automatic page segmentation, but no OSD, or OCR.
  3    Fully automatic page segmentation, but no OSD. (Default)
  4    Assume a single column of text of variable sizes.
  5    Assume a single uniform block of vertically aligned text.
  6    Assume a single uniform block of text.
  7    Treat the image as a single text line.
  8    Treat the image as a single word.
  9    Treat the image as a single word in a circle.
 10    Treat the image as a single character.
 11    Sparse text. Find as much text as possible in no particular order.
 12    Sparse text with OSD.
 13    Raw line. Treat the image as a single text line,
       bypassing hacks that are Tesseract-specific.

OCR Engine modes: (see https://github.com/tesseract-ocr/tesseract/wiki#linux)
  0    Legacy engine only.
  1    Neural nets LSTM engine only.
  2    Legacy + LSTM engines.
  3    Default, based on what is available.

Single options:
  -h, --help            Show minimal help message.
  --help-extra          Show extra help for advanced users.
  --help-psm            Show page segmentation modes.
  --help-oem            Show OCR Engine modes.
  -v, --version         Show version information.
  --list-langs          List available languages for tesseract engine.
  --print-parameters    Print tesseract parameters.
</code></pre>
<h3 id="run-script"><a href="#run-script" class="headerlink" title="run script"></a>run script</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">python text_recognition.py --east frozen_east_text_detection.pb \</span><br><span class="line">    --image images/example_01.jpg</span><br><span class="line">[INFO] loading EAST text detector...</span><br><span class="line">OCR TEXT</span><br><span class="line">========</span><br><span class="line">OH OK</span><br></pre></td></tr></table></figure>

<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a target="_blank" rel="noopener" href="https://www.pyimagesearch.com/2018/09/17/opencv-ocr-and-text-recognition-with-tesseract/">OpenCV OCR and text recognition with Tesseract</a></li>
</ul>
<h2 id="History"><a href="#History" class="headerlink" title="History"></a>History</h2><ul>
<li>20180920:  created.</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kezunlin.me/blog/python-virtualenv-tutorial/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="kezunlin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kezunlin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/python-virtualenv-tutorial/" class="post-title-link" itemprop="url">python virtualenv tutorial</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-09-20 14:28:00" itemprop="dateCreated datePublished" datetime="2018-09-20T14:28:00+08:00">2018-09-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-10-14 13:39:28" itemprop="dateModified" datetime="2024-10-14T13:39:28+08:00">2024-10-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/python/" itemprop="url" rel="index"><span itemprop="name">python</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Guide"><a href="#Guide" class="headerlink" title="Guide"></a>Guide</h2><h3 id="install-python"><a href="#install-python" class="headerlink" title="install python"></a>install python</h3><p>install commands</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> apt-get install python3-pip python3-dev</span><br><span class="line"></span><br><span class="line">pip3 -V</span><br><span class="line">pip 8.1.1 from /usr/lib/python3/dist-packages (python 3.5)</span><br></pre></td></tr></table></figure>

<h3 id="change-pip-source"><a href="#change-pip-source" class="headerlink" title="change pip source"></a>change pip source</h3><h4 id="ubuntu"><a href="#ubuntu" class="headerlink" title="ubuntu"></a>ubuntu</h4><p>edit <code>.pip/pip.conf</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[global]</span><br><span class="line">index-url = http://pypi.douban.com/simple</span><br><span class="line">[install]</span><br><span class="line">trusted-host = pypi.douban.com</span><br></pre></td></tr></table></figure>

<h4 id="windows"><a href="#windows" class="headerlink" title="windows"></a>windows</h4><p>edit <code>C:\Users\zunli\AppData\Roaming\pip\pip.ini</code></p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[global]</span></span><br><span class="line"><span class="attr">index-url</span> = http://pypi.douban.com/simple</span><br><span class="line"><span class="section">[install]</span></span><br><span class="line"><span class="attr">trusted-host</span> = pypi.douban.com</span><br></pre></td></tr></table></figure>

<h3 id="temp-solutions"><a href="#temp-solutions" class="headerlink" title="temp solutions"></a>temp solutions</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple tensorflow-gpu==1.4.0</span><br></pre></td></tr></table></figure>

<h3 id="install-virtualenv"><a href="#install-virtualenv" class="headerlink" title="install virtualenv"></a>install virtualenv</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> pip3 install virtualenv virtualenvwrapper</span><br></pre></td></tr></table></figure>

<p>vim <code>.bashrc</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># for virtualenv and virtualenvwrapper</span></span><br><span class="line"><span class="built_in">export</span> WORKON_HOME=<span class="variable">$HOME</span>/.local</span><br><span class="line"><span class="built_in">export</span> VIRTUALENVWRAPPER_PYTHON=/usr/bin/python3</span><br><span class="line"><span class="built_in">source</span> /usr/local/bin/virtualenvwrapper.sh</span><br></pre></td></tr></table></figure>

<p>source <code>.bashrc</code></p>
<h4 id="mkvirtualenv"><a href="#mkvirtualenv" class="headerlink" title="mkvirtualenv"></a>mkvirtualenv</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kezunlin@ke: mkvirtualenv py3 -p python3</span><br><span class="line">(py3) kezunlin@ke:~$ </span><br></pre></td></tr></table></figure>

<h4 id="commands"><a href="#commands" class="headerlink" title="commands"></a>commands</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ls</span> <span class="variable">$WORKON_HOME</span></span><br><span class="line">mkvirtualenv py3 -p python3</span><br><span class="line">mkvirtualenv py2 -p python2</span><br><span class="line">rmvirtualenv py3</span><br><span class="line"></span><br><span class="line">lsvirtualenv </span><br><span class="line">lssitepackages</span><br><span class="line"></span><br><span class="line">workon py3</span><br><span class="line">deactivate</span><br></pre></td></tr></table></figure>

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- kzl in-article ad -->
<p><ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-5653382914441020"
     data-ad-slot="7925631830"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>



<h2 id="Opencv-with-virtualenv"><a href="#Opencv-with-virtualenv" class="headerlink" title="Opencv with virtualenv"></a>Opencv with virtualenv</h2><h3 id="python2"><a href="#python2" class="headerlink" title="python2"></a>python2</h3><p>OpenCV should now be installed in </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">locate cv2.so</span><br><span class="line">/usr/local/lib/python2.7/dist-packages/cv2.so</span><br></pre></td></tr></table></figure>

<p>However, our <code>py2</code>  virtual environment is located in our home directory — thus to use OpenCV within our <code>py2</code> environment, we first need to sym-link OpenCV into the site-packages  directory of the <code>py2</code>  virtual environment:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~/.local/py2/lib/python2.7/site-packages/</span><br><span class="line"><span class="built_in">ln</span> -s /usr/local/lib/python2.7/site-packages/cv2.so cv2.so</span><br><span class="line"><span class="built_in">ln</span> -s /usr/local/lib/python2.7/dist-packages/cv2.so cv2.so</span><br><span class="line"> ```   </span><br><span class="line"></span><br><span class="line">import opencv </span><br><span class="line"></span><br><span class="line">```bash</span><br><span class="line">workon py2</span><br><span class="line">python</span><br><span class="line">&gt;import cv2</span><br><span class="line">&gt;<span class="built_in">print</span>(cv2.__version__)</span><br><span class="line"><span class="string">&#x27;3.1.0&#x27;</span></span><br></pre></td></tr></table></figure>

<h3 id="python3"><a href="#python3" class="headerlink" title="python3"></a>python3</h3><p>you may get error </p>
<pre><code>ImportError: dynamic module does not define init function (PyInit_cv2) 
</code></pre>
<p>when import <code>cv2</code> in python3 (no such problem in python2).</p>
<p>install <code>opencv-python</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">workon py3</span><br><span class="line">pip3 install opencv-contrib-python</span><br></pre></td></tr></table></figure>

<p>test version</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">workon py3</span><br><span class="line">python</span><br><span class="line">import cv2</span><br><span class="line"><span class="built_in">print</span>(cv2.__version__)</span><br><span class="line"><span class="string">&#x27;3.4.2&#x27;</span></span><br></pre></td></tr></table></figure>

<h2 id="install-pycharm"><a href="#install-pycharm" class="headerlink" title="install pycharm"></a>install pycharm</h2><h3 id="apt-get-slow"><a href="#apt-get-slow" class="headerlink" title="apt-get (slow)"></a>apt-get (slow)</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> add-apt-repository ppa:mystic-mirage/pycharm</span><br><span class="line"></span><br><span class="line"><span class="built_in">sudo</span> apt update</span><br><span class="line"></span><br><span class="line"><span class="comment"># no free</span></span><br><span class="line"><span class="built_in">sudo</span> apt install pycharm</span><br><span class="line"></span><br><span class="line"><span class="comment"># free</span></span><br><span class="line"><span class="built_in">sudo</span> apt install pycharm-community</span><br><span class="line"></span><br><span class="line"><span class="comment"># remove</span></span><br><span class="line"><span class="built_in">sudo</span> apt remove pycharm pycharm-community &amp;&amp; <span class="built_in">sudo</span> apt autoremove</span><br></pre></td></tr></table></figure>

<h3 id="offical-faster"><a href="#offical-faster" class="headerlink" title="offical (faster)"></a>offical (faster)</h3><p>download from <a target="_blank" rel="noopener" href="https://www.jetbrains.com/pycharm/download/download-thanks.html?platform=linux">here</a></p>
<p>start by</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh pycharm.sh</span><br></pre></td></tr></table></figure>

<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/iamjqy/p/7000874.html">install pycharm</a></li>
</ul>
<h2 id="History"><a href="#History" class="headerlink" title="History"></a>History</h2><ul>
<li>20180920: created.</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kezunlin.me/blog/compile-opencv-on-ubuntu-16-04/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="kezunlin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kezunlin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/compile-opencv-on-ubuntu-16-04/" class="post-title-link" itemprop="url">compile and install opencv on ubuntu 16.04</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-09-19 10:00:00" itemprop="dateCreated datePublished" datetime="2018-09-19T10:00:00+08:00">2018-09-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-10-14 13:39:28" itemprop="dateModified" datetime="2024-10-14T13:39:28+08:00">2024-10-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/cpp/" itemprop="url" rel="index"><span itemprop="name">cpp</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Series"><a href="#Series" class="headerlink" title="Series"></a>Series</h2><ul>
<li><strong><a href="https://kezunlin.me/post/15f5c3e8/">Part 1: compile opencv on ubuntu 16.04</a></strong></li>
<li><a href="https://kezunlin.me/post/6580691f/">Part 2: compile opencv with CUDA support on windows 10</a></li>
<li><a href="https://kezunlin.me/post/61d55ab4/">Part 3: opencv mat for loop</a></li>
<li><a href="https://kezunlin.me/post/7a6ba82e/">Part 4: speed up opencv image processing with openmp</a></li>
</ul>
<h2 id="Guide"><a href="#Guide" class="headerlink" title="Guide"></a>Guide</h2><p>requirements:</p>
<ul>
<li>ubuntu: 16.04</li>
<li>opencv: 3.3.0</li>
</ul>
<h3 id="install-dependencies"><a href="#install-dependencies" class="headerlink" title="install dependencies"></a>install dependencies</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> apt-get install build-essential</span><br><span class="line"><span class="built_in">sudo</span> apt-get install cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev</span><br><span class="line"><span class="built_in">sudo</span> apt-get install python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev</span><br><span class="line"></span><br><span class="line"><span class="built_in">sudo</span> apt-get install cmake-gui</span><br></pre></td></tr></table></figure>

<h3 id="compile"><a href="#compile" class="headerlink" title="compile"></a>compile</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/opencv/opencv.git</span><br><span class="line">wget https://github.com/opencv/opencv/archive/3.1.0.zip</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> opencv-3.1.0</span><br><span class="line"><span class="built_in">mkdir</span> build</span><br><span class="line"><span class="built_in">cd</span> build &amp;&amp; cmake-gui ..</span><br><span class="line"></span><br><span class="line"><span class="comment"># may take several minutes</span></span><br><span class="line"><span class="built_in">sudo</span> make -j8 </span><br><span class="line"></span><br><span class="line"><span class="comment"># install to /usr/local/bin</span></span><br><span class="line"><span class="built_in">sudo</span> make install</span><br></pre></td></tr></table></figure>

<h3 id="check-version"><a href="#check-version" class="headerlink" title="check version"></a>check version</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">opencv_version</span><br><span class="line">3.3.0</span><br></pre></td></tr></table></figure>

<h3 id="python-cv2"><a href="#python-cv2" class="headerlink" title="python cv2"></a>python cv2</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">python</span><br><span class="line">&gt;&gt;&gt; import cv2</span><br><span class="line">&gt;&gt;&gt; cv2.__version__</span><br></pre></td></tr></table></figure>

<h3 id="pip-install-opencv"><a href="#pip-install-opencv" class="headerlink" title="pip install opencv"></a>pip install opencv</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">workon py3</span><br><span class="line">pip install opencv-contrib-python</span><br><span class="line"></span><br><span class="line">python</span><br><span class="line">&gt;import cv2</span><br><span class="line">&gt;cv2.__version__</span><br><span class="line"><span class="string">&#x27;3.3.0&#x27;</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>for virtualenv, see <a href="https://kezunlin.me/post/f07cae6a/">python virtualenv tutorial</a></p>
</blockquote>
<h3 id="opencv-samples"><a href="#opencv-samples" class="headerlink" title="opencv samples"></a>opencv samples</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> samples</span><br><span class="line">cmake .</span><br><span class="line">make</span><br></pre></td></tr></table></figure>

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- kzl in-article ad -->
<p><ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-5653382914441020"
     data-ad-slot="7925631830"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>



<h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><h3 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;opencv2/core.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;opencv2/imgproc.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;opencv2/highgui.hpp&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> cv;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Mat image = <span class="built_in">imread</span>(<span class="string">&quot;../image/cat.jpg&quot;</span>,<span class="number">0</span>);</span><br><span class="line">    <span class="built_in">imshow</span>(<span class="string">&quot;image&quot;</span>,image);</span><br><span class="line">    <span class="built_in">waitKey</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="CMakeLists-txt"><a href="#CMakeLists-txt" class="headerlink" title="CMakeLists.txt"></a>CMakeLists.txt</h3><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">2.8</span>.<span class="number">8</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">project</span>(demo)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Find includes in corresponding build directories</span></span><br><span class="line"><span class="keyword">set</span>(CMAKE_INCLUDE_CURRENT_DIR <span class="keyword">ON</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">find_package</span>(OpenCV REQUIRED COMPONENTS core highgui imgproc features2d calib3d) </span><br><span class="line"><span class="keyword">include_directories</span>(<span class="variable">$&#123;OpenCV_INCLUDE_DIRS&#125;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">message</span>( [opencv] <span class="variable">$&#123;OpenCV_INCLUDE_DIRS&#125;</span> )</span><br><span class="line"><span class="keyword">message</span>( [opencv] <span class="variable">$&#123;$&#123;OpenCV_LIBS&#125;</span>&#125; )</span><br><span class="line"><span class="keyword">message</span>( [opencv] <span class="variable">$&#123;$&#123;OpenCV_LIBRARIES&#125;</span>&#125; )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">add_executable</span>(<span class="variable">$&#123;PROJECT_NAME&#125;</span> </span><br><span class="line">    demo.cpp</span><br><span class="line">)</span><br><span class="line"><span class="keyword">target_link_libraries</span>(<span class="variable">$&#123;PROJECT_NAME&#125;</span> <span class="variable">$&#123;OpenCV_LIBRARIES&#125;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://kezunlin.me/post/6580691f/">Guide to compile and use opencv with CUDA support on windows 10</a></li>
<li><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/9842127/using-a-mask-with-an-adaptive-threshold">opencv with openmp</a></li>
<li><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/21990580/best-way-to-parallelize-opencv-cvcanny-with-openmp">opencv WITH_OPENMP</a></li>
</ul>
<h2 id="History"><a href="#History" class="headerlink" title="History"></a>History</h2><ul>
<li>20180919:  created.</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kezunlin.me/blog/install-and-configure-cuda-9-2-with-cudnn-7-1-on-ubuntu-16-04/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="kezunlin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kezunlin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/install-and-configure-cuda-9-2-with-cudnn-7-1-on-ubuntu-16-04/" class="post-title-link" itemprop="url">install and configure cuda 9.2 with cudnn 7.1 on ubuntu 16.04</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-09-17 16:28:00" itemprop="dateCreated datePublished" datetime="2018-09-17T16:28:00+08:00">2018-09-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-10-14 13:39:28" itemprop="dateModified" datetime="2024-10-14T13:39:28+08:00">2024-10-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>cuda 9.2</p>
<ul>
<li>nvidia driver 396.54</li>
<li>cuda 9.2 (not install driver,install toolkit and samples)</li>
<li>cudnn 7.1.4 for cuda9.2 (for TensorRT) caffe,tensorflow, baidu anakin</li>
</ul>
<p>cuda 8.0 </p>
<ul>
<li>nvidia driver 384.130</li>
<li>cuda 8.0 (not install driver,install toolkit and samples)</li>
<li>cudnn 6.0.21 for cuda8.0 caffe</li>
</ul>
<h3 id="prepare"><a href="#prepare" class="headerlink" title="prepare"></a>prepare</h3><p>GUI vs tty</p>
<ul>
<li>ctrl+alt+F7 to enter GUI</li>
<li>ctrl+alt+F1-F6 to enter tty1-6, login with(username,password)</li>
</ul>
<p>use <code>fbterm</code> instead of default terminal when we are in tty1</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> apt-get -y install fbterm</span><br><span class="line"><span class="built_in">sudo</span> fbterm</span><br></pre></td></tr></table></figure>

<p>cuda and cudnn</p>
<ul>
<li>download <code>cuda_9.2.148_396.37_linux.run</code> from <a target="_blank" rel="noopener" href="https://developer.nvidia.com/cuda-downloads">cuda</a></li>
<li>download <code>cudnn-9.2-linux-x64-v7.1.tgz</code> from <a target="_blank" rel="noopener" href="https://developer.nvidia.com/rdp/cudnn-archive">cudnn</a></li>
</ul>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- kzl in-article ad -->
<p><ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-5653382914441020"
     data-ad-slot="7925631830"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

<h2 id="Steps"><a href="#Steps" class="headerlink" title="Steps"></a>Steps</h2><h3 id="install-general-dependencies"><a href="#install-general-dependencies" class="headerlink" title="install general dependencies"></a>install general dependencies</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libhdf5-serial-dev protobuf-compiler</span><br><span class="line">apt-get install --no-install-recommends libboost-all-dev</span><br><span class="line"></span><br><span class="line"><span class="comment"># blas</span></span><br><span class="line"><span class="built_in">sudo</span> apt-get install libopenblas-dev liblapack-dev libatlas-base-dev</span><br><span class="line"></span><br><span class="line"><span class="built_in">sudo</span> apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev</span><br><span class="line"></span><br><span class="line"><span class="built_in">sudo</span> apt-get install git cmake build-essential</span><br><span class="line"></span><br><span class="line"><span class="comment"># fix missing </span></span><br><span class="line"><span class="comment">#sudo apt-get install freeglut3-dev build-essential libx11-dev libxmu-dev libxi-dev libgl1-mesa-glx libglu1-mesa libglu1-mesa-dev</span></span><br></pre></td></tr></table></figure>


<h3 id="GUI-mode"><a href="#GUI-mode" class="headerlink" title="GUI mode"></a>GUI mode</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># disable default ubuntu driver</span></span><br><span class="line"><span class="built_in">sudo</span> vim /etc/modprobe.d/blacklist-nouveau.conf</span><br><span class="line"></span><br><span class="line">blacklist nouveau</span><br><span class="line">blacklist lbm-nouveau</span><br><span class="line">options nouveau modeset=0</span><br><span class="line"><span class="built_in">alias</span> nouveau off</span><br><span class="line"><span class="built_in">alias</span> lbm-nouveau off</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> options nouveau modeset=0 | <span class="built_in">sudo</span> <span class="built_in">tee</span> -a /etc/modprobe.d/nouveau-kms.conf</span><br><span class="line"><span class="built_in">sudo</span> update-initramfs -u</span><br><span class="line"><span class="built_in">sudo</span> reboot</span><br></pre></td></tr></table></figure>

<h3 id="tty-mode"><a href="#tty-mode" class="headerlink" title="tty mode"></a>tty mode</h3><blockquote>
<p>ctrl+alt+F1 to enter tty1, login with(username,password)</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> fbterm</span><br><span class="line"></span><br><span class="line"><span class="comment"># stop x-server before install cuda driver</span></span><br><span class="line"><span class="built_in">sudo</span> service lightdm stop</span><br></pre></td></tr></table></figure>

<h4 id="remove-previous-nvidia-driver-cuda-toolkit"><a href="#remove-previous-nvidia-driver-cuda-toolkit" class="headerlink" title="remove previous nvidia driver + cuda toolkit"></a>remove previous nvidia driver + cuda toolkit</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> apt-get remove --purge nvidia-*</span><br><span class="line"><span class="comment"># remove 8.0</span></span><br><span class="line"><span class="built_in">sudo</span> /usr/local/cuda-8.0/bin/uninstall_cuda_8.0.pl</span><br><span class="line"><span class="comment"># remove 9.2</span></span><br><span class="line"><span class="built_in">sudo</span> /usr/local/cuda-9.2/bin/uninstall_cuda_9.2.pl</span><br></pre></td></tr></table></figure>

<h4 id="install-nvidia-driver-from-ppa"><a href="#install-nvidia-driver-from-ppa" class="headerlink" title="install nvidia driver from ppa"></a>install nvidia driver from ppa</h4><blockquote>
<p>DO NOT use <code>cuda_xxx_linux.run</code> to install nvidia driver, otherwise we<br>get <code>Loop Login Problem</code> when we reboot.</p>
<p>安装显卡驱动推荐使用官方ppa源的方式进行安装，使用<code>cuda_xxx_linux.run</code>文件离线安装会导致循环登录问题。</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> add-apt-repository ppa:graphics-drivers/ppa</span><br><span class="line">sudp apt-get update</span><br><span class="line"></span><br><span class="line"><span class="built_in">sudo</span> apt-cache search nvidia-*</span><br><span class="line"><span class="comment"># nvidia-384</span></span><br><span class="line"><span class="comment"># nvidia-396</span></span><br><span class="line"><span class="built_in">sudo</span> apt-get -y install nvidia-396</span><br><span class="line"></span><br><span class="line"><span class="comment"># test </span></span><br><span class="line"><span class="built_in">sudo</span> nvidia-smi</span><br><span class="line">```  </span><br><span class="line"></span><br><span class="line"><span class="comment">#### install cuda toolkit from run file</span></span><br><span class="line"></span><br><span class="line">&gt; 1. DO NOT install nvidia driver, install cuda toolkit + samples.</span><br><span class="line">&gt;</span><br><span class="line">&gt; 2. use default install path `/usr/local/cuda-9.2`</span><br><span class="line">&gt; </span><br><span class="line">&gt; 3. use `/usr/local/cuda-9.2/bin/uninstall_cuda_9.2.pl` to uninstall </span><br><span class="line"></span><br><span class="line">```bash</span><br><span class="line"><span class="built_in">chmod</span> +x ./cuda_9.2.148_396.37_linux.run</span><br><span class="line"></span><br><span class="line"><span class="comment"># Using unspported compiler---&gt; override</span></span><br><span class="line">./cuda_9.2.148_396.37_linux.run --override</span><br></pre></td></tr></table></figure>

<p>output </p>
<pre><code>---------------------------------------
Do you accept the previously read EULA? 
(accept/decline/quit): accept

Install NVIDIA Accelerated Graphics Driver for Linux-x86_64 396.37? (y)es/(n)o/(q)uit: no

Install the CUDA 9.2 Toolkit? 
(y)es/(n)o/(q)uit: yes

Enter Toolkit Location 
    [ default is /usr/local/cuda-9.2 ]:

Do you want to install a symbolic link at /usr/local/cuda? (y)es/(n)o/(q)uit: yes


Install the CUDA 9.2 Samples? 
(y)es/(n)o/(q)uit: yes

Enter CUDA Samples Location 
    [ default is /home/kezunlin ]: 


Installing the CUDA Toolkit in /usr/local/cuda-9.2 ...
Installing the CUDA Samples in /home/kezunlin ...

===========
= Summary =
===========

Driver:   Not Selected
Toolkit:  Installed in /usr/local/cuda-9.2
Samples:  Installed in /home/kezunlin

Please make sure that
 -   PATH includes /usr/local/cuda-9.2/bin
 -   LD_LIBRARY_PATH includes /usr/local/cuda-9.2/lib64, or, add /usr/local/cuda-9.2/lib64 to /etc/ld.so.conf and run ldconfig as root

To uninstall the CUDA Toolkit, run the uninstall script in /usr/local/cuda-9.2/bin

Please see CUDA_Installation_Guide_Linux.pdf in /usr/local/cuda-9.2/doc/pdf for detailed information on setting up CUDA.

***WARNING: Incomplete installation! This installation did not install the CUDA Driver. A driver of version at least 384.00 is required for CUDA 9.2 functionality to work.
To install the driver using this installer, run the following command, replacing &lt;CudaInstaller&gt; with the name of this run file:
    sudo &lt;CudaInstaller&gt;.run -silent -driver

Logfile is /tmp/cuda_install_6659.log
</code></pre>
<h3 id="reboot-to-enter-GUI"><a href="#reboot-to-enter-GUI" class="headerlink" title="reboot to enter GUI"></a>reboot to enter GUI</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> reboot </span><br></pre></td></tr></table></figure>

<blockquote>
<p>OK. we no longer have <code>Loop Login Problem</code>.</p>
</blockquote>
<h3 id="add-library-path"><a href="#add-library-path" class="headerlink" title="add library path"></a>add library path</h3><p>system env </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vim .bashrc</span><br><span class="line"></span><br><span class="line"><span class="comment"># for cuda and cudnn</span></span><br><span class="line"><span class="built_in">export</span> PATH=/usr/local/cuda/bin:<span class="variable">$PATH</span></span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=/usr/local/cuda/lib64:<span class="variable">$LD_LIBRARY_PATH</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">source</span> .bashrc</span><br></pre></td></tr></table></figure>

<p>or by conf file</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> vim /etc/ld.so.conf.d/cuda.conf</span><br><span class="line">/usr/local/cuda/lib64</span><br><span class="line"></span><br><span class="line"><span class="built_in">sudo</span> ldconifg</span><br></pre></td></tr></table></figure>

<h3 id="test"><a href="#test" class="headerlink" title="test"></a>test</h3><h4 id="nvidia-smi"><a href="#nvidia-smi" class="headerlink" title="nvidia-smi"></a>nvidia-smi</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">nvidia-smi</span><br><span class="line">Tue Sep 18 10:35:55 2018       </span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 396.54                 Driver Version: 396.54                    |</span><br><span class="line">|-------------------------------+----------------------+----------------------+</span><br><span class="line">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|===============================+======================+======================|</span><br><span class="line">|   0  GeForce GTX 1060    Off  | 00000000:01:00.0 Off |                  N/A |</span><br><span class="line">| N/A   58C    P0    31W /  N/A |    288MiB /  6078MiB |      0%      Default |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line"></span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                       GPU Memory |</span><br><span class="line">|  GPU       PID   Type   Process name                             Usage      |</span><br><span class="line">|=============================================================================|</span><br><span class="line">|    0      1636      G   /usr/lib/xorg/Xorg                           164MiB |</span><br><span class="line">|    0      2569      G   compiz                                        40MiB |</span><br><span class="line">|    0      4828      G   ...-token=2DAB0000EFF3321D4D304928FA64B811    81MiB |</span><br><span class="line">+-----------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure>

<p>or </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> /proc/driver/nvidia/version</span><br></pre></td></tr></table></figure>

<h4 id="nvcc"><a href="#nvcc" class="headerlink" title="nvcc"></a>nvcc</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">nvcc -V</span><br><span class="line">nvcc: NVIDIA (R) Cuda compiler driver</span><br><span class="line">Copyright (c) 2005-2018 NVIDIA Corporation</span><br><span class="line">Built on Tue_Jun_12_23:07:04_CDT_2018</span><br><span class="line">Cuda compilation tools, release 9.2, V9.2.148</span><br></pre></td></tr></table></figure>

<h4 id="deviceQuery"><a href="#deviceQuery" class="headerlink" title="deviceQuery"></a>deviceQuery</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~/NVIDIA_CUDA-9.2_Samples/1_Utilities/deviceQuery</span><br><span class="line">make </span><br><span class="line">./deviceQuery </span><br></pre></td></tr></table></figure>

<p>output </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">./deviceQuery Starting...</span><br><span class="line"></span><br><span class="line">    CUDA Device Query (Runtime API) version (CUDART static linking)</span><br><span class="line"></span><br><span class="line">Detected 1 CUDA Capable device(s)</span><br><span class="line"></span><br><span class="line">Device 0: <span class="string">&quot;GeForce GTX 1060&quot;</span></span><br><span class="line">    CUDA Driver Version / Runtime Version          9.2 / 9.2</span><br><span class="line">    CUDA Capability Major/Minor version number:    6.1</span><br><span class="line">    Total amount of global memory:                 6078 MBytes (6373572608 bytes)</span><br><span class="line">    (10) Multiprocessors, (128) CUDA Cores/MP:     1280 CUDA Cores</span><br><span class="line">    GPU Max Clock rate:                            1733 MHz (1.73 GHz)</span><br><span class="line">    Memory Clock rate:                             4004 Mhz</span><br><span class="line">    Memory Bus Width:                              192-bit</span><br><span class="line">    L2 Cache Size:                                 1572864 bytes</span><br><span class="line">    Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)</span><br><span class="line">    Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers</span><br><span class="line">    Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers</span><br><span class="line">    Total amount of constant memory:               65536 bytes</span><br><span class="line">    Total amount of shared memory per block:       49152 bytes</span><br><span class="line">    Total number of registers available per block: 65536</span><br><span class="line">    Warp size:                                     32</span><br><span class="line">    Maximum number of threads per multiprocessor:  2048</span><br><span class="line">    Maximum number of threads per block:           1024</span><br><span class="line">    Max dimension size of a thread block (x,y,z): (1024, 1024, 64)</span><br><span class="line">    Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)</span><br><span class="line">    Maximum memory pitch:                          2147483647 bytes</span><br><span class="line">    Texture alignment:                             512 bytes</span><br><span class="line">    Concurrent copy and kernel execution:          Yes with 2 copy engine(s)</span><br><span class="line">    Run time <span class="built_in">limit</span> on kernels:                     Yes</span><br><span class="line">    Integrated GPU sharing Host Memory:            No</span><br><span class="line">    Support host page-locked memory mapping:       Yes</span><br><span class="line">    Alignment requirement <span class="keyword">for</span> Surfaces:            Yes</span><br><span class="line">    Device has ECC support:                        Disabled</span><br><span class="line">    Device supports Unified Addressing (UVA):      Yes</span><br><span class="line">    Device supports Compute Preemption:            Yes</span><br><span class="line">    Supports Cooperative Kernel Launch:            Yes</span><br><span class="line">    Supports MultiDevice Co-op Kernel Launch:      Yes</span><br><span class="line">    Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0</span><br><span class="line">    Compute Mode:</span><br><span class="line">        &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;</span><br><span class="line"></span><br><span class="line">deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 9.2, CUDA Runtime Version = 9.2, NumDevs = 1</span><br><span class="line">Result = PASS</span><br></pre></td></tr></table></figure>

<p>we get <code>Result = PASS</code>.</p>
<h3 id="install-cudnn"><a href="#install-cudnn" class="headerlink" title="install cudnn"></a>install cudnn</h3><p>download <code>cudnn-9.2-linux-x64-v7.1.tgz</code> for ubuntu 16.04</p>
<ul>
<li>copy <code>include</code> to <code>/usr/local/cuda-9.2/include</code></li>
<li>copy <code>lib64</code> to <code>/usr/local/cuda-9.2/lib64</code></li>
</ul>
<p>commands </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tar -xzvf cudnn-9.2-linux-x64-v7.1.tgz </span><br><span class="line"><span class="built_in">sudo</span> <span class="built_in">cp</span> cuda/include/cudnn.h /usr/local/cuda/include/</span><br><span class="line"><span class="built_in">sudo</span> <span class="built_in">cp</span> cuda/lib64/* /usr/local/cuda/lib64/</span><br></pre></td></tr></table></figure>


<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a target="_blank" rel="noopener" href="https://www.pyimagesearch.com/2017/09/27/setting-up-ubuntu-16-04-cuda-gpu-for-deep-learning-with-python/">setting-up-ubuntu-16-04-cuda-gpu-for-deep-learning-with-python</a></li>
</ul>
<h2 id="History"><a href="#History" class="headerlink" title="History"></a>History</h2><ul>
<li>20180917: created.</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kezunlin.me/blog/install%20-and-configure-%20tensorrt-4-on-%20ubuntu-16.04/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="kezunlin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kezunlin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/install%20-and-configure-%20tensorrt-4-on-%20ubuntu-16.04/" class="post-title-link" itemprop="url">install and configure tensorrt 4 on ubuntu 16.04</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-09-07 12:36:00" itemprop="dateCreated datePublished" datetime="2018-09-07T12:36:00+08:00">2018-09-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-10-14 13:39:28" itemprop="dateModified" datetime="2024-10-14T13:39:28+08:00">2024-10-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Series"><a href="#Series" class="headerlink" title="Series"></a>Series</h2><ul>
<li><strong><a href="https://kezunlin.me/post/dacc4196/">Part 1: install and configure tensorrt 4 on ubuntu 16.04</a></strong></li>
<li><a href="https://kezunlin.me/post/bcdfb73c/">Part 2: tensorrt fp32 fp16 tutorial</a></li>
<li><a href="https://kezunlin.me/post/30e0cb19/">Part 3: tensorrt int8 tutorial</a></li>
</ul>
<h2 id="Guide"><a href="#Guide" class="headerlink" title="Guide"></a>Guide</h2><h3 id="version"><a href="#version" class="headerlink" title="version"></a>version</h3><ul>
<li>ubuntu 16.04 (14.04,16.04 only) not support Windows</li>
<li><del>CUDA 8.0</del> (8.0,9.0,9.2 only)</li>
<li>CUDA 9.2</li>
<li>cudnn 7.1.4 (7.1 only)</li>
<li>TensorRT 4.0.1.6 </li>
<li>TensorFlow-gpu v1.4+</li>
<li>python: 3.5.2 (2.7 or 3.5)</li>
</ul>
<h3 id="TensorRT-support-matrix"><a href="#TensorRT-support-matrix" class="headerlink" title="TensorRT support matrix"></a>TensorRT support matrix</h3><ul>
<li><p>4.0.1.6<br><img src="https://kezunlin.me/images/posts/635233-20180912150412426-572657767.png" alt="support matrix"></p>
</li>
<li><p>5.0.2.6<br><img src="https://kezunlin.me/images/posts/635233-20181119095055341-1432624267.png" alt="support matrix"></p>
</li>
</ul>
<h3 id="hardware-precision-matrix"><a href="#hardware-precision-matrix" class="headerlink" title="hardware precision matrix"></a>hardware precision matrix</h3><p>hardware precision support matrix<br><img src="https://kezunlin.me/images/posts/635233-20190422151849087-402485522.png" alt="hardware precision support matrix"></p>
<blockquote>
<p>see <a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-support-matrix/index.html#hardware-precision-matrix">tensorrt-support-matrix</a></p>
</blockquote>
<h4 id="ubuntu"><a href="#ubuntu" class="headerlink" title="ubuntu"></a>ubuntu</h4><ul>
<li>GeForce 1060 (fp32,int8)  no fp16</li>
</ul>
<h4 id="jetson-products"><a href="#jetson-products" class="headerlink" title="jetson products"></a>jetson products</h4><ul>
<li>Jetson TX1   (fp32,fp16)</li>
<li>Jetson TX2    (fp32,fp16)</li>
<li>Jetson AGX Xavier (fp32,fp16,int8,dla)</li>
<li>Jetson Nano (Jetbot)</li>
</ul>
<h3 id="install"><a href="#install" class="headerlink" title="install"></a>install</h3><h4 id="download-and-install"><a href="#download-and-install" class="headerlink" title="download and install"></a>download and install</h4><p>download <code>TensorRT-4.0.1.6.Ubuntu-16.04.4.x86_64-gnu.cuda-8.0.cudnn7.1.tar.gz</code> from <a target="_blank" rel="noopener" href="https://developer.nvidia.com/tensorrt">here</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tar zxvf TensorRT-4.0.1.6.Ubuntu-16.04.4.x86_64-gnu.cuda-8.0.cudnn7.1.tar.gz </span><br><span class="line"></span><br><span class="line"><span class="built_in">ls</span> TensorRT-4.0.1.6</span><br><span class="line">bin  data  doc  graphsurgeon  include  lib  python  samples  targets  TensorRT-Release-Notes.pdf  uff</span><br><span class="line"></span><br><span class="line"><span class="built_in">sudo</span> <span class="built_in">mv</span> TensorRT-4.0.1.6 /opt/</span><br><span class="line"><span class="built_in">cd</span> /opt</span><br><span class="line"><span class="built_in">sudo</span> <span class="built_in">ln</span> -s TensorRT-4.0.1.6/ tensorrt</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Updates: from cuda-8.0 &#x3D;&#x3D;&#x3D;&gt; cuda-9.2. download <code>TensorRT-4.0.1.6.Ubuntu-16.04.4.x86_64-gnu.cuda-9.2.cudnn7.1.tar.gz</code> from <a target="_blank" rel="noopener" href="https://developer.nvidia.com/tensorrt">here</a></p>
</blockquote>
<h4 id="add-lib-to-path"><a href="#add-lib-to-path" class="headerlink" title="add lib to path"></a>add lib to path</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> vim /etc/ld.so.conf.d/tensorrt</span><br><span class="line">/opt/tensorrt/lib</span><br><span class="line"></span><br><span class="line"><span class="built_in">sudo</span> ldconfig</span><br></pre></td></tr></table></figure>

<p>or </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bashrc</span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=/opt/tensorrt/lib:<span class="variable">$LD_LIBRARY_PATH</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure>

<h4 id="python-package"><a href="#python-package" class="headerlink" title="python package"></a>python package</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /opt/tensorrt/python</span><br><span class="line"><span class="built_in">sudo</span> pip2 install tensorrt-4.0.1.6-cp27-cp27mu-linux_x86_64.whl </span><br></pre></td></tr></table></figure>

<p>or </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /opt/tensorrt/python</span><br><span class="line"><span class="built_in">sudo</span> pip3 install tensorrt-4.0.1.6-cp35-cp35m-linux_x86_64.whl </span><br></pre></td></tr></table></figure>

<h4 id="uff-package"><a href="#uff-package" class="headerlink" title="uff package"></a>uff package</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /opt/tensorrt/uff </span><br><span class="line"><span class="built_in">sudo</span> pip install uff-0.4.0-py2.py3-none-any.whl </span><br><span class="line"></span><br><span class="line"><span class="built_in">which</span> convert-to-uff</span><br><span class="line">/usr/local/bin/convert-to-uff</span><br></pre></td></tr></table></figure>

<h3 id="folder-structure"><a href="#folder-structure" class="headerlink" title="folder structure"></a>folder structure</h3><h4 id="include"><a href="#include" class="headerlink" title="include"></a>include</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tree include/</span><br><span class="line">include/</span><br><span class="line">├── NvCaffeParser.h</span><br><span class="line">├── NvInfer.h</span><br><span class="line">├── NvInferPlugin.h</span><br><span class="line">├── NvOnnxConfig.h</span><br><span class="line">├── NvOnnxParser.h</span><br><span class="line">├── NvUffParser.h</span><br><span class="line">└── NvUtils.h</span><br></pre></td></tr></table></figure>

<h4 id="lib"><a href="#lib" class="headerlink" title="lib"></a>lib</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ls</span> -al *.4.1.2</span><br><span class="line">lrwxrwxrwx 1 kezunlin kezunlin       21 6月  12 15:42 libnvcaffe_parser.so.4.1.2 -&gt; libnvparsers.so.4.1.2</span><br><span class="line">-rwxrwxr-x 1 kezunlin kezunlin  2806840 6月  12 15:42 libnvinfer_plugin.so.4.1.2</span><br><span class="line">-rwxrwxr-x 1 kezunlin kezunlin 80434488 6月  12 15:42 libnvinfer.so.4.1.2</span><br><span class="line">-rwxrwxr-x 1 kezunlin kezunlin  3951712 6月  12 15:42 libnvparsers.so.4.1.2</span><br></pre></td></tr></table></figure>

<h4 id="bin"><a href="#bin" class="headerlink" title="bin"></a>bin</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tree bin</span><br><span class="line">bin</span><br><span class="line">├── download-digits-model.py</span><br><span class="line">├── giexec</span><br><span class="line">└── trtexec</span><br></pre></td></tr></table></figure>

<h4 id="sample"><a href="#sample" class="headerlink" title="sample"></a>sample</h4><p>add envs </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bashrc</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensorrt cuda and cudnn</span></span><br><span class="line"><span class="built_in">export</span> CUDA_INSTALL_DIR=/usr/local/cuda</span><br><span class="line"><span class="built_in">export</span> CUDNN_INSTALL_DIR=/usr/local/cuda</span><br></pre></td></tr></table></figure>

<h4 id="compile-all"><a href="#compile-all" class="headerlink" title="compile all"></a>compile all</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> samples/</span><br><span class="line">make -j8</span><br></pre></td></tr></table></figure>

<blockquote>
<p>generate all sample_xxx to bin&#x2F; folder.</p>
</blockquote>
<h4 id="compile-sampleMNIST"><a href="#compile-sampleMNIST" class="headerlink" title="compile sampleMNIST"></a>compile sampleMNIST</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> samples/sampleMNIST</span><br><span class="line"><span class="built_in">ls</span> </span><br><span class="line">Makefile  sampleMNIST.cpp</span><br><span class="line">make -j8</span><br></pre></td></tr></table></figure>

<p>error occurs</p>
<pre><code>dpkg-query: no packages found matching cuda-cudart-[0-9]*
../Makefile.config:6: CUDA_INSTALL_DIR variable is not specified, using /usr/local/cuda- by default, use CUDA_INSTALL_DIR=&lt;cuda_directory&gt; to change.
../Makefile.config:9: CUDNN_INSTALL_DIR variable is not specified, using  by default, use CUDNN_INSTALL_DIR=&lt;cudnn_directory&gt; to change.
</code></pre>
<p>fix solutions:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bashrc</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensorrt cuda and cudnn</span></span><br><span class="line"><span class="built_in">export</span> CUDA_INSTALL_DIR=/opt/cuda</span><br><span class="line"><span class="built_in">export</span> CUDNN_INSTALL_DIR=/opt/cuda</span><br></pre></td></tr></table></figure>

<p>make again </p>
<pre><code>:
:
Compiling: sampleMNIST.cpp
Compiling: sampleMNIST.cpp
Linking: ../../bin/sample_mnist
Linking: ../../bin/sample_mnist_debug
# Copy every EXTRA_FILE of this sample to bin dir
</code></pre>
<p>test <code>sample_mnist</code></p>
<pre><code>./sample_mnist
Reading Caffe prototxt: ../../../data/mnist/mnist.prototxt
Reading Caffe model: ../../../data/mnist/mnist.caffemodel

Input:

@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@%.:@@@@@@@@@@@@
@@@@@@@@@@@@@: *@@@@@@@@@@@@
@@@@@@@@@@@@* =@@@@@@@@@@@@@
@@@@@@@@@@@% :@@@@@@@@@@@@@@
@@@@@@@@@@@- *@@@@@@@@@@@@@@
@@@@@@@@@@# .@@@@@@@@@@@@@@@
@@@@@@@@@@: #@@@@@@@@@@@@@@@
@@@@@@@@@+ -@@@@@@@@@@@@@@@@
@@@@@@@@@: %@@@@@@@@@@@@@@@@
@@@@@@@@+ +@@@@@@@@@@@@@@@@@
@@@@@@@@:.%@@@@@@@@@@@@@@@@@
@@@@@@@% -@@@@@@@@@@@@@@@@@@
@@@@@@@% -@@@@@@#..:@@@@@@@@
@@@@@@@% +@@@@@-    :@@@@@@@
@@@@@@@% =@@@@%.#@@- +@@@@@@
@@@@@@@@..%@@@*+@@@@ :@@@@@@
@@@@@@@@= -%@@@@@@@@ :@@@@@@
@@@@@@@@@- .*@@@@@@+ +@@@@@@
@@@@@@@@@@+  .:-+-: .@@@@@@@
@@@@@@@@@@@@+:    :*@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@

Output:

0: 
1: 
2: 
3: 
4: 
5: 
6: **********
7: 
8: 
9: 
</code></pre>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- kzl in-article ad -->
<p><ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-5653382914441020"
     data-ad-slot="7925631830"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h2 id="Sample"><a href="#Sample" class="headerlink" title="Sample"></a>Sample</h2><p>compile all samples </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> sample</span><br><span class="line">make -j8</span><br></pre></td></tr></table></figure>

<h3 id="sample-mnist"><a href="#sample-mnist" class="headerlink" title="sample_mnist"></a>sample_mnist</h3><p>see above. skip.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">ldd sample_mnist</span><br><span class="line">    linux-vdso.so.1 =&gt;  (0x00007ffecd9f3000)</span><br><span class="line">    libnvinfer.so.4 =&gt; /opt/tensorrt/lib/libnvinfer.so.4 (0x00007f48de6f2000)</span><br><span class="line">    libnvparsers.so.4.1.2 =&gt; /opt/tensorrt/lib/libnvparsers.so.4.1.2 (0x00007f48de12c000)</span><br><span class="line">    librt.so.1 =&gt; /lib/x86_64-linux-gnu/librt.so.1 (0x00007f48ddf24000)</span><br><span class="line">    libdl.so.2 =&gt; /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f48ddd20000)</span><br><span class="line">    libpthread.so.0 =&gt; /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f48ddb03000)</span><br><span class="line">    libstdc++.so.6 =&gt; /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f48dd781000)</span><br><span class="line">    libm.so.6 =&gt; /lib/x86_64-linux-gnu/libm.so.6 (0x00007f48dd478000)</span><br><span class="line">    libgcc_s.so.1 =&gt; /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f48dd262000)</span><br><span class="line">    libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007f48dce98000)</span><br><span class="line">    libcudnn.so.7 =&gt; /usr/local/cuda/lib64/libcudnn.so.7 (0x00007f48c8818000)</span><br><span class="line">    libcublas.so.9.2 =&gt; /usr/local/cuda/lib64/libcublas.so.9.2 (0x00007f48c4dca000)</span><br><span class="line">    libcudart.so.9.2 =&gt; /usr/local/cuda/lib64/libcudart.so.9.2 (0x00007f48c4b60000)</span><br><span class="line">    /lib64/ld-linux-x86-64.so.2 (0x00007f48e42bc000)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>libnvinfer.so, libnvparsers.so, libcudart.so, libcudnn.so, libcublas.so</p>
</blockquote>
<h3 id="sample-onnx-mnist"><a href="#sample-onnx-mnist" class="headerlink" title="sample_onnx_mnist"></a>sample_onnx_mnist</h3><pre><code>./sample_onnx_mnist



---------------------------



@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@%.-@@@@@@@@@@@
@@@@@@@@@@@*-    %@@@@@@@@@@
@@@@@@@@@@= .-.  *@@@@@@@@@@
@@@@@@@@@= +@@@  *@@@@@@@@@@
@@@@@@@@* =@@@@  %@@@@@@@@@@
@@@@@@@@..@@@@%  @@@@@@@@@@@
@@@@@@@# *@@@@-  @@@@@@@@@@@
@@@@@@@: @@@@%   @@@@@@@@@@@
@@@@@@@: @@@@-   @@@@@@@@@@@
@@@@@@@: =+*= +: *@@@@@@@@@@
@@@@@@@*.    +@: *@@@@@@@@@@
@@@@@@@@%#**#@@: *@@@@@@@@@@
@@@@@@@@@@@@@@@: -@@@@@@@@@@
@@@@@@@@@@@@@@@+ :@@@@@@@@@@
@@@@@@@@@@@@@@@*  @@@@@@@@@@
@@@@@@@@@@@@@@@@  %@@@@@@@@@
@@@@@@@@@@@@@@@@  #@@@@@@@@@
@@@@@@@@@@@@@@@@: +@@@@@@@@@
@@@@@@@@@@@@@@@@- +@@@@@@@@@
@@@@@@@@@@@@@@@@*:%@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@


 Prob 0  0.00000: 
 Prob 1  0.00001: 
 Prob 2  0.00002: 
 Prob 3  0.00003: 
 Prob 4  0.00044: 
 Prob 5  0.00005: 
 Prob 6  0.00006: 
 Prob 7  0.00007: 
 Prob 8  0.00008: 
 Prob 9  0.99969: **********
</code></pre>
<h3 id="sample-uff-mnist"><a href="#sample-uff-mnist" class="headerlink" title="sample_uff_mnist"></a>sample_uff_mnist</h3><pre><code>../../../data/mnist/lenet5.uff



---------------------------



@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@%.-@@@@@@@@@@@
@@@@@@@@@@@*-    %@@@@@@@@@@
@@@@@@@@@@= .-.  *@@@@@@@@@@
@@@@@@@@@= +@@@  *@@@@@@@@@@
@@@@@@@@* =@@@@  %@@@@@@@@@@
@@@@@@@@..@@@@%  @@@@@@@@@@@
@@@@@@@# *@@@@-  @@@@@@@@@@@
@@@@@@@: @@@@%   @@@@@@@@@@@
@@@@@@@: @@@@-   @@@@@@@@@@@
@@@@@@@: =+*= +: *@@@@@@@@@@
@@@@@@@*.    +@: *@@@@@@@@@@
@@@@@@@@%#**#@@: *@@@@@@@@@@
@@@@@@@@@@@@@@@: -@@@@@@@@@@
@@@@@@@@@@@@@@@+ :@@@@@@@@@@
@@@@@@@@@@@@@@@*  @@@@@@@@@@
@@@@@@@@@@@@@@@@  %@@@@@@@@@
@@@@@@@@@@@@@@@@  #@@@@@@@@@
@@@@@@@@@@@@@@@@: +@@@@@@@@@
@@@@@@@@@@@@@@@@- +@@@@@@@@@
@@@@@@@@@@@@@@@@*:%@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@
10 eltCount
--- OUTPUT ---
0 =&gt; -2.75228	 : 
1 =&gt; -1.51534	 : 
2 =&gt; -4.11729	 : 
3 =&gt; 0.316925	 : 
4 =&gt; 3.73423	 : 
5 =&gt; -3.00593	 : 
6 =&gt; -6.18866	 : 
7 =&gt; -1.02671	 : 
8 =&gt; 1.937	 : 
9 =&gt; 14.8275	 : ***

Average over 10 runs is 0.0843257 ms.
</code></pre>
<h3 id="sample-mnist-api"><a href="#sample-mnist-api" class="headerlink" title="sample_mnist_api"></a>sample_mnist_api</h3><pre><code>./sample_mnist_api
Loading weights: ../../../data/mnist/mnistapi.wts

Input:

@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@+ @@@@@@@@@@@@@@
@@@@@@@@@@@@. @@@@@@@@@@@@@@
@@@@@@@@@@@@- @@@@@@@@@@@@@@
@@@@@@@@@@@#  @@@@@@@@@@@@@@
@@@@@@@@@@@#  *@@@@@@@@@@@@@
@@@@@@@@@@@@  :@@@@@@@@@@@@@
@@@@@@@@@@@@= .@@@@@@@@@@@@@
@@@@@@@@@@@@#  %@@@@@@@@@@@@
@@@@@@@@@@@@% .@@@@@@@@@@@@@
@@@@@@@@@@@@%  %@@@@@@@@@@@@
@@@@@@@@@@@@%  %@@@@@@@@@@@@
@@@@@@@@@@@@@= +@@@@@@@@@@@@
@@@@@@@@@@@@@* -@@@@@@@@@@@@
@@@@@@@@@@@@@*  @@@@@@@@@@@@
@@@@@@@@@@@@@@  @@@@@@@@@@@@
@@@@@@@@@@@@@@  *@@@@@@@@@@@
@@@@@@@@@@@@@@  *@@@@@@@@@@@
@@@@@@@@@@@@@@  *@@@@@@@@@@@
@@@@@@@@@@@@@@  *@@@@@@@@@@@
@@@@@@@@@@@@@@* @@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@

Output:

0: 
1: **********
2: 
3: 
4: 
5: 
6: 
7: 
8: 
9: 
</code></pre>
<h3 id="sample-int8"><a href="#sample-int8" class="headerlink" title="sample_int8"></a>sample_int8</h3><pre><code>./sample_int8 mnist

FP32 run:400 batches of size 100 starting at 100
........................................
Top1: 0.9904, Top5: 1
Processing 40000 images averaged 0.00332707 ms/image and 0.332707 ms/batch.

FP16 run:400 batches of size 100 starting at 100
Engine could not be created at this precision

INT8 run:400 batches of size 100 starting at 100
........................................
Top1: 0.9909, Top5: 1
Processing 40000 images averaged 0.00215323 ms/image and 0.215323 ms/batch.
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html">TensorRT developer guide</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.htm">TensorRT install guide</a></li>
<li><a target="_blank" rel="noopener" href="https://devblogs.nvidia.com/speed-up-inference-tensorrt/">speed-up-inference-tensorrt</a></li>
<li><a target="_blank" rel="noopener" href="https://devblogs.nvidia.com/large-scale-object-detection-tensorrt/">large-scale-object-detection-tensorrt</a></li>
<li><a target="_blank" rel="noopener" href="https://devblogs.nvidia.com/int8-inference-autonomous-vehicles-tensorrt/">int8-inference-autonomous-vehicles-tensorrt</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-api/c_api/classnvinfer1_1_1_i_builder.html">tensorrt api</a><br><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-support-matrix/index.html#hardware-precision-matrix">tensorrt-support-matrix</a></li>
</ul>
<h2 id="History"><a href="#History" class="headerlink" title="History"></a>History</h2><ul>
<li>20180907: created.</li>
<li>20181119: add tensorrt-5.0.</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kezunlin.me/blog/anakin-guide-on-ubuntu-16-04/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="kezunlin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kezunlin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/anakin-guide-on-ubuntu-16-04/" class="post-title-link" itemprop="url">compile baidu anakin on ubuntu 16.04</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-09-03 18:16:00" itemprop="dateCreated datePublished" datetime="2018-09-03T18:16:00+08:00">2018-09-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-10-14 13:39:28" itemprop="dateModified" datetime="2024-10-14T13:39:28+08:00">2024-10-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Guide"><a href="#Guide" class="headerlink" title="Guide"></a>Guide</h2><h3 id="version"><a href="#version" class="headerlink" title="version"></a>version</h3><ul>
<li>gcc 4.8.5&#x2F;5.4.0</li>
<li>g++ 4.8.5&#x2F;5.4.0</li>
<li>cmake 3.2.2</li>
<li>nvidia driver 396.54 + cuda 9.2 + cudnn 7.1.4</li>
<li>protobuf 3.4.0</li>
</ul>
<h3 id="install-nvidia-docker2"><a href="#install-nvidia-docker2" class="headerlink" title="install nvidia-docker2"></a>install nvidia-docker2</h3><p>see <a href="https://kezunlin.me/post/6293dd25/">nvidia-docker2 guide on ubuntu 16.04</a></p>
<p>test </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> docker run --runtime=nvidia --<span class="built_in">rm</span> nvidia/cuda nvidia-smi</span><br></pre></td></tr></table></figure>

<h3 id="build-and-run"><a href="#build-and-run" class="headerlink" title="build and run"></a>build and run</h3><p>build</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/PaddlePaddle/Anakin.git anakin</span><br><span class="line"><span class="built_in">cd</span> anakin/docker</span><br><span class="line">./anakin_docker_build_and_run.sh -p NVIDIA-GPU -o Ubuntu -m Build</span><br></pre></td></tr></table></figure>

<p>error occur with <code>cudnn</code>. skip.</p>
<p>run</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./anakin_docker_build_and_run.sh  -p NVIDIA-GPU -o Ubuntu -m Run</span><br></pre></td></tr></table></figure>

<h3 id="compile-anakin"><a href="#compile-anakin" class="headerlink" title="compile anakin"></a>compile anakin</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> docker run -it --runtime=nvidia fdcda959f60a bin/bash</span><br><span class="line">root@962077742ae9:/# <span class="built_in">cd</span> Anakin/</span><br><span class="line">git checkout developing</span><br></pre></td></tr></table></figure>

<p>build</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. use script to build</span></span><br><span class="line">./tools/gpu_build.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. or you can build directly.</span></span><br><span class="line"><span class="built_in">mkdir</span> build  </span><br><span class="line"><span class="built_in">cd</span> build  </span><br><span class="line">cmake ..  </span><br><span class="line">make -j8</span><br></pre></td></tr></table></figure>

<h4 id="x86-build"><a href="#x86-build" class="headerlink" title="x86 build"></a>x86 build</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./tools/x86_build.sh</span><br></pre></td></tr></table></figure>
<p>OK. no errors.</p>
<h4 id="gpu-build"><a href="#gpu-build" class="headerlink" title="gpu build"></a>gpu build</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./tools/gpu_build.sh</span><br></pre></td></tr></table></figure>

<p>build errors occur. no <code>cudnn</code> found.</p>
<h2 id="compile-anakin-in-host"><a href="#compile-anakin-in-host" class="headerlink" title="compile anakin in host"></a>compile anakin in host</h2><h3 id="install-protobuf"><a href="#install-protobuf" class="headerlink" title="install protobuf"></a>install protobuf</h3><p>install protobuf 3.4.0, see <a href="https://kezunlin.me/post/d60ff6fe/">Part 1: compile protobuf-cpp on ubuntu 16.04</a></p>
<h3 id="configure-env"><a href="#configure-env" class="headerlink" title="configure env"></a>configure env</h3><p>vim .bashrc </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cuda for anakin</span></span><br><span class="line"><span class="built_in">export</span> PATH=/usr/local/cuda/bin:<span class="variable">$PATH</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># CUDNN for anakin</span></span><br><span class="line"><span class="built_in">export</span> CUDNN_ROOT=/usr/local/cuda/</span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=<span class="variable">$&#123;CUDNN_ROOT&#125;</span>/lib64:<span class="variable">$LD_LIBRARY_PATH</span></span><br><span class="line"><span class="built_in">export</span> CPLUS_INCLUDE_PATH=<span class="variable">$&#123;CUDNN_ROOT&#125;</span>/include:<span class="variable">$CPLUS_INCLUDE_PATH</span></span><br></pre></td></tr></table></figure>

<p>source .bashrc</p>
<h3 id="build-anakin"><a href="#build-anakin" class="headerlink" title="build anakin"></a>build anakin</h3><h4 id="x86-build-1"><a href="#x86-build-1" class="headerlink" title="x86 build"></a>x86 build</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git checkout developing  </span><br><span class="line">./tools/x86_build.sh</span><br><span class="line"></span><br><span class="line"><span class="built_in">mv</span> output x86_output</span><br></pre></td></tr></table></figure>
<p>OK. no errors.</p>
<p>if error occurs, then </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">rm</span> -rf CMakeFiles</span><br><span class="line"><span class="built_in">rm</span> -rf anakin/framework/model_parser/proto/*.h</span><br><span class="line"><span class="built_in">rm</span> output</span><br><span class="line"></span><br><span class="line"><span class="built_in">chown</span> -R kezunlin:kezunlin anakin</span><br></pre></td></tr></table></figure>

<h4 id="gpu-build-1"><a href="#gpu-build-1" class="headerlink" title="gpu build"></a>gpu build</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">./tools/gpu_build.sh</span><br><span class="line"><span class="built_in">mv</span> output gpu_output</span><br></pre></td></tr></table></figure>

<h4 id="gpu-build-with-cmake"><a href="#gpu-build-with-cmake" class="headerlink" title="gpu build with cmake"></a>gpu build with cmake</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> anakin</span><br><span class="line"><span class="built_in">mkdir</span> build</span><br><span class="line"><span class="built_in">cd</span> build &amp;&amp; cmake-gui ..</span><br></pre></td></tr></table></figure>

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- kzl in-article ad -->
<p><ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-5653382914441020"
     data-ad-slot="7925631830"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h2 id="anakin-overview"><a href="#anakin-overview" class="headerlink" title="anakin overview"></a>anakin overview</h2><p><img src="https://kezunlin.me/images/posts/635233-20180906173919156-600432026.png" alt="anakin"></p>
<p>用Anakin来进行前向计算主要分为三个步骤：</p>
<ol>
<li>将外部模型通过Anakin Parser解析为Anakin模型</li>
<li>加载Anakin模型生成原始计算图，然后需要对原始计算图进行优化。</li>
<li>Anakin会选择不同硬件平台执行计算图。</li>
</ol>
<h3 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h3><p><code>Tensor</code>接受三个模板参数:</p>
<pre><code> template&lt;typename TargetType, DataType datatype, typename LayOutType = NCHW&gt;
 class Tensor .../* Inherit other class */&#123;
  //some implements
  ...
 &#125;;
</code></pre>
<ul>
<li><a href="#target">TargetType</a>是平台类型，如X86，GPU等等，在Anakin内部有相应的标识与之对应；</li>
<li><a href="#datatype">datatype</a>是普通的数据类型，在Anakin内部也有相应的标志与之对应；</li>
<li><a href="#layout">LayOutType</a>是数据分布类型，如batch x channel x height x width [NxCxHxW], 在Anakin内部用一个struct来标识。 Anakin中数据类型与基本数据类型的对应如下:</li>
</ul>
<p><span id='target'>TargetType</span></p>
<p>Anakin TargetType | platform<br>:—-: |<br>NV | NVIDIA GPU<br>ARM | ARM<br>AMD | AMD GPU<br>X86 | X86<br>NVHX86 | NVIDIA GPU with Pinned Memory</p>
<p><sapn id='datatype'>DataType</span></p>
<table>
<thead>
<tr>
<th align="center">Anakin DataType</th>
<th align="center">C++</th>
<th align="center">Description</th>
</tr>
</thead>
<tbody><tr>
<td align="center">AK_HALF</td>
<td align="center">short</td>
<td align="center">fp16</td>
</tr>
<tr>
<td align="center">AK_FLOAT</td>
<td align="center">float</td>
<td align="center">fp32</td>
</tr>
<tr>
<td align="center">AK_DOUBLE</td>
<td align="center">double</td>
<td align="center">fp64</td>
</tr>
<tr>
<td align="center">AK_INT8</td>
<td align="center">char</td>
<td align="center">int8</td>
</tr>
<tr>
<td align="center">AK_INT16</td>
<td align="center">short</td>
<td align="center">int16</td>
</tr>
<tr>
<td align="center">AK_INT32</td>
<td align="center">int</td>
<td align="center">int32</td>
</tr>
<tr>
<td align="center">AK_INT64</td>
<td align="center">long</td>
<td align="center">int64</td>
</tr>
<tr>
<td align="center">AK_UINT8</td>
<td align="center">unsigned char</td>
<td align="center">uint8</td>
</tr>
<tr>
<td align="center">AK_UINT16</td>
<td align="center">unsigned short</td>
<td align="center">uint8</td>
</tr>
<tr>
<td align="center">AK_UINT32</td>
<td align="center">unsigned int</td>
<td align="center">uint32</td>
</tr>
<tr>
<td align="center">AK_STRING</td>
<td align="center">std::string</td>
<td align="center">&#x2F;</td>
</tr>
<tr>
<td align="center">AK_BOOL</td>
<td align="center">bool</td>
<td align="center">&#x2F;</td>
</tr>
<tr>
<td align="center">AK_SHAPE</td>
<td align="center">&#x2F;</td>
<td align="center">Anakin Shape</td>
</tr>
<tr>
<td align="center">AK_TENSOR</td>
<td align="center">&#x2F;</td>
<td align="center">Anakin Tensor</td>
</tr>
</tbody></table>
<p><span id = 'layout'>LayOutType </span></p>
<table>
<thead>
<tr>
<th align="center">Anakin LayOutType ( Tensor LayOut )</th>
<th align="center">Tensor Dimention</th>
<th align="center">Tensor Support</th>
<th align="center">Op Support</th>
</tr>
</thead>
<tbody><tr>
<td align="center">W</td>
<td align="center">1-D</td>
<td align="center">YES</td>
<td align="center">NO</td>
</tr>
<tr>
<td align="center">HW</td>
<td align="center">2-D</td>
<td align="center">YES</td>
<td align="center">NO</td>
</tr>
<tr>
<td align="center">WH</td>
<td align="center">2-D</td>
<td align="center">YES</td>
<td align="center">NO</td>
</tr>
<tr>
<td align="center">NW</td>
<td align="center">2-D</td>
<td align="center">YES</td>
<td align="center">YES</td>
</tr>
<tr>
<td align="center">NHW</td>
<td align="center">3-D</td>
<td align="center">YES</td>
<td align="center">YES</td>
</tr>
<tr>
<td align="center">NCHW ( default )</td>
<td align="center">4-D</td>
<td align="center">YES</td>
<td align="center">YES</td>
</tr>
<tr>
<td align="center">NHWC</td>
<td align="center">4-D</td>
<td align="center">YES</td>
<td align="center">NO</td>
</tr>
<tr>
<td align="center">NCHW_C4</td>
<td align="center">5-D</td>
<td align="center">YES</td>
<td align="center">YES</td>
</tr>
</tbody></table>
<p>理论上，Anakin支持申明1维以上的tensor，但是对于Anakin中的Op来说，只支持NW、NHW、NCHW、NCHW_C4这四种LayOut，其中NCHW是默认的LayOutType，NCHW_C4是专门针对于int8这种数据类型的。</p>
<h3 id="Graph"><a href="#Graph" class="headerlink" title="Graph"></a>Graph</h3><p><code>Graph</code>类负责加载Anakin模型生成计算图、对图进行优化、存储模型等操作。</p>
<pre><code>template&lt;typename TargetType, DataType Dtype, Precision Ptype&gt;
class Graph ... /* inherit other class*/&#123;

  //some implements
  ...

&#125;;
</code></pre>
<h4 id="load"><a href="#load" class="headerlink" title="load"></a>load</h4><pre><code>//some declarations
...
auto graph = new Graph&lt;NV, AK_FLOAT, Precision::FP32&gt;();
std::string model_path = &quot;the/path/to/where/your/models/are&quot;;
const char *model_path1 = &quot;the/path/to/where/your/models/are&quot;;

//Loading Anakin model to generate a compute graph.
auto status = graph-&gt;load(model_path);

//Or this way.
auto status = graph-&gt;load(model_path1);
//Check whether load operation success.
if(!status)&#123;
  std::cout &lt;&lt; &quot;error&quot; &lt;&lt; endl;
  //do something...
&#125;
</code></pre>
<h4 id="optimize"><a href="#optimize" class="headerlink" title="optimize"></a>optimize</h4><pre><code>//some declarations
...
//Load graph.
...
//According to the ops of loaded graph, optimize compute graph.
graph-&gt;Optimize();
</code></pre>
<h4 id="save"><a href="#save" class="headerlink" title="save"></a>save</h4><pre><code>//some declarations
...
//Load graph.
...
// save a model
//save_model_path: the path to where your model is.
auto status = graph-&gt;save(save_model_path);

//Checking
if(!status)&#123;
  cout &lt;&lt; &quot;error&quot; &lt;&lt; endl;
  //do somethin...
&#125;
</code></pre>
<h3 id="Net"><a href="#Net" class="headerlink" title="Net"></a>Net</h3><p><code>Net</code>是计算图的执行器，通过Net对象获得输入和输出。</p>
<pre><code>template&lt;typename TargetType, DataType Dtype, Precision PType, OpRunType RunType = OpRunType::ASYNC&gt;
class Net&#123;
  //some implements
  ...

&#125;;
</code></pre>
<ul>
<li><a href="#precision">Precision</a>指定Op的精度。</li>
<li><a href="#opruntype">OpRunType</a>表示同步或异步类型，异步是默认类型。OpRunType::SYNC表示同步，在GPU上只有单个流；OpRunType::ASYNC表示异步，在GPU上有多个流并以异步方式执行。</li>
</ul>
<p><span id = 'precision'> Precision </span></p>
<table>
<thead>
<tr>
<th align="center">Precision</th>
<th align="center">Op support</th>
</tr>
</thead>
<tbody><tr>
<td align="center">Precision::INT4</td>
<td align="center">NO</td>
</tr>
<tr>
<td align="center">Precision::INT8</td>
<td align="center">NO</td>
</tr>
<tr>
<td align="center">Precision::FP16</td>
<td align="center">NO</td>
</tr>
<tr>
<td align="center">Precision::FP32</td>
<td align="center">YES</td>
</tr>
<tr>
<td align="center">Precision::FP64</td>
<td align="center">NO</td>
</tr>
</tbody></table>
<p>现在Op的精度只支持FP32， 但在将来我们会支持剩下的Precision.</p>
<p><span id = 'opruntype'> OpRunType </span></p>
<table>
<thead>
<tr>
<th align="center">OpRunType</th>
<th align="center">Sync&#x2F;Aync</th>
<th align="center">Description</th>
</tr>
</thead>
<tbody><tr>
<td align="center">OpRunType::SYNC</td>
<td align="center">Synchronization</td>
<td align="center">single-stream on GPU</td>
</tr>
<tr>
<td align="center">OpRunType::ASYNC</td>
<td align="center">Asynchronization</td>
<td align="center">multi-stream on GPU</td>
</tr>
</tbody></table>
<h4 id="create-a-executor"><a href="#create-a-executor" class="headerlink" title="create a executor"></a>create a executor</h4><pre><code>//some declarations
...
//Create a pointer to a graph.
auto graph = new Graph&lt;NV, AK_FLOAT, Precision::FP32&gt;();
//do something...
...

//create a executor
Net&lt;NV, AK_FLOAT, Precision::FP32&gt; executor(*graph);
</code></pre>
<h4 id="get-input-tensor"><a href="#get-input-tensor" class="headerlink" title="get input tensor"></a>get input tensor</h4><pre><code>//some declaratinos
...

//create a executor
//TargetType is NV [NVIDIA GPU]
Net&lt;NV, AK_FLOAT, Precision::FP32&gt; executor(*graph);

//Get the first input tensor.
//The following tensors(tensor_in0, tensor_in2 ...) are resident at GPU.
//Note: Member function get_in returns an pointer to tensor.
Tensor&lt;NV, AK_FLOAT&gt;* tensor_in0 = executor.get_in(&quot;input_0&quot;);

//If you have multiple input tensors
//You just type this code below.
Tensor&lt;NV, AK_FLOAT&gt;* tensor_in1 = executor.get_in(&quot;input_1&quot;);
...
auto tensor_inn = executor.get_in(&quot;input_n&quot;);
</code></pre>
<h4 id="fill-input-tensor"><a href="#fill-input-tensor" class="headerlink" title="fill input tensor"></a>fill input tensor</h4><pre><code>//This tensor is resident at GPU.
auto tensor_d_in = executor.get_in(&quot;input_0&quot;);

//If we want to feed above tensor, we must feed the tensor which is resident at host. And then copy the host tensor to the device&#39;s one.

//using Tensor4d = Tensor&lt;Ttype, Dtype&gt;;
Tensor4d&lt;X86, AK_FLOAT&gt; tensor_h_in; //host tensor;
//Tensor&lt;X86, AK_FLOAT&gt; tensor_h_in; 

//Allocate memory for host tensor.
tensor_h_in.re_alloc(tensor_d_in-&gt;valid_shape());
//Get a writable pointer to tensor.
float *h_data = tensor_h_in.mutable_data();

//Feed your tensor.
/** example
for(int i = 0; i &lt; tensor_h_in.size(); i++)&#123;
  h_data[i] = 1.0f;
&#125;
*/
//Copy host tensor&#39;s data to device tensor.
tensor_d_in-&gt;copy_from(tensor_h_in);

// And then
</code></pre>
<h4 id="get-output-tensor"><a href="#get-output-tensor" class="headerlink" title="get output tensor"></a>get output tensor</h4><pre><code>//Note: this tensor are resident at GPU.
Tensor&lt;NV, AK_FLOAT&gt;* tensor_out_d = executor.get_out(&quot;pred_out&quot;);
</code></pre>
<h4 id="execute-graph"><a href="#execute-graph" class="headerlink" title="execute graph"></a>execute graph</h4><pre><code>executor.prediction();
</code></pre>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- kzl in-article ad -->
<p><ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-5653382914441020"
     data-ad-slot="7925631830"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h3 id="code-example"><a href="#code-example" class="headerlink" title="code example"></a>code example</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">std::string model_path = <span class="string">&quot;your_Anakin_models/xxxxx.anakin.bin&quot;</span>;</span><br><span class="line"><span class="comment">// Create an empty graph object.</span></span><br><span class="line"><span class="keyword">auto</span> graph = <span class="keyword">new</span> <span class="built_in">Graph</span>&lt;NV, AK_FLOAT, Precision::FP32&gt;();</span><br><span class="line"><span class="comment">// Load Anakin model.</span></span><br><span class="line"><span class="keyword">auto</span> status = graph-&gt;<span class="built_in">load</span>(model_path);</span><br><span class="line"><span class="keyword">if</span>(!status ) &#123;</span><br><span class="line">    <span class="built_in">LOG</span>(FATAL) &lt;&lt; <span class="string">&quot; [ERROR] &quot;</span> &lt;&lt; status.<span class="built_in">info</span>();</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// Reshape</span></span><br><span class="line">graph-&gt;<span class="built_in">Reshape</span>(<span class="string">&quot;input_0&quot;</span>, &#123;<span class="number">10</span>, <span class="number">384</span>, <span class="number">960</span>, <span class="number">10</span>&#125;);</span><br><span class="line"><span class="comment">// You must optimize graph for the first time.</span></span><br><span class="line">graph-&gt;<span class="built_in">Optimize</span>();</span><br><span class="line"><span class="comment">// Create a executer.</span></span><br><span class="line"><span class="function">Net&lt;NV, AK_FLOAT, Precision::FP32&gt; <span class="title">net_executer</span><span class="params">(*graph)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//Get your input tensors through some specific string such as &quot;input_0&quot;, &quot;input_1&quot;, and </span></span><br><span class="line"><span class="comment">//so on. </span></span><br><span class="line"><span class="comment">//And then, feed the input tensor.</span></span><br><span class="line"><span class="comment">//If you don&#x27;t know Which input do these specific string (&quot;input_0&quot;, &quot;input_1&quot;) correspond with, you can launch dash board to find out.</span></span><br><span class="line"><span class="keyword">auto</span> d_tensor_in_p = net_executer.<span class="built_in">get_in</span>(<span class="string">&quot;input_0&quot;</span>);</span><br><span class="line">Tensor4d&lt;X86, AK_FLOAT&gt; h_tensor_in;</span><br><span class="line"><span class="keyword">auto</span> valid_shape_in = d_tensor_in_p-&gt;<span class="built_in">valid_shape</span>();</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>; i&lt;valid_shape_in.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">    <span class="built_in">LOG</span>(INFO) &lt;&lt; <span class="string">&quot;detect input dims[&quot;</span> &lt;&lt; i &lt;&lt; <span class="string">&quot;]&quot;</span> &lt;&lt; valid_shape_in[i]; <span class="comment">//see tensor&#x27;s dimentions</span></span><br><span class="line">&#125;</span><br><span class="line">h_tensor_in.<span class="built_in">re_alloc</span>(valid_shape_in);</span><br><span class="line"><span class="type">float</span>* h_data = h_tensor_in.<span class="built_in">mutable_data</span>();</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>; i&lt;h_tensor_in.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">    h_data[i] = <span class="number">1.0f</span>;</span><br><span class="line">&#125;</span><br><span class="line">d_tensor_in_p-&gt;<span class="built_in">copy_from</span>(h_tensor_in);</span><br><span class="line"></span><br><span class="line"><span class="comment">//Do inference.</span></span><br><span class="line">net_executer.<span class="built_in">prediction</span>();</span><br><span class="line"></span><br><span class="line"><span class="comment">//Get result tensor through the name of output node.</span></span><br><span class="line"><span class="comment">//And also, you need to see the dash board again to find out how many output nodes are and remember their name.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//For example, you&#x27;ve got a output node named obj_pre_out</span></span><br><span class="line"><span class="comment">//Then, you can get an output tensor.</span></span><br><span class="line"><span class="keyword">auto</span> d_tensor_out_0_p = net_executer.<span class="built_in">get_out</span>(<span class="string">&quot;obj_pred_out&quot;</span>); <span class="comment">//get_out returns a pointer to output tensor.</span></span><br><span class="line"><span class="keyword">auto</span> d_tensor_out_1_p = net_executer.<span class="built_in">get_out</span>(<span class="string">&quot;lc_pred_out&quot;</span>); <span class="comment">//get_out returns a pointer to output tensor.</span></span><br><span class="line"><span class="comment">//......</span></span><br><span class="line"><span class="comment">// do something else ...</span></span><br><span class="line"><span class="comment">//...</span></span><br><span class="line"><span class="comment">//save model.</span></span><br><span class="line"><span class="comment">//You might not optimize the graph when you load the saved model again.</span></span><br><span class="line">std::string save_model_path = model_path + std::<span class="built_in">string</span>(<span class="string">&quot;.saved&quot;</span>);</span><br><span class="line"><span class="keyword">auto</span> status = graph-&gt;<span class="built_in">save</span>(save_model_path);</span><br><span class="line"><span class="keyword">if</span> (!status ) &#123;</span><br><span class="line">    <span class="built_in">LOG</span>(FATAL) &lt;&lt; <span class="string">&quot; [ERROR] &quot;</span> &lt;&lt; status.<span class="built_in">info</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="anakin-converter"><a href="#anakin-converter" class="headerlink" title="anakin converter"></a>anakin converter</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> anakin/tools/external_converter_v2</span><br><span class="line"><span class="built_in">sudo</span> pip install flask prettytable</span><br><span class="line"></span><br><span class="line">vim config.yaml </span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line">python converter.py</span><br></pre></td></tr></table></figure>

<p>config.yaml</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">OPTIONS:</span></span><br><span class="line">    <span class="attr">Framework:</span> <span class="string">CAFFE</span></span><br><span class="line">    <span class="attr">SavePath:</span> <span class="string">./output</span></span><br><span class="line">    <span class="attr">ResultName:</span> <span class="string">mylenet</span></span><br><span class="line">    <span class="attr">Config:</span></span><br><span class="line">        <span class="attr">LaunchBoard:</span> <span class="string">ON</span></span><br><span class="line">        <span class="attr">Server:</span></span><br><span class="line">            <span class="attr">ip:</span> <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line">            <span class="attr">port:</span> <span class="number">8888</span></span><br><span class="line">        <span class="attr">OptimizedGraph:</span> </span><br><span class="line">            <span class="attr">enable:</span> <span class="string">OFF</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">./anakin_optimized/lenet.anakin.bin.saved</span></span><br><span class="line">    <span class="attr">LOGGER:</span></span><br><span class="line">        <span class="attr">LogToPath:</span> <span class="string">./log/</span></span><br><span class="line">        <span class="attr">WithColor:</span> <span class="string">ON</span> </span><br><span class="line"></span><br><span class="line"><span class="attr">TARGET:</span></span><br><span class="line">    <span class="attr">CAFFE:</span></span><br><span class="line">        <span class="comment"># path to proto files</span></span><br><span class="line">        <span class="attr">ProtoPaths:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">/home/kezunlin/program/caffe/src/caffe/proto/caffe.proto</span></span><br><span class="line">        <span class="attr">PrototxtPath:</span> <span class="string">/home/kezunlin/program/caffe/examples/mnist/lenet.prototxt</span></span><br><span class="line">        <span class="attr">ModelPath:</span> <span class="string">/home/kezunlin/program/caffe/examples/mnist/lenet_iter_10000.caffemodel</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">FLUID:</span></span><br><span class="line">        <span class="comment"># path of fluid inference model</span></span><br><span class="line">        <span class="attr">Debug:</span> <span class="literal">NULL</span>                            <span class="comment"># Generally no need to modify.</span></span><br><span class="line">        <span class="attr">ModelPath:</span> <span class="string">/path/to/your/model/</span>        <span class="comment"># The upper path of a fluid inference model.</span></span><br><span class="line">        <span class="attr">NetType:</span>                               <span class="comment"># Generally no need to modify.</span></span><br><span class="line">    </span><br><span class="line">    <span class="attr">LEGO:</span></span><br><span class="line">        <span class="comment"># path to proto files</span></span><br><span class="line">        <span class="attr">ProtoPath:</span></span><br><span class="line">        <span class="attr">PrototxtPath:</span></span><br><span class="line">        <span class="attr">ModelPath:</span></span><br><span class="line">    </span><br><span class="line">    <span class="attr">TENSORFLOW:</span></span><br><span class="line">        <span class="attr">ProtoPaths:</span> <span class="string">/</span></span><br><span class="line">        <span class="attr">PrototxtPath:</span> <span class="string">/</span></span><br><span class="line">        <span class="attr">ModelPath:</span> <span class="string">/</span></span><br><span class="line">        <span class="attr">OutPuts:</span></span><br><span class="line">    </span><br><span class="line">    <span class="attr">ONNX:</span></span><br><span class="line">        <span class="attr">ProtoPath:</span></span><br><span class="line">        <span class="attr">PrototxtPath:</span></span><br><span class="line">        <span class="attr">ModelPath:</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>input: caffe.proto + lenet.prototxt + lenet_iter_10000.caffemodel</li>
<li>output: output&#x2F;mylenet.anakin.bin + log&#x2F;xxx.log</li>
</ul>
<h2 id="anakin-test"><a href="#anakin-test" class="headerlink" title="anakin test"></a>anakin test</h2><h3 id="model-test-cpp"><a href="#model-test-cpp" class="headerlink" title="model_test.cpp"></a>model_test.cpp</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> Anakin/test/framework/net/model_test.cpp</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> gpu_output</span><br><span class="line">./unit_test/model_test <span class="string">&#x27;/home/kezunlin/program/anakin/demo/model/&#x27;</span> </span><br></pre></td></tr></table></figure>

<h3 id="example-nv-cnn-net-cpp"><a href="#example-nv-cnn-net-cpp" class="headerlink" title="example_nv_cnn_net.cpp"></a>example_nv_cnn_net.cpp</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> Anakin/examples/cuda/example_nv_cnn_net.cpp</span><br></pre></td></tr></table></figure>

<h2 id="my-example"><a href="#my-example" class="headerlink" title="my example"></a>my example</h2><h3 id="my-workspace"><a href="#my-workspace" class="headerlink" title="my workspace"></a>my workspace</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ls</span> demo/</span><br><span class="line">anakin_lib  build  cmake  CMakeLists.txt  image  model  src</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tree demo/src/ demo/model/ demo/cmake demo/image</span><br><span class="line">demo/src/</span><br><span class="line">└── demo.cpp</span><br><span class="line">demo/model/</span><br><span class="line">└── mylenet.anakin.bin</span><br><span class="line">demo/cmake</span><br><span class="line">├── anakin-config.cmake</span><br><span class="line">├── msg_color.cmake</span><br><span class="line">├── statistic.cmake</span><br><span class="line">└── utils.cmake</span><br><span class="line">demo/image</span><br><span class="line">├── big.jpg</span><br><span class="line">└── cat.jpg</span><br><span class="line"></span><br><span class="line">0 directories, 8 files</span><br></pre></td></tr></table></figure>

<h3 id="anakin-lib"><a href="#anakin-lib" class="headerlink" title="anakin_lib"></a>anakin_lib</h3><p>use <code>./tools/gpu_build.sh</code> to generate <code>gpu_build_sm61</code> and rename to <code>anakin_lib</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">./tools/gpu_build.sh</span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">mv</span> gpu_build_sm61 anakin_lib</span><br><span class="line"></span><br><span class="line"><span class="built_in">ls</span> anakin_lib/</span><br><span class="line">anakin_config.h  libanakin_saber_common.so        libanakin.so        <span class="built_in">log</span>    unit_test</span><br><span class="line">framework        libanakin_saber_common.so.0.1.2  libanakin.so.0.1.2  saber  utils</span><br></pre></td></tr></table></figure>

<h4 id="anakin-config-cmake"><a href="#anakin-config-cmake" class="headerlink" title="anakin-config.cmake"></a>anakin-config.cmake</h4><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span>(ANAKIN_FOUND <span class="keyword">TRUE</span>) <span class="comment"># auto </span></span><br><span class="line"><span class="keyword">set</span>(ANAKIN_VERSION <span class="number">0.1</span>.<span class="number">2</span>)</span><br><span class="line"><span class="keyword">set</span>(ANAKIN_ROOT_DIR <span class="string">&quot;/home/kezunlin/program/anakin/demo/anakin_lib&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span>(ANAKIN_ROOT <span class="variable">$&#123;ANAKIN_ROOT_DIR&#125;</span>)</span><br><span class="line"><span class="keyword">set</span>(ANAKIN_FRAMEWORK <span class="variable">$&#123;ANAKIN_ROOT&#125;</span>/framework)</span><br><span class="line"><span class="keyword">set</span>(ANAKIN_SABER <span class="variable">$&#123;ANAKIN_ROOT&#125;</span>/saber)</span><br><span class="line"><span class="keyword">set</span>(ANAKIN_UTILS <span class="variable">$&#123;ANAKIN_ROOT&#125;</span>/utils)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span>(ANAKIN_FRAMEWORK_CORE <span class="variable">$&#123;ANAKIN_FRAMEWORK&#125;</span>/core)</span><br><span class="line"><span class="keyword">set</span>(ANAKIN_FRAMEWORK_GRAPH <span class="variable">$&#123;ANAKIN_FRAMEWORK&#125;</span>/graph)</span><br><span class="line"><span class="keyword">set</span>(ANAKIN_FRAMEWORK_LITE <span class="variable">$&#123;ANAKIN_FRAMEWORK&#125;</span>/lite)</span><br><span class="line"><span class="keyword">set</span>(ANAKIN_FRAMEWORK_MODEL_PARSER <span class="variable">$&#123;ANAKIN_FRAMEWORK&#125;</span>/model_parser)</span><br><span class="line"><span class="keyword">set</span>(ANAKIN_FRAMEWORK_OPERATORS <span class="variable">$&#123;ANAKIN_FRAMEWORK&#125;</span>/operators)</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span>(ANAKIN_SABER_CORE <span class="variable">$&#123;ANAKIN_SABER&#125;</span>/core)</span><br><span class="line"><span class="keyword">set</span>(ANAKIN_SABER_FUNCS <span class="variable">$&#123;ANAKIN_SABER&#125;</span>/funcs)</span><br><span class="line"><span class="keyword">set</span>(ANAKIN_SABER_LITE <span class="variable">$&#123;ANAKIN_SABER&#125;</span>/lite)</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span>(ANAKIN_UTILS_LOGGER <span class="variable">$&#123;ANAKIN_UTILS&#125;</span>/logger)</span><br><span class="line"><span class="keyword">set</span>(ANAKIN_UTILS_UINT_TEST <span class="variable">$&#123;ANAKIN_UTILS&#125;</span>/unit_test)</span><br><span class="line"></span><br><span class="line"><span class="comment">#find_path(ANAKIN_INCLUDE_DIR NAMES anakin_config.h PATHS &quot;$&#123;ANAKIN_ROOT_DIR&#125;&quot;) </span></span><br><span class="line"><span class="keyword">mark_as_advanced</span>(ANAKIN_INCLUDE_DIR) <span class="comment"># show entry in cmake-gui</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">find_library</span>(ANAKIN_SABER_COMMON_LIBRARY NAMES anakin_saber_common PATHS <span class="string">&quot;$&#123;ANAKIN_ROOT_DIR&#125;&quot;</span>) </span><br><span class="line"><span class="keyword">mark_as_advanced</span>(ANAKIN_SABER_COMMON_LIBRARY) <span class="comment"># show entry in cmake-gui</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">find_library</span>(ANAKIN_LIBRARY NAMES anakin PATHS <span class="string">&quot;$&#123;ANAKIN_ROOT_DIR&#125;&quot;</span>) </span><br><span class="line"><span class="keyword">mark_as_advanced</span>(ANAKIN_LIBRARY) <span class="comment"># show entry in cmake-gui</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># use xxx_INCLUDE_DIRS and xxx_LIBRARIES in CMakeLists.txt</span></span><br><span class="line"><span class="keyword">set</span>(ANAKIN_INCLUDE_DIRS </span><br><span class="line">    <span class="variable">$&#123;ANAKIN_ROOT&#125;</span> </span><br><span class="line">    <span class="variable">$&#123;ANAKIN_FRAMEWORK&#125;</span> </span><br><span class="line">    <span class="variable">$&#123;ANAKIN_SABER&#125;</span> </span><br><span class="line">    <span class="variable">$&#123;ANAKIN_UTILS&#125;</span> </span><br><span class="line"></span><br><span class="line">    <span class="variable">$&#123;ANAKIN_FRAMEWORK_CORE&#125;</span> </span><br><span class="line">    <span class="variable">$&#123;ANAKIN_FRAMEWORK_GRAPH&#125;</span> </span><br><span class="line">    <span class="variable">$&#123;ANAKIN_FRAMEWORK_LITE&#125;</span> </span><br><span class="line">    <span class="variable">$&#123;ANAKIN_FRAMEWORK_MODEL_PARSER&#125;</span> </span><br><span class="line">    <span class="variable">$&#123;ANAKIN_FRAMEWORK_OPERATORS&#125;</span> </span><br><span class="line"></span><br><span class="line">    <span class="variable">$&#123;ANAKIN_SABER_CORE&#125;</span> </span><br><span class="line">    <span class="variable">$&#123;ANAKIN_SABER_FUNCS&#125;</span> </span><br><span class="line">    <span class="variable">$&#123;ANAKIN_SABER_LITE&#125;</span> </span><br><span class="line"></span><br><span class="line">    <span class="variable">$&#123;ANAKIN_UTILS_LOGGER&#125;</span> </span><br><span class="line">    <span class="variable">$&#123;ANAKIN_UTILS_UINT_TEST&#125;</span> </span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span>(ANAKIN_LIBRARIES <span class="variable">$&#123;ANAKIN_SABER_COMMON_LIBRARY&#125;</span> <span class="variable">$&#123;ANAKIN_LIBRARY&#125;</span> )</span><br><span class="line"></span><br><span class="line"><span class="keyword">message</span>( <span class="string">&quot;anakin-config.cmake &quot;</span> <span class="variable">$&#123;ANAKIN_ROOT_DIR&#125;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="CMakeLists-txt"><a href="#CMakeLists-txt" class="headerlink" title="CMakeLists.txt"></a>CMakeLists.txt</h3><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">2.8</span>.<span class="number">8</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">project</span>(demo)</span><br><span class="line"></span><br><span class="line"><span class="keyword">include</span>(cmake/msg_color.cmake)</span><br><span class="line"><span class="keyword">include</span>(cmake/utils.cmake)</span><br><span class="line"><span class="keyword">include</span>(cmake/statistic.cmake)</span><br><span class="line"></span><br><span class="line"><span class="comment">#add_definitions( -Dshared_DEBUG) # define macro</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span>(CMAKE_CXX_FLAGS <span class="string">&quot;$&#123;CMAKE_CXX_FLAGS&#125; -std=c++11&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span>(ROOT_CMAKE_DIR ./cmake)</span><br><span class="line"><span class="keyword">set</span>(CMAKE_PREFIX_PATH <span class="variable">$&#123;CMAKE_PREFIX_PATH&#125;</span> <span class="string">&quot;$&#123;ROOT_CMAKE_DIR&#125;;$&#123;CMAKE_PREFIX_PATH&#125;&quot;</span>)</span><br><span class="line"><span class="keyword">MESSAGE</span>( [cmake] <span class="string">&quot; CMAKE_PREFIX_PATH = $&#123;CMAKE_PREFIX_PATH&#125; for find_package&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Find includes in corresponding build directories</span></span><br><span class="line"><span class="keyword">set</span>(CMAKE_INCLUDE_CURRENT_DIR <span class="keyword">ON</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">find_package</span>(OpenCV REQUIRED COMPONENTS core highgui imgproc features2d calib3d) </span><br><span class="line"><span class="keyword">include_directories</span>(<span class="variable">$&#123;OpenCV_INCLUDE_DIRS&#125;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># find anakin-config.cmake file</span></span><br><span class="line"><span class="comment">#include(cmake/anakin-config.cmake)</span></span><br><span class="line"><span class="keyword">find_package</span>(ANAKIN REQUIRED)</span><br><span class="line"><span class="keyword">include_directories</span>(<span class="variable">$&#123;ANAKIN_INCLUDE_DIRS&#125;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#message( [opencv] $&#123;OpenCV_INCLUDE_DIRS&#125; )</span></span><br><span class="line"><span class="comment">#message( [opencv] $&#123;OpenCV_LIBS&#125; )</span></span><br><span class="line"><span class="comment">#message( [anakin] $&#123;ANAKIN_INCLUDE_DIRS&#125; )</span></span><br><span class="line"><span class="comment">#message( [anakin] $&#123;ANAKIN_LIBRARIES&#125; )</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">add_executable</span>(<span class="variable">$&#123;PROJECT_NAME&#125;</span> </span><br><span class="line">    src/demo.cpp</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># dl pthread </span></span><br><span class="line"><span class="comment"># error with  -std=c++11 -lpthread -ldl </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">target_link_libraries</span>(<span class="variable">$&#123;PROJECT_NAME&#125;</span> </span><br><span class="line">    dl </span><br><span class="line">    pthread</span><br><span class="line">    <span class="variable">$&#123;OpenCV_LIBS&#125;</span> </span><br><span class="line">    <span class="variable">$&#123;ANAKIN_LIBRARIES&#125;</span></span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="src-demo-cpp"><a href="#src-demo-cpp" class="headerlink" title="src&#x2F;demo.cpp"></a>src&#x2F;demo.cpp</h3><p>edit from <code>Anakin/examples/cuda/example_nv_cnn_net.cpp</code></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="comment">// opencv</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;opencv2/core.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;opencv2/imgproc.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;opencv2/highgui.hpp&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> cv;</span><br><span class="line"></span><br><span class="line"><span class="comment">// anakin</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;utils/logger/logger.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;framework/graph/graph.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;framework/core/net/net.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*util to fill tensor*/</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;saber/core/tensor_op.h&quot;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> anakin;</span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> anakin::graph;</span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> anakin::saber;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">+------------+-----------------+-------+-----------+</span></span><br><span class="line"><span class="comment">| Input Name |      Shape      | Alias | Data Type |</span></span><br><span class="line"><span class="comment">+------------+-----------------+-------+-----------+</span></span><br><span class="line"><span class="comment">|  input_0   | [64, 1, 28, 28] |  NULL |    NULL   |</span></span><br><span class="line"><span class="comment">+------------+-----------------+-------+-----------+</span></span><br><span class="line"><span class="comment">+-------------+</span></span><br><span class="line"><span class="comment">| Output Name |</span></span><br><span class="line"><span class="comment">+-------------+</span></span><br><span class="line"><span class="comment">|   prob_out  |</span></span><br><span class="line"><span class="comment">+-------------+</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">fill_tensor</span><span class="params">(Tensor4d&lt;X86, AK_FLOAT&gt;&amp; h_tensor_in, <span class="type">const</span> cv::Mat&amp; image)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// write data to tensor</span></span><br><span class="line">    <span class="type">int</span> height = image.rows;</span><br><span class="line">    <span class="type">int</span> width = image.cols;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">LOG</span>(INFO)&lt;&lt;<span class="string">&quot;height*width =&quot;</span>&lt;&lt; height*width &lt;&lt;std::endl;  <span class="comment">// 784</span></span><br><span class="line">    <span class="built_in">LOG</span>(INFO)&lt;&lt;<span class="string">&quot;h_tensor_in.size() =&quot;</span>&lt;&lt;h_tensor_in.<span class="built_in">size</span>()&lt;&lt;std::endl; <span class="comment">// 784</span></span><br><span class="line"></span><br><span class="line">    <span class="type">float</span>* tensor_ptr = h_tensor_in.<span class="built_in">mutable_data</span>(); <span class="comment">// int, float or double.</span></span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">float</span>* ptr;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> h = <span class="number">0</span>; h &lt; height; ++h)</span><br><span class="line">    &#123;</span><br><span class="line">        ptr = image.<span class="built_in">ptr</span>&lt;<span class="type">float</span>&gt;(h); <span class="comment">// row ptr</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> w = <span class="number">0</span>; w &lt; width; ++w)</span><br><span class="line">        &#123;</span><br><span class="line">            *tensor_ptr++ = *ptr++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">const</span> <span class="type">char</span>** argv)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">char</span> *model_path = <span class="string">&quot;../model/mylenet.anakin.bin&quot;</span>;</span><br><span class="line"></span><br><span class="line">    Mat image = <span class="built_in">imread</span>(<span class="string">&quot;../image/cat.jpg&quot;</span>,<span class="number">0</span>);</span><br><span class="line">    cv::<span class="built_in">resize</span>(image,image,<span class="built_in">Size</span>(<span class="number">28</span>,<span class="number">28</span>));</span><br><span class="line">    <span class="comment">//imshow(&quot;image&quot;,image);</span></span><br><span class="line">    <span class="comment">//waitKey(0);</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/*init graph object, graph is the skeleton of model*/</span></span><br><span class="line">    Graph&lt;NV, AK_FLOAT, Precision::FP32&gt; graph;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*load model from file to init the graph*/</span></span><br><span class="line">    <span class="keyword">auto</span> status = graph.<span class="built_in">load</span>(model_path);</span><br><span class="line">    <span class="keyword">if</span> (!status) &#123;</span><br><span class="line">        <span class="built_in">LOG</span>(FATAL) &lt;&lt; <span class="string">&quot; [ERROR] &quot;</span> &lt;&lt; status.<span class="built_in">info</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*set net input shape and use this shape to optimize the graph(fusion and init operator),shape is n,c,h,w*/</span></span><br><span class="line">    graph.<span class="built_in">Reshape</span>(<span class="string">&quot;input_0&quot;</span>, &#123;<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>&#125;);</span><br><span class="line">    graph.<span class="built_in">Optimize</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*net_executer is the executor object of model. use graph to init Net*/</span></span><br><span class="line">    <span class="function">Net&lt;NV, AK_FLOAT, Precision::FP32&gt; <span class="title">net_executer</span><span class="params">(graph, <span class="literal">true</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*use input string to get the input tensor of net. for we use NV as target, the tensor of net_executer is on GPU memory*/</span></span><br><span class="line">    <span class="keyword">auto</span> d_tensor_in_p = net_executer.<span class="built_in">get_in</span>(<span class="string">&quot;input_0&quot;</span>);</span><br><span class="line">    <span class="keyword">auto</span> valid_shape_in = d_tensor_in_p-&gt;<span class="built_in">valid_shape</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*create tensor located in host*/</span></span><br><span class="line">    Tensor4d&lt;X86, AK_FLOAT&gt; h_tensor_in;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*alloc for host tensor*/</span></span><br><span class="line">    h_tensor_in.<span class="built_in">re_alloc</span>(valid_shape_in);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*init host tensor by random*/</span></span><br><span class="line">    <span class="comment">//fill_tensor_host_rand(h_tensor_in, -1.0f, 1.0f);</span></span><br><span class="line"></span><br><span class="line">    image.<span class="built_in">convertTo</span>(image, CV_32FC1); <span class="comment">// faster</span></span><br><span class="line">    <span class="built_in">fill_tensor</span>(h_tensor_in,image);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*use host tensor to int device tensor which is net input*/</span></span><br><span class="line">    d_tensor_in_p-&gt;<span class="built_in">copy_from</span>(h_tensor_in);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*run infer*/</span></span><br><span class="line">    net_executer.<span class="built_in">prediction</span>();</span><br><span class="line"></span><br><span class="line">    <span class="built_in">LOG</span>(INFO)&lt;&lt;<span class="string">&quot;infer finish&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*get the out put of net, which is a device tensor*/</span></span><br><span class="line">    <span class="keyword">auto</span> d_out=net_executer.<span class="built_in">get_out</span>(<span class="string">&quot;prob_out&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*create another host tensor, and copy the content of device tensor to host*/</span></span><br><span class="line">    Tensor4d&lt;X86, AK_FLOAT&gt; h_tensor_out;</span><br><span class="line">    h_tensor_out.<span class="built_in">re_alloc</span>(d_out-&gt;<span class="built_in">valid_shape</span>());</span><br><span class="line">    h_tensor_out.<span class="built_in">copy_from</span>(*d_out);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*show output content*/</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;h_tensor_out.<span class="built_in">valid_size</span>();i++)&#123;</span><br><span class="line">        <span class="built_in">LOG</span>(INFO)&lt;&lt;<span class="string">&quot;out [&quot;</span>&lt;&lt;i&lt;&lt;<span class="string">&quot;] = &quot;</span>&lt;&lt;h_tensor_out.<span class="built_in">data</span>()[i];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>compile demo</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> build</span><br><span class="line"><span class="built_in">cd</span> build </span><br><span class="line">cmake ..</span><br><span class="line">make </span><br><span class="line">./demo </span><br></pre></td></tr></table></figure>

<p>output </p>
<pre><code>ERR| 16:45:56.00581| 110838.067s|         37CBF8C0| operator_attr.h:94]  you have set the argument: is_reverse , so it&#39;s igrored by anakin
 ERR| 16:45:56.00581| 110838.067s|         37CBF8C0| operator_attr.h:94]  you have set the argument: is_reverse , so it&#39;s igrored by anakin
   0| 16:45:56.00681| 0.098s|         37CBF8C0| parser.cpp:96] graph name: LeNet
   0| 16:45:56.00681| 0.099s|         37CBF8C0| parser.cpp:101] graph in: input_0
   0| 16:45:56.00681| 0.099s|         37CBF8C0| parser.cpp:107] graph out: prob_out
   0| 16:45:56.00742| 0.159s|         37CBF8C0| graph.cpp:153]  processing in-ordered fusion : ConvBatchnormScaleReluPool
   0| 16:45:56.00742| 0.160s|         37CBF8C0| graph.cpp:153]  processing in-ordered fusion : ConvBatchnormScaleRelu
   0| 16:45:56.00742| 0.160s|         37CBF8C0| graph.cpp:153]  processing in-ordered fusion : ConvReluPool
   0| 16:45:56.00742| 0.160s|         37CBF8C0| graph.cpp:153]  processing in-ordered fusion : ConvBatchnormScale
   0| 16:45:56.00742| 0.160s|         37CBF8C0| graph.cpp:153]  processing in-ordered fusion : DeconvRelu
   0| 16:45:56.00742| 0.160s|         37CBF8C0| graph.cpp:153]  processing in-ordered fusion : ConvRelu
   0| 16:45:56.00742| 0.160s|         37CBF8C0| graph.cpp:153]  processing in-ordered fusion : PermutePower
   0| 16:45:56.00742| 0.160s|         37CBF8C0| graph.cpp:153]  processing in-ordered fusion : ConvBatchnorm
   0| 16:45:56.00742| 0.160s|         37CBF8C0| graph.cpp:153]  processing in-ordered fusion : EltwiseRelu
   0| 16:45:56.00742| 0.160s|         37CBF8C0| graph.cpp:153]  processing in-ordered fusion : EltwiseActivation
 WAN| 16:45:56.00743| 0.160s|         37CBF8C0| net.cpp:663] Detect and initial 1 lanes.
   0| 16:45:56.00743| 0.161s|         37CBF8C0| env.h:44] found 1 device(s)
   0| 16:45:56.00743| 0.161s|         37CBF8C0| cuda_device.cpp:45] Device id: 0 , name: GeForce GTX 1060
   0| 16:45:56.00743| 0.161s|         37CBF8C0| cuda_device.cpp:47] Multiprocessors: 10
   0| 16:45:56.00743| 0.161s|         37CBF8C0| cuda_device.cpp:50] frequency:1733MHz
   0| 16:45:56.00743| 0.161s|         37CBF8C0| cuda_device.cpp:52] CUDA Capability : 6.1
   0| 16:45:56.00743| 0.161s|         37CBF8C0| cuda_device.cpp:54] total global memory: 6078MBytes.
 WAN| 16:45:56.00743| 0.161s|         37CBF8C0| net.cpp:667] Current used device id : 0
 WAN| 16:45:56.00744| 0.161s|         37CBF8C0| input.cpp:16] Parsing Input op parameter.
   0| 16:45:56.00744| 0.161s|         37CBF8C0| input.cpp:19]  |-- shape [0]: 1
   0| 16:45:56.00744| 0.161s|         37CBF8C0| input.cpp:19]  |-- shape [1]: 1
   0| 16:45:56.00744| 0.161s|         37CBF8C0| input.cpp:19]  |-- shape [2]: 28
   0| 16:45:56.00744| 0.161s|         37CBF8C0| input.cpp:19]  |-- shape [3]: 28
 ERR| 16:45:56.00744| 0.161s|         37CBF8C0| net.cpp:210] node_ptr-&gt;get_op_name()  sass not support yet.
 ERR| 16:45:56.00744| 0.161s|         37CBF8C0| net.cpp:210] node_ptr-&gt;get_op_name()  sass not support yet.
 WAN| 16:45:57.00269| 0.686s|         37CBF8C0| context.h:40] device index exceeds the number of devices, set to default device(0)!
   0| 16:45:57.00270| 0.687s|         37CBF8C0| net.cpp:300] Temp mem used:        0 MB
   0| 16:45:57.00270| 0.687s|         37CBF8C0| net.cpp:301] Original mem used:    0 MB
   0| 16:45:57.00270| 0.687s|         37CBF8C0| net.cpp:302] Model mem used:       1 MB
   0| 16:45:57.00270| 0.687s|         37CBF8C0| net.cpp:303] System mem used:      153 MB
   0| 16:45:57.00270| 0.687s|         37CBF8C0| demo.cpp:40] height*width =784
   0| 16:45:57.00270| 0.687s|         37CBF8C0| demo.cpp:41] h_tensor_in.size() =784
   0| 16:45:57.00270| 0.688s|         37CBF8C0| demo.cpp:105] infer finish
   0| 16:45:57.00270| 0.688s|         37CBF8C0| demo.cpp:117] out [0] = 0
   0| 16:45:57.00270| 0.688s|         37CBF8C0| demo.cpp:117] out [1] = 0
   0| 16:45:57.00270| 0.688s|         37CBF8C0| demo.cpp:117] out [2] = 0
   0| 16:45:57.00270| 0.688s|         37CBF8C0| demo.cpp:117] out [3] = 1
   0| 16:45:57.00270| 0.688s|         37CBF8C0| demo.cpp:117] out [4] = 0
   0| 16:45:57.00270| 0.688s|         37CBF8C0| demo.cpp:117] out [5] = 0
   0| 16:45:57.00270| 0.688s|         37CBF8C0| demo.cpp:117] out [6] = 0
   0| 16:45:57.00270| 0.688s|         37CBF8C0| demo.cpp:117] out [7] = 0
   0| 16:45:57.00270| 0.688s|         37CBF8C0| demo.cpp:117] out [8] = 0
   0| 16:45:57.00270| 0.688s|         37CBF8C0| demo.cpp:117] out [9] = 0
</code></pre>
<h2 id="For-Windows-skip"><a href="#For-Windows-skip" class="headerlink" title="For Windows (skip)"></a>For Windows (skip)</h2><h3 id="version-1"><a href="#version-1" class="headerlink" title="version"></a>version</h3><ul>
<li>windows 10</li>
<li>vs 2015</li>
<li>cmake 3.2.2</li>
<li>cuda 8.0 + cudnn 6.0.21 (same as caffe) sm_61</li>
<li><a target="_blank" rel="noopener" href="https://github.com/protocolbuffers/protobuf/releases/download/v3.4.0/protobuf-cpp-3.4.0.zip">protobuf 3.4.0</a></li>
</ul>
<h3 id="protobuf"><a href="#protobuf" class="headerlink" title="protobuf"></a>protobuf</h3><p>see <a href="https://kezunlin.me/post/876d75f2/">compile protobuf-cpp on windows 10</a></p>
<h3 id="compile"><a href="#compile" class="headerlink" title="compile"></a>compile</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#git clone https://github.com/PaddlePaddle/Anakin.git anakin</span></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/kezunlin/Anakin.git anakin</span><br><span class="line"><span class="built_in">cd</span> anakin </span><br><span class="line"><span class="built_in">mkdir</span> build &amp;&amp; <span class="built_in">cd</span> build &amp;&amp; cmake-gui ..</span><br></pre></td></tr></table></figure>

<p>with options </p>
<pre><code>CUDNN_ROOT &quot;C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0/&quot;
PROTOBUF_ROOT &quot;C:/Program Files/protobuf&quot; 

BUILD_SHARED ON
USE_GPU_PLACE ON
USE_OPENMP OFF
USE_OPENCV ON
</code></pre>
<p>generate <code>Anakin.sln</code> and compile with <code>VS 2015</code> with <code>x64 Release</code> mode.</p>
<h3 id="error-fixs"><a href="#error-fixs" class="headerlink" title="error fixs"></a>error fixs</h3><blockquote>
<p>we get 101 errors, hard to fix.<br>skip now.</p>
</blockquote>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/PaddlePaddle/Anakin/blob/developing/docker/README.md">anakin with docker on ubuntu</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/PaddlePaddle/Anakin/blob/developing/docs/Manual/Tutorial_ch.md">anakin tutorial</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/PaddlePaddle/Anakin/blob/developing/docs/Manual/Converter_ch.md">Converter</a></li>
</ul>
<h2 id="History"><a href="#History" class="headerlink" title="History"></a>History</h2><ul>
<li>20180903: created.</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kezunlin.me/blog/nvidia-docker2-guide-on-ubuntu-16-04/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="kezunlin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kezunlin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/nvidia-docker2-guide-on-ubuntu-16-04/" class="post-title-link" itemprop="url">how to install docker and nvidia-docker2  on ubuntu 16.04</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-09-03 17:43:00" itemprop="dateCreated datePublished" datetime="2018-09-03T17:43:00+08:00">2018-09-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-10-14 13:39:28" itemprop="dateModified" datetime="2024-10-14T13:39:28+08:00">2024-10-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Docker-Guide"><a href="#Docker-Guide" class="headerlink" title="Docker Guide"></a>Docker Guide</h2><h3 id="install-docker"><a href="#install-docker" class="headerlink" title="install docker"></a>install docker</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># step 1: install tools</span></span><br><span class="line"><span class="built_in">sudo</span> apt-get update</span><br><span class="line"><span class="built_in">sudo</span> apt-get -y install apt-transport-https ca-certificates curl software-properties-common</span><br><span class="line"></span><br><span class="line"><span class="comment"># step 2: install GPG </span></span><br><span class="line">curl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | <span class="built_in">sudo</span> apt-key add -</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 3: add apt repo</span></span><br><span class="line"><span class="built_in">sudo</span> add-apt-repository <span class="string">&quot;deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu <span class="subst">$(lsb_release -cs)</span> stable&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 4: install docker-ce</span></span><br><span class="line"><span class="built_in">sudo</span> apt-get -y update</span><br><span class="line"><span class="built_in">sudo</span> apt-get -y install docker-ce</span><br></pre></td></tr></table></figure>

<p>install docker-ce for given version</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Step 1: search versions</span></span><br><span class="line"><span class="comment"># apt-cache madison docker-ce</span></span><br><span class="line"><span class="comment">#   docker-ce | 17.03.1~ce-0~ubuntu-xenial | http://mirrors.aliyun.com/docker-ce/linux/ubuntu xenial/stable amd64 Packages</span></span><br><span class="line"><span class="comment">#   docker-ce | 17.03.0~ce-0~ubuntu-xenial | http://mirrors.aliyun.com/docker-ce/linux/ubuntu xenial/stable amd64 Packages</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 2: install given version</span></span><br><span class="line"><span class="comment"># sudo apt-get -y install docker-ce=17.03.1~ce-0~ubuntu-xenial</span></span><br></pre></td></tr></table></figure>

<h3 id="test-docker"><a href="#test-docker" class="headerlink" title="test docker"></a>test docker</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> docker version</span><br><span class="line">Client:</span><br><span class="line"> Version:           18.06.1-ce</span><br><span class="line"> API version:       1.38</span><br><span class="line"> Go version:        go1.10.3</span><br><span class="line"> Git commit:        e68fc7a</span><br><span class="line"> Built:             Tue Aug 21 17:24:56 2018</span><br><span class="line"> OS/Arch:           linux/amd64</span><br><span class="line"> Experimental:      <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">Server:</span><br><span class="line"> Engine:</span><br><span class="line">  Version:          18.06.1-ce</span><br><span class="line">  API version:      1.38 (minimum version 1.12)</span><br><span class="line">  Go version:       go1.10.3</span><br><span class="line">  Git commit:       e68fc7a</span><br><span class="line">  Built:            Tue Aug 21 17:23:21 2018</span><br><span class="line">  OS/Arch:          linux/amd64</span><br><span class="line">  Experimental:     <span class="literal">false</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h3 id="docker-namespace"><a href="#docker-namespace" class="headerlink" title="docker namespace"></a>docker namespace</h3><h4 id="host"><a href="#host" class="headerlink" title="host"></a>host</h4><pre><code>id
uid=1000(kezunlin) gid=1000(kezunlin) groups=1000(kezunlin),4(adm),24(cdrom),27(sudo),30(dip),46(plugdev),113(lpadmin),128(sambashare)

sudo docker images
sudo docker run -it --name kzl -v /home/kezunlin/workspace/:/home/kezunlin/workspace nvidia/cuda
</code></pre>
<h4 id="container"><a href="#container" class="headerlink" title="container"></a>container</h4><pre><code>root@6f167ef72a80:/home/kezunlin/workspace# ll
total 48
drwxrwxr-x 12 1000 1000 4096 Nov 30 10:04 ./
drwxr-xr-x  3 root root 4096 Nov 30 10:14 ../
drwxrwxr-x 10 1000 1000 4096 Dec  5  2017 MyGit/
drwxrwxr-x 12 1000 1000 4096 Oct 31 03:01 blog/
drwxrwxr-x  5 1000 1000 4096 Sep 20 07:33 opencv/
drwxrwxr-x  4 1000 1000 4096 Oct 31 07:55 openmp/
drwxrwxr-x  5 1000 1000 4096 Jan  9  2018 qt/
drwxrwxr-x  2 1000 1000 4096 Jan  4  2018 ros/
drwxrwxr-x  4 1000 1000 4096 Nov 16  2017 voc/
drwxrwxr-x  5 1000 1000 4096 Aug  7 03:19 vs/
root@6f167ef72a80:/home/kezunlin/workspace# touch 1.txt

root@6f167ef72a80:/home/kezunlin/workspace# id
uid=0(root) gid=0(root) groups=0(root)
</code></pre>
<h4 id="host-1"><a href="#host-1" class="headerlink" title="host"></a>host</h4><pre><code>ll /home/kezunlin/workspace/
total 48
drwxrwxr-x 12 kezunlin kezunlin 4096 11月 30 18:14 ./
drwxr-xr-x 47 kezunlin kezunlin 4096 11月 30 18:04 ../

-rw-r--r--  1 root     root        0 11月 30 18:14 1.txt

drwxrwxr-x 12 kezunlin kezunlin 4096 10月 31 11:01 blog/
drwxrwxr-x  5 kezunlin kezunlin 4096 9月  20 15:33 opencv/
drwxrwxr-x  4 kezunlin kezunlin 4096 10月 31 15:55 openmp/
drwxrwxr-x  5 kezunlin kezunlin 4096 1月   9  2018 qt/
drwxrwxr-x  2 kezunlin kezunlin 4096 1月   4  2018 ros/
drwxrwxr-x  4 kezunlin kezunlin 4096 11月 16  2017 voc/
drwxrwxr-x  5 kezunlin kezunlin 4096 8月   7 11:19 vs/
</code></pre>
<h3 id="install-nvidia-docker2"><a href="#install-nvidia-docker2" class="headerlink" title="install nvidia-docker2"></a>install nvidia-docker2</h3><blockquote>
<p>The machine running the CUDA container only requires the NVIDIA driver, the CUDA toolkit doesn’t have to be installed.<br>Host系统只需要安装NVIDIA driver即可运行CUDA container。</p>
</blockquote>
<h4 id="install"><a href="#install" class="headerlink" title="install"></a>install</h4><p>remove nvidia-docker 1.0</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># If you have nvidia-docker 1.0 installed: we need to remove it and all existing GPU containers</span></span><br><span class="line">docker volume <span class="built_in">ls</span> -q -f driver=nvidia-docker | xargs -r -I&#123;&#125; -n1 docker ps -q -a -f volume=&#123;&#125; | xargs -r docker <span class="built_in">rm</span> -f</span><br><span class="line"><span class="built_in">sudo</span> apt-get purge -y nvidia-docker</span><br></pre></td></tr></table></figure>

<p>Add the package repositories</p>
<p>vim repo.sh</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | \</span><br><span class="line">  <span class="built_in">sudo</span> apt-key add -</span><br><span class="line">distribution=$(. /etc/os-release;<span class="built_in">echo</span> $ID<span class="variable">$VERSION_ID</span>)</span><br><span class="line">curl -s -L https://nvidia.github.io/nvidia-docker/<span class="variable">$distribution</span>/nvidia-docker.list | \</span><br><span class="line">  <span class="built_in">sudo</span> <span class="built_in">tee</span> /etc/apt/sources.list.d/nvidia-docker.list</span><br></pre></td></tr></table></figure>

<p>run scripts </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">chmod</span> +x repo.sh</span><br><span class="line">./repo.sh</span><br></pre></td></tr></table></figure>

<p>Install nvidia-docker2 and reload the Docker daemon configuration</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> apt-get install -y nvidia-docker2</span><br><span class="line"><span class="built_in">sudo</span> pkill -SIGHUP dockerd</span><br></pre></td></tr></table></figure>

<h4 id="test"><a href="#test" class="headerlink" title="test"></a>test</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> docker run --runtime=nvidia --<span class="built_in">rm</span> nvidia/cuda nvidia-smi</span><br></pre></td></tr></table></figure>
<p>output </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">Unable to find image <span class="string">&#x27;nvidia/cuda:latest&#x27;</span> locally</span><br><span class="line">latest: Pulling from nvidia/cuda</span><br><span class="line">8ee29e426c26: Pull complete </span><br><span class="line">6e83b260b73b: Pull complete </span><br><span class="line">e26b65fd1143: Pull complete </span><br><span class="line">40dca07f8222: Pull complete </span><br><span class="line">b420ae9e10b3: Pull complete </span><br><span class="line">a579c1327556: Pull complete </span><br><span class="line">b440bb8df79e: Pull complete </span><br><span class="line">de3b2ccf9562: Pull complete </span><br><span class="line">a69a544d350e: Pull complete </span><br><span class="line">02348b5db71c: Pull complete </span><br><span class="line">Digest: sha256:5996fa2fc0666972360502fe32118286177b879a8a1a834a176e7786021b8cee</span><br><span class="line">Status: Downloaded newer image <span class="keyword">for</span> nvidia/cuda:latest</span><br><span class="line">Mon Sep  3 10:08:27 2018       </span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 384.130                Driver Version: 384.130                   |</span><br><span class="line">|-------------------------------+----------------------+----------------------+</span><br><span class="line">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|===============================+======================+======================|</span><br><span class="line">|   0  GeForce GTX 1060    Off  | 00000000:01:00.0 Off |                  N/A |</span><br><span class="line">| N/A   59C    P8     8W /  N/A |    408MiB /  6072MiB |     40%      Default |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line">                                                                               </span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                       GPU Memory |</span><br><span class="line">|  GPU       PID   Type   Process name                             Usage      |</span><br><span class="line">|=============================================================================|</span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>or by tty</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> docker run --runtime=nvidia -t -i --privileged nvidia/cuda bash</span><br><span class="line"></span><br><span class="line">root@8f3ebd5ecbb6:/# nvidia-smi</span><br><span class="line">Tue Sep  4 01:26:31 2018       </span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 384.130                Driver Version: 384.130                   |</span><br><span class="line">|-------------------------------+----------------------+----------------------+</span><br><span class="line">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|===============================+======================+======================|</span><br><span class="line">|   0  GeForce GTX 1060    Off  | 00000000:01:00.0 Off |                  N/A |</span><br><span class="line">| N/A   56C    P0    31W /  N/A |    374MiB /  6072MiB |      0%      Default |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line">                                                                               </span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                       GPU Memory |</span><br><span class="line">|  GPU       PID   Type   Process name                             Usage      |</span><br><span class="line">|=============================================================================|</span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- kzl in-article ad -->
<p><ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-5653382914441020"
     data-ad-slot="7925631830"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h3 id="Advanced-Topics"><a href="#Advanced-Topics" class="headerlink" title="Advanced Topics"></a>Advanced Topics</h3><h4 id="Default-runtime"><a href="#Default-runtime" class="headerlink" title="Default runtime"></a>Default runtime</h4><p>The default runtime used by the Docker® Engine is <code>runc</code>, our runtime can become the default one by configuring the docker daemon with <code>--default-runtime=nvidia</code>. Doing so will remove the need to add the <code>--runtime=nvidia</code> argument to docker run. It is also the only way to have GPU access during docker build.</p>
<h4 id="Environment-variables"><a href="#Environment-variables" class="headerlink" title="Environment variables"></a>Environment variables</h4><p>The behavior of the runtime can be modified through environment variables (such as <code>NVIDIA_VISIBLE_DEVICES</code>).<br>Those environment variables are consumed by <code>nvidia-container-runtime</code> and are documented here.<br>Our official CUDA images use default values for these variables.</p>
<h3 id="docker-command"><a href="#docker-command" class="headerlink" title="docker command"></a>docker command</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> docker image list</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">nvidia/cuda         latest              04a9ce0dec6d        3 weeks ago         1.96GB</span><br><span class="line"></span><br><span class="line"><span class="built_in">sudo</span> docker run -it --privileged nvidia/cuda bash</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">docker build --network=host -t anakin:<span class="variable">$tag</span> . -f <span class="variable">$DockerfilePath</span></span><br></pre></td></tr></table></figure>

<h2 id="kubernetes-with-GPU"><a href="#kubernetes-with-GPU" class="headerlink" title="kubernetes with GPU"></a>kubernetes with GPU</h2><p>kubernetes 对于 GPU 的支持截止到 1.9 版本，算是经历了3个阶段：</p>
<ul>
<li><p>kubernetes 1.3 版本开始支持GPU，但是只支持单个 GPU卡；</p>
</li>
<li><p>kubernetes 1.6 版本开始支持对多个GPU卡的支持；</p>
</li>
<li><p>kubernetes 1.8 版本以 device plugin 方式提供对GPU的支持。</p>
<p>ls &#x2F;dev&#x2F;nvidia*<br>&#x2F;dev&#x2F;nvidia0  &#x2F;dev&#x2F;nvidia2  &#x2F;dev&#x2F;nvidia4  &#x2F;dev&#x2F;nvidia6  &#x2F;dev&#x2F;nvidiactl<br>&#x2F;dev&#x2F;nvidia1  &#x2F;dev&#x2F;nvidia3  &#x2F;dev&#x2F;nvidia5  &#x2F;dev&#x2F;nvidia7</p>
</li>
<li><p>Kubernetes 1.8~1.9，通过<code>k8s-device-plugin</code> 获取每个Node上GPU的信息，根据这些信息对GPU资源进行管理和调度。需要结合 nvidia-docker2 使用。</p>
</li>
<li><p><code>k8s-device-plugin</code>也是由 nvidia 提供，在kubernetes中可以DaemonSet方式运行。</p>
</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/nvidia-docker/wiki/Installation-(version-2.0)#prerequisites">nvidia-docker2</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.docker.com/engine/reference/commandline/run/">docker command</a></li>
<li><a target="_blank" rel="noopener" href="http://ju.outofmemory.cn/entry/348390">GPU 在 docker 和 kubernetes 中的使用</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/27376696">Kubernetes GPU使用指南</a></li>
<li><a target="_blank" rel="noopener" href="https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/">scheduling-gpus</a></li>
</ul>
<h2 id="History"><a href="#History" class="headerlink" title="History"></a>History</h2><ul>
<li>20180903: created.</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kezunlin.me/blog/install-and-configure-tensorflow-on-windows-10/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="kezunlin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kezunlin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/install-and-configure-tensorflow-on-windows-10/" class="post-title-link" itemprop="url">install and configure tensorflow on windows 10</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-08-29 18:54:00" itemprop="dateCreated datePublished" datetime="2018-08-29T18:54:00+08:00">2018-08-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-10-14 13:39:28" itemprop="dateModified" datetime="2024-10-14T13:39:28+08:00">2024-10-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Series"><a href="#Series" class="headerlink" title="Series"></a>Series</h2><ul>
<li><a href="https://kezunlin.me/post/f7eab850/">Part 1: Tensorflow for Unbutu 16.04</a></li>
<li><strong><a href="https://kezunlin.me/post/a123bf9e/">Part 2: Tensorflow for Windows 10</a></strong></li>
</ul>
<h2 id="Tutorial"><a href="#Tutorial" class="headerlink" title="Tutorial"></a>Tutorial</h2><h3 id="version"><a href="#version" class="headerlink" title="version"></a>version</h3><p>version 1:</p>
<ul>
<li>windows 10 64 bit + GTX 1060(8G) + cuda driver</li>
<li>windows 10 64 bit + GTX 1080(12G) + cuda driver</li>
<li>CUDA 8.0 + cudnn 6.0.1(win10) + tensorflow-gpu 1.4.0</li>
<li>python 3.5.3</li>
</ul>
<p>version 2:</p>
<ul>
<li>windows 10 64 bit + GeForce Titan Xp(12G) + cuda driver for Titan xp</li>
<li>CUDA 9.0 + cudnn 7.1.4(win10) + tensorflow-gpu 1.8.0 ( 1.8.0, 1.9.0 for cuda 9.0)</li>
</ul>
<p>version 3: </p>
<ul>
<li>windows 10 64 bit + Quadro P4000(8G) + cuda driver for Quadro P4000(实测用Titan Xp的driver也可以)</li>
<li>CUDA 9.0 + cudnn 7.1.4(win10) + tensorflow-gpu 1.8.0 ( 1.8.0, 1.9.0 for cuda 9.0)</li>
</ul>
<p>errors </p>
<pre><code>error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows
</code></pre>
<p>see <a target="_blank" rel="noopener" href="https://pypi.org/project/tensorflow-gpu/1.4.0/#files">tensorflow-gpu&#x3D;&#x3D;1.4.0</a></p>
<blockquote>
<p>Tips:  for tensorflow-gpu&#x3D;&#x3D;1.4.0<br>on linux, support python 2.7,3.3,3.4,3.5,3.6.<br>on windows, only support python 3.5,3.6. </p>
</blockquote>
<p>see <a target="_blank" rel="noopener" href="https://pypi.org/project/tensorflow-gpu/1.8.0/#files">tensorflow-gpu&#x3D;&#x3D;1.8.0</a></p>
<blockquote>
<p>Tips:  for tensorflow-gpu&#x3D;&#x3D;1.8.0<br>on linux, support python 2.7,3.3,3.4,3.5,3.6.<br>on windows, only support python 3.5,3.6. </p>
</blockquote>
<p>from <code>Tensorflow1.6</code> use <code>CUDA9.0+cuDNN7</code>.</p>
<p><img src="https://kezunlin.me/images/posts/635233-20180830181732605-573127103.png" alt="tensorflow download pages"></p>
<h3 id="cuda-cudnn"><a href="#cuda-cudnn" class="headerlink" title="cuda &amp; cudnn"></a>cuda &amp; cudnn</h3><p>see <a href="https://kezunlin.me/post/1739694c/">Part 1: Install and Configure Caffe on windows 10</a></p>
<p>system env </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0\bin</span><br></pre></td></tr></table></figure>

<h3 id="python"><a href="#python" class="headerlink" title="python"></a>python</h3><p>install <code>python 3.5.3</code>,add python and pip path to system env.</p>
<p>copy <code>python.exe</code> to <code>python3.exe</code>,<br>copy <code>pip.exe</code> to <code>pip3.exe</code></p>
<p>system env</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">C:\Users\zunli\AppData\Local\Programs\Python\Python35\</span><br><span class="line">C:\Users\zunli\AppData\Local\Programs\Python\Python35\Scripts</span><br></pre></td></tr></table></figure>

<h4 id="test"><a href="#test" class="headerlink" title="test"></a>test</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">python3</span><br><span class="line">Python 3.5.3 (v3.5.3:1880cb95a742, Jan 16 2017, 16:02:32) [MSC v.1900 64 bit (AMD64)] on win32</span><br><span class="line">Type <span class="string">&quot;help&quot;</span>, <span class="string">&quot;copyright&quot;</span>, <span class="string">&quot;credits&quot;</span> or <span class="string">&quot;license&quot;</span> <span class="keyword">for</span> more information.</span><br><span class="line">&gt;&gt;&gt; ^Z</span><br></pre></td></tr></table></figure>

<p>pip3 </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip3 -V</span><br><span class="line">pip 9.0.1 from c:\<span class="built_in">users</span>\zunli\appdata\<span class="built_in">local</span>\programs\python\python35\lib\site-packages (python 3.5)</span><br></pre></td></tr></table></figure>

<h3 id="tensorflow"><a href="#tensorflow" class="headerlink" title="tensorflow"></a>tensorflow</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple Pillow scipy sklearn scikit-image matplotlib</span><br></pre></td></tr></table></figure>

<h4 id="1-4-0"><a href="#1-4-0" class="headerlink" title="1.4.0"></a>1.4.0</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple tensorflow-gpu==1.4.0 keras=2.1.0</span><br></pre></td></tr></table></figure>

<h4 id="1-8-0"><a href="#1-8-0" class="headerlink" title="1.8.0"></a>1.8.0</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple tensorflow-gpu==1.8.0 keras=2.2.0</span><br></pre></td></tr></table></figure>

<h4 id="test-tensorflow"><a href="#test-tensorflow" class="headerlink" title="test tensorflow"></a>test tensorflow</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">hello=tf.constant(<span class="string">&#x27;hhh&#x27;</span>)</span><br><span class="line">sess=tf.Session()</span><br><span class="line"><span class="built_in">print</span> (sess.run(hello))</span><br></pre></td></tr></table></figure>

<h4 id="test-cuda-and-gpu"><a href="#test-cuda-and-gpu" class="headerlink" title="test cuda and gpu"></a>test cuda and gpu</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.test.is_built_with_cuda()  <span class="comment"># 判断CUDA是否可以用</span></span><br><span class="line"></span><br><span class="line">b = tf.test.is_gpu_available(</span><br><span class="line">    cuda_only=<span class="literal">False</span>,</span><br><span class="line">    min_cuda_compute_capability=<span class="literal">None</span></span><br><span class="line">)  <span class="comment"># 判断GPU是否可以用</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br></pre></td></tr></table></figure>

<h4 id="test-gpu"><a href="#test-gpu" class="headerlink" title="test gpu"></a>test gpu</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">&#x27;/cpu:0&#x27;</span>):</span><br><span class="line">    a = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], shape=[<span class="number">3</span>], name=<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">    b = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], shape=[<span class="number">3</span>], name=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">&#x27;/gpu:0&#x27;</span>):</span><br><span class="line">    c = a + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意：allow_soft_placement=True表明：计算设备可自行选择，如果没有这个参数，会报错。</span></span><br><span class="line"><span class="comment"># 因为不是所有的操作都可以被放在GPU上，如果强行将无法放在GPU上的操作指定到GPU上，将会报错。</span></span><br><span class="line">sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=<span class="literal">True</span>, log_device_placement=<span class="literal">True</span>))</span><br><span class="line"><span class="comment"># sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))</span></span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"><span class="built_in">print</span>(sess.run(c))</span><br></pre></td></tr></table></figure>

<p><img src="https://kezunlin.me/images/posts/635233-20180913114222255-1520775956.png" alt="gpu run"></p>
<h3 id="pycharm"><a href="#pycharm" class="headerlink" title="pycharm"></a>pycharm</h3><p>run code with pycharm</p>
<p><img src="https://kezunlin.me/images/posts/635233-20180829180600357-194824745.png" alt="pycharm with python3"></p>
<h3 id="jupyter-notebook"><a href="#jupyter-notebook" class="headerlink" title="jupyter notebook"></a>jupyter notebook</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pip install ipykernel</span><br><span class="line">python -m ipykernel install --user --name=tensorflow</span><br><span class="line"></span><br><span class="line">Installed kernelspec tensorflow <span class="keyword">in</span> C:\Users\zunli\AppData\Roaming\jupyter\kernels\tensorflow</span><br></pre></td></tr></table></figure>

<h3 id="error-fix"><a href="#error-fix" class="headerlink" title="error fix"></a>error fix</h3><p>errors:</p>
<pre><code>No matching distribution found for tensorflow
</code></pre>
<p>solution: use <code>python 3.5</code> instead of <code>python 2.7</code></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/fo40225/tensorflow-windows-wheel">tensorflow-windows-wheel</a></li>
</ul>
<h2 id="History"><a href="#History" class="headerlink" title="History"></a>History</h2><ul>
<li>20180829: created.</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kezunlin.me/blog/c-11-lambda/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="kezunlin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kezunlin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/c-11-lambda/" class="post-title-link" itemprop="url">c++ 11 lambda</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-08-23 16:29:00" itemprop="dateCreated datePublished" datetime="2018-08-23T16:29:00+08:00">2018-08-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-10-14 13:39:28" itemprop="dateModified" datetime="2024-10-14T13:39:28+08:00">2024-10-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/cpp/" itemprop="url" rel="index"><span itemprop="name">cpp</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Guide"><a href="#Guide" class="headerlink" title="Guide"></a>Guide</h2><h3 id="syntax"><a href="#syntax" class="headerlink" title="syntax"></a>syntax</h3><p>syntax</p>
<pre><code>[ capture clause ] (parameters) -&gt; return-type  
&#123;   
   definition of method   
&#125; 
</code></pre>
<h3 id="capture"><a href="#capture" class="headerlink" title="capture"></a>capture</h3><p>We can capture external variables from enclosing scope by three ways :</p>
<pre><code>  Capture by reference
  Capture by value (making a copy)
  Capture by both (mixed capture)
</code></pre>
<p>Syntax used for capturing variables :</p>
<pre><code>    []:   capture nothing
  [&amp;] : capture all external variable by reference
  [=] : capture all external variable by value (making a copy)
  [a, &amp;b] : capture a by value and b by reference 
  [this] :	Capture the this pointer of the enclosing class
</code></pre>
<p>C++11中的Lambda表达式捕获外部变量主要有以下形式：</p>
<pre><code>捕获形式	说明
[]	不捕获任何外部变量
[变量名, …]	默认以值得形式捕获指定的多个外部变量（用逗号分隔），如果引用捕获，需要显示声明（使用&amp;说明符）
[this]	以值的形式捕获this指针
[=]	以值的形式捕获所有外部变量
[&amp;]	以引用形式捕获所有外部变量
[=, &amp;x]	变量x以引用形式捕获，其余变量以传值形式捕获
[&amp;, x]	变量x以值的形式捕获，其余变量以引用形式捕获
</code></pre>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- kzl in-article ad -->
<p><ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-5653382914441020"
     data-ad-slot="7925631830"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>



<h3 id="example-code"><a href="#example-code" class="headerlink" title="example code"></a>example code</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span> </span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std; </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">test_labmda_0</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// call lambda with ending ();</span></span><br><span class="line">    [] () </span><br><span class="line">    &#123; </span><br><span class="line">      cout &lt;&lt; <span class="string">&quot;Hello, my Greek friends&quot;</span>; </span><br><span class="line">    &#125;();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// return value </span></span><br><span class="line">    <span class="keyword">auto</span> l1 = [] () </span><br><span class="line">    &#123; </span><br><span class="line">      <span class="keyword">return</span> <span class="number">1</span>; </span><br><span class="line">    &#125; ; <span class="comment">// compiler knows this returns an integer</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> l2 = [] () -&gt; <span class="type">int</span> </span><br><span class="line">    &#123; </span><br><span class="line">      <span class="keyword">return</span> <span class="number">1</span>; </span><br><span class="line">    &#125; ; <span class="comment">// now we&#x27;re telling the compiler what we want</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Function to print vector </span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">printVector</span><span class="params">(<span class="type">const</span> vector&lt;<span class="type">int</span>&gt;&amp; v)</span> </span></span><br><span class="line"><span class="function"></span>&#123; </span><br><span class="line">    <span class="comment">// lambda expression to print vector </span></span><br><span class="line">    for_each(v.<span class="built_in">begin</span>(), v.<span class="built_in">end</span>(), [](<span class="type">int</span> i) </span><br><span class="line">    &#123; </span><br><span class="line">        std::cout &lt;&lt; i &lt;&lt; <span class="string">&quot; &quot;</span>; </span><br><span class="line">    &#125;); </span><br><span class="line">    cout &lt;&lt; endl; </span><br><span class="line">&#125; </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">test_lambda_1</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    vector&lt;<span class="type">int</span>&gt; v &#123;<span class="number">4</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">7</span>&#125;; </span><br><span class="line">    <span class="built_in">printVector</span>(v); </span><br><span class="line"></span><br><span class="line">    <span class="comment">// capture nothing</span></span><br><span class="line">    std::<span class="built_in">sort</span>(v.<span class="built_in">begin</span>(), v.<span class="built_in">end</span>(), [](<span class="type">const</span> <span class="type">int</span>&amp; a, <span class="type">const</span> <span class="type">int</span>&amp; b) -&gt; <span class="type">bool</span></span><br><span class="line">    &#123; </span><br><span class="line">        <span class="keyword">return</span> a &gt; b; </span><br><span class="line">    &#125;); </span><br><span class="line">    <span class="built_in">printVector</span>(v); </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> ans = <span class="built_in">accumulate</span>(v.<span class="built_in">begin</span>(),v.<span class="built_in">end</span>(),<span class="number">0</span>, </span><br><span class="line">      [](<span class="type">int</span> i,<span class="type">int</span> j)</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="keyword">return</span> i+j;</span><br><span class="line">      &#125;</span><br><span class="line">    );</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;SUM = &quot;</span> &lt;&lt; ans &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">test_lambda_2</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    vector&lt;<span class="type">int</span>&gt; v1 = &#123;<span class="number">3</span>, <span class="number">1</span>, <span class="number">7</span>, <span class="number">9</span>&#125;; </span><br><span class="line">    vector&lt;<span class="type">int</span>&gt; v2 = &#123;<span class="number">10</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">16</span>, <span class="number">9</span>&#125;; </span><br><span class="line">  </span><br><span class="line">    <span class="comment">//  access v1 and v2 by reference </span></span><br><span class="line">    <span class="keyword">auto</span> pushinto = [&amp;] (<span class="type">int</span> m) </span><br><span class="line">    &#123; </span><br><span class="line">        v<span class="number">1.</span><span class="built_in">push_back</span>(m); </span><br><span class="line">        v<span class="number">2.</span><span class="built_in">push_back</span>(m); </span><br><span class="line">    &#125;; </span><br><span class="line">  </span><br><span class="line">    <span class="comment">// it pushes 20 in both v1 and v2 </span></span><br><span class="line">    <span class="built_in">pushinto</span>(<span class="number">20</span>); </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// access v1 by value (copy) </span></span><br><span class="line">    <span class="keyword">auto</span> printv = [v1]() </span><br><span class="line">    &#123; </span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> p = v<span class="number">1.</span><span class="built_in">begin</span>(); p != v<span class="number">1.</span><span class="built_in">end</span>(); p++) </span><br><span class="line">        &#123; </span><br><span class="line">            cout &lt;&lt; *p &lt;&lt; <span class="string">&quot; &quot;</span>; </span><br><span class="line">        &#125; </span><br><span class="line">        cout &lt;&lt; endl; </span><br><span class="line">    &#125;; </span><br><span class="line">    <span class="built_in">printv</span>();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> N = <span class="number">5</span>; </span><br><span class="line">    <span class="comment">// below snippet find first number greater than N </span></span><br><span class="line">    <span class="comment">// [N]  denotes,   can access only N by value </span></span><br><span class="line">    vector&lt;<span class="type">int</span>&gt;:: iterator p = <span class="built_in">find_if</span>(v<span class="number">1.</span><span class="built_in">begin</span>(), v<span class="number">1.</span><span class="built_in">end</span>(), [N](<span class="type">int</span> i) </span><br><span class="line">    &#123; </span><br><span class="line">        <span class="keyword">return</span> i &gt; N; </span><br><span class="line">    &#125;); </span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;First number greater than 5 is : &quot;</span> &lt;&lt; *p &lt;&lt; endl; </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Foo</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Foo</span> () : _x( <span class="number">3</span> ) &#123;&#125;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">func</span> <span class="params">()</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="comment">// a very silly, but illustrative way of printing out the value of _x</span></span><br><span class="line">        [<span class="keyword">this</span>] () </span><br><span class="line">        &#123; </span><br><span class="line">          cout &lt;&lt; <span class="keyword">this</span>-&gt;_x; </span><br><span class="line">        &#125; ();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">        <span class="type">int</span> _x;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">test_labmda_3</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Foo f;</span><br><span class="line">    f.<span class="built_in">func</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">main_demo</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">   <span class="built_in">test_lambda_0</span>();</span><br><span class="line">   <span class="built_in">test_lambda_1</span>();</span><br><span class="line">   <span class="built_in">test_lambda_2</span>();</span><br><span class="line">   <span class="built_in">test_labmda_3</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> <span class="type">const</span> *argv[])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">main_demo</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a target="_blank" rel="noopener" href="https://www.cprogramming.com/c++11/c++11-lambda-closures.html">c++11-lambda-closures</a></li>
<li><a target="_blank" rel="noopener" href="https://appdividend.com/2019/06/19/c-lambda-function-tutorial-lambda-expression-in-c-example/">c-lambda-function-tutorial-lambda-expression-in-c-example</a></li>
<li><a target="_blank" rel="noopener" href="https://www.geeksforgeeks.org/lambda-expression-in-c/">lambda-expression-in-c</a></li>
</ul>
<h2 id="History"><a href="#History" class="headerlink" title="History"></a>History</h2><ul>
<li>20180823: created.</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kezunlin.me/blog/opencv-mat-for-loop/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="kezunlin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kezunlin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/opencv-mat-for-loop/" class="post-title-link" itemprop="url">speed up opencv image mat for loop</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-08-23 11:19:00" itemprop="dateCreated datePublished" datetime="2018-08-23T11:19:00+08:00">2018-08-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-10-14 13:39:28" itemprop="dateModified" datetime="2024-10-14T13:39:28+08:00">2024-10-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/cpp/" itemprop="url" rel="index"><span itemprop="name">cpp</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Series"><a href="#Series" class="headerlink" title="Series"></a>Series</h2><ul>
<li><a href="https://kezunlin.me/post/15f5c3e8/">Part 1: compile opencv on ubuntu 16.04</a></li>
<li><a href="https://kezunlin.me/post/6580691f/">Part 2: compile opencv with CUDA support on windows 10</a></li>
<li><strong><a href="https://kezunlin.me/post/61d55ab4/">Part 3: opencv mat for loop</a></strong></li>
<li><a href="https://kezunlin.me/post/7a6ba82e/">Part 4: speed up opencv image processing with openmp</a></li>
</ul>
<h2 id="Guide"><a href="#Guide" class="headerlink" title="Guide"></a>Guide</h2><h3 id="Mat"><a href="#Mat" class="headerlink" title="Mat"></a>Mat</h3><ul>
<li>for gray image,     use type <code>&lt;uchar&gt;</code></li>
<li>for RGB color image，use type <code>&lt;Vec3b&gt;</code></li>
</ul>
<p>gray format storage<br><img src="https://kezunlin.me/images/posts/635233-20180906105458661-1770351097.png" alt="gray"></p>
<p>color format storage: BGR<br><img src="https://kezunlin.me/images/posts/635233-20180906105501938-701356597.png" alt="BGR">  </p>
<blockquote>
<p>we can use method <code>isContinuous()</code> to judge whether the memory buffer is continuous or not.</p>
</blockquote>
<h3 id="color-space-reduction"><a href="#color-space-reduction" class="headerlink" title="color space reduction"></a>color space reduction</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">uchar <span class="title">color_space_reduction</span><span class="params">(uchar pixel)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="comment">/*</span></span><br><span class="line"><span class="comment">	0-9 ===&gt;0</span></span><br><span class="line"><span class="comment">	10-19===&gt;10</span></span><br><span class="line"><span class="comment">	20-29===&gt;20</span></span><br><span class="line"><span class="comment">	...</span></span><br><span class="line"><span class="comment">	240-249===&gt;24</span></span><br><span class="line"><span class="comment">	250-255===&gt;25</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">	map from 256*256*256===&gt;26*26*26</span></span><br><span class="line"><span class="comment">	*/</span></span><br><span class="line"></span><br><span class="line">	<span class="type">int</span> divideWith = <span class="number">10</span>;</span><br><span class="line">	uchar new_pixel = (pixel / divideWith)*divideWith;</span><br><span class="line">	<span class="keyword">return</span> new_pixel;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="color-table"><a href="#color-table" class="headerlink" title="color table"></a>color table</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">get_color_table</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="comment">// cache color value in table[256]</span></span><br><span class="line">	<span class="type">int</span> divideWith = <span class="number">10</span>;</span><br><span class="line">	uchar table[<span class="number">256</span>];</span><br><span class="line">	<span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">256</span>; ++i)</span><br><span class="line">		table[i] = divideWith* (i / divideWith);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- kzl in-article ad -->
<p><ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-5653382914441020"
     data-ad-slot="7925631830"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>




<h2 id="C"><a href="#C" class="headerlink" title="C++"></a>C++</h2><h3 id="ptr"><a href="#ptr" class="headerlink" title="ptr []"></a>ptr []</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// C ptr []: faster but not safe</span></span><br><span class="line"><span class="function">Mat&amp; <span class="title">ScanImageAndReduce_Cptr</span><span class="params">(Mat&amp; I, <span class="type">const</span> uchar* <span class="type">const</span> table)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="comment">// accept only char type matrices</span></span><br><span class="line">	<span class="built_in">CV_Assert</span>(I.<span class="built_in">depth</span>() != <span class="built_in">sizeof</span>(uchar));</span><br><span class="line">	<span class="type">int</span> channels = I.<span class="built_in">channels</span>();</span><br><span class="line">	<span class="type">int</span> nRows = I.rows;</span><br><span class="line">	<span class="type">int</span> nCols = I.cols* channels;</span><br><span class="line">	<span class="keyword">if</span> (I.<span class="built_in">isContinuous</span>())</span><br><span class="line">	&#123;</span><br><span class="line">		nCols *= nRows;</span><br><span class="line">		nRows = <span class="number">1</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="type">int</span> i, j;</span><br><span class="line">	uchar* p;</span><br><span class="line">	<span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; nRows; ++i)</span><br><span class="line">	&#123;</span><br><span class="line">		p = I.<span class="built_in">ptr</span>&lt;uchar&gt;(i);</span><br><span class="line">		<span class="keyword">for</span> (j = <span class="number">0</span>; j &lt; nCols; ++j)</span><br><span class="line">		&#123;</span><br><span class="line">			p[j] = table[p[j]];</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> I;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="ptr-1"><a href="#ptr-1" class="headerlink" title="ptr ++"></a>ptr ++</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// C ptr ++: faster but not safe</span></span><br><span class="line"><span class="function">Mat&amp; <span class="title">ScanImageAndReduce_Cptr2</span><span class="params">(Mat&amp; I, <span class="type">const</span> uchar* <span class="type">const</span> table)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="comment">// accept only char type matrices</span></span><br><span class="line">	<span class="built_in">CV_Assert</span>(I.<span class="built_in">depth</span>() != <span class="built_in">sizeof</span>(uchar));</span><br><span class="line">	<span class="type">int</span> channels = I.<span class="built_in">channels</span>();</span><br><span class="line">	<span class="type">int</span> nRows = I.rows;</span><br><span class="line">	<span class="type">int</span> nCols = I.cols* channels;</span><br><span class="line">	<span class="keyword">if</span> (I.<span class="built_in">isContinuous</span>())</span><br><span class="line">	&#123;</span><br><span class="line">		nCols *= nRows;</span><br><span class="line">		nRows = <span class="number">1</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	uchar* start = I.<span class="built_in">ptr</span>&lt;uchar&gt;(<span class="number">0</span>); <span class="comment">// same as I.ptr&lt;uchar&gt;(0,0)</span></span><br><span class="line">	uchar* end = start + nRows * nCols;</span><br><span class="line">	<span class="keyword">for</span> (uchar* p=start; p &lt; end; ++p)</span><br><span class="line">	&#123;</span><br><span class="line">		*p = table[*p];</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> I;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="at-i-j"><a href="#at-i-j" class="headerlink" title="at(i,j)"></a>at<uchar>(i,j)</h3> <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">// at&lt;uchar&gt;(i,j): random access, slow</span></span><br><span class="line"><span class="function">Mat&amp; <span class="title">ScanImageAndReduce_atRandomAccess</span><span class="params">(Mat&amp; I, <span class="type">const</span> uchar* <span class="type">const</span> table)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="comment">// accept only char type matrices</span></span><br><span class="line">	<span class="built_in">CV_Assert</span>(I.<span class="built_in">depth</span>() != <span class="built_in">sizeof</span>(uchar));</span><br><span class="line">	<span class="type">const</span> <span class="type">int</span> channels = I.<span class="built_in">channels</span>();</span><br><span class="line">	<span class="keyword">switch</span> (channels)</span><br><span class="line">	&#123;</span><br><span class="line">	<span class="keyword">case</span> <span class="number">1</span>:</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; I.rows; ++i)</span><br><span class="line">			<span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; I.cols; ++j)</span><br><span class="line">				I.<span class="built_in">at</span>&lt;uchar&gt;(i, j) = table[I.<span class="built_in">at</span>&lt;uchar&gt;(i, j)];</span><br><span class="line">		<span class="keyword">break</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">case</span> <span class="number">3</span>:</span><br><span class="line">	&#123;</span><br><span class="line">		Mat_&lt;Vec3b&gt; _I = I;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; I.rows; ++i)</span><br><span class="line">			<span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; I.cols; ++j)</span><br><span class="line">			&#123;</span><br><span class="line">				_I(i, j)[<span class="number">0</span>] = table[_I(i, j)[<span class="number">0</span>]];</span><br><span class="line">				_I(i, j)[<span class="number">1</span>] = table[_I(i, j)[<span class="number">1</span>]];</span><br><span class="line">				_I(i, j)[<span class="number">2</span>] = table[_I(i, j)[<span class="number">2</span>]];</span><br><span class="line">			&#125;</span><br><span class="line">		I = _I;</span><br><span class="line">		<span class="keyword">break</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> I;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="Iterator"><a href="#Iterator" class="headerlink" title="Iterator"></a>Iterator</h3> <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">// MatIterator_&lt;uchar&gt;: safe but slow</span></span><br><span class="line"><span class="function">Mat&amp; <span class="title">ScanImageAndReduce_Iterator</span><span class="params">(Mat&amp; I, <span class="type">const</span> uchar* <span class="type">const</span> table)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="comment">// accept only char type matrices</span></span><br><span class="line">	<span class="built_in">CV_Assert</span>(I.<span class="built_in">depth</span>() != <span class="built_in">sizeof</span>(uchar));</span><br><span class="line">	<span class="type">const</span> <span class="type">int</span> channels = I.<span class="built_in">channels</span>();</span><br><span class="line">	<span class="keyword">switch</span> (channels)</span><br><span class="line">	&#123;</span><br><span class="line">	<span class="keyword">case</span> <span class="number">1</span>:</span><br><span class="line">	&#123;</span><br><span class="line">		MatIterator_&lt;uchar&gt; it, end;</span><br><span class="line">		<span class="keyword">for</span> (it = I.<span class="built_in">begin</span>&lt;uchar&gt;(), end = I.<span class="built_in">end</span>&lt;uchar&gt;(); it != end; ++it)</span><br><span class="line">			*it = table[*it];</span><br><span class="line">		<span class="keyword">break</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">case</span> <span class="number">3</span>:</span><br><span class="line">	&#123;</span><br><span class="line">		MatIterator_&lt;Vec3b&gt; it, end;</span><br><span class="line">		<span class="keyword">for</span> (it = I.<span class="built_in">begin</span>&lt;Vec3b&gt;(), end = I.<span class="built_in">end</span>&lt;Vec3b&gt;(); it != end; ++it)</span><br><span class="line">		&#123;</span><br><span class="line">			(*it)[<span class="number">0</span>] = table[(*it)[<span class="number">0</span>]];</span><br><span class="line">			(*it)[<span class="number">1</span>] = table[(*it)[<span class="number">1</span>]];</span><br><span class="line">			(*it)[<span class="number">2</span>] = table[(*it)[<span class="number">2</span>]];</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> I;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="opencv-LUT"><a href="#opencv-LUT" class="headerlink" title="opencv LUT"></a>opencv LUT</h3> <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">// LUT</span></span><br><span class="line"><span class="function">Mat&amp; <span class="title">ScanImageAndReduce_LUT</span><span class="params">(Mat&amp; I, <span class="type">const</span> uchar* <span class="type">const</span> table)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="function">Mat <span class="title">lookUpTable</span><span class="params">(<span class="number">1</span>, <span class="number">256</span>, CV_8U)</span></span>;</span><br><span class="line">	uchar* p = lookUpTable.data;</span><br><span class="line">	<span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">256</span>; ++i)</span><br><span class="line">		p[i] = table[i];</span><br><span class="line"></span><br><span class="line">	cv::<span class="built_in">LUT</span>(I, lookUpTable, I);</span><br><span class="line">	<span class="keyword">return</span> I;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
 <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- kzl in-article ad -->
<p><ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-5653382914441020"
     data-ad-slot="7925631830"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>



<h3 id="forEach"><a href="#forEach" class="headerlink" title="forEach"></a>forEach</h3><blockquote>
<p><code>forEach</code> method of the <code>Mat</code> class that utilizes all the cores on your machine to apply any function at every pixel.</p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Parallel execution with function object.</span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">ForEachOperator</span></span><br><span class="line">&#123;</span><br><span class="line">	uchar m_table[<span class="number">256</span>];</span><br><span class="line">	<span class="built_in">ForEachOperator</span>(<span class="type">const</span> uchar* <span class="type">const</span> table)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">for</span> (<span class="type">size_t</span> i = <span class="number">0</span>; i &lt; <span class="number">256</span>; i++)</span><br><span class="line">		&#123;</span><br><span class="line">			m_table[i] = table[i];</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="type">void</span> <span class="title">operator</span> <span class="params">()</span><span class="params">(uchar&amp; p, <span class="type">const</span> <span class="type">int</span> * position)</span> <span class="type">const</span></span></span><br><span class="line"><span class="function">	</span>&#123;</span><br><span class="line">		<span class="comment">// Perform a simple operation</span></span><br><span class="line">		p = m_table[p];</span><br><span class="line">	&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// forEach use multiple processors, very fast</span></span><br><span class="line"><span class="function">Mat&amp; <span class="title">ScanImageAndReduce_forEach</span><span class="params">(Mat&amp; I, <span class="type">const</span> uchar* <span class="type">const</span> table)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	I.forEach&lt;uchar&gt;(<span class="built_in">ForEachOperator</span>(table));</span><br><span class="line">	<span class="keyword">return</span> I;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="forEach-with-lambda"><a href="#forEach-with-lambda" class="headerlink" title="forEach with lambda"></a>forEach with lambda</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// forEach lambda use multiple processors, very fast (lambda slower than ForEachOperator)</span></span><br><span class="line"><span class="function">Mat&amp; <span class="title">ScanImageAndReduce_forEach_with_lambda</span><span class="params">(Mat&amp; I, <span class="type">const</span> uchar* <span class="type">const</span> table)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	I.forEach&lt;uchar&gt;</span><br><span class="line">	(</span><br><span class="line">		[=](uchar &amp;p, <span class="type">const</span> <span class="type">int</span> * position) -&gt; <span class="type">void</span></span><br><span class="line">		&#123;</span><br><span class="line">			p = table[p];</span><br><span class="line">		&#125;</span><br><span class="line">	);</span><br><span class="line">	<span class="keyword">return</span> I;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="time-cost"><a href="#time-cost" class="headerlink" title="time cost"></a>time cost</h3><h4 id="no-foreach"><a href="#no-foreach" class="headerlink" title="no foreach"></a>no foreach</h4><pre><code>[1 Cptr   ] times=5000, total_cost=988 ms, avg_cost=0.1976 ms
[1 Cptr2  ] times=5000, total_cost=1704 ms, avg_cost=0.3408 ms
[2 atRandom] times=5000, total_cost=9611 ms, avg_cost=1.9222 ms
[3 Iterator] times=5000, total_cost=20195 ms, avg_cost=4.039 ms
[4 LUT    ] times=5000, total_cost=899 ms, avg_cost=0.1798 ms

[1 Cptr   ] times=10000, total_cost=2425 ms, avg_cost=0.2425 ms
[1 Cptr2  ] times=10000, total_cost=3391 ms, avg_cost=0.3391 ms
[2 atRandom] times=10000, total_cost=20024 ms, avg_cost=2.0024 ms
[3 Iterator] times=10000, total_cost=39980 ms, avg_cost=3.998 ms
[4 LUT    ] times=10000, total_cost=103 ms, avg_cost=0.0103 ms
</code></pre>
<h4 id="foreach"><a href="#foreach" class="headerlink" title="foreach"></a>foreach</h4><pre><code>[5 forEach     ] times=200000, total_cost=199 ms, avg_cost=0.000995 ms
[5 forEach lambda] times=200000, total_cost=521 ms, avg_cost=0.002605 ms

[5 forEach     ] times=20000, total_cost=17 ms, avg_cost=0.00085 ms
[5 forEach lambda] times=20000, total_cost=23 ms, avg_cost=0.00115 ms
</code></pre>
<h4 id="results"><a href="#results" class="headerlink" title="results"></a>results</h4><p>Loop Type | Time Cost (us)<br>:—-:   |<br>ptr []   | 242<br>ptr ++   | 339<br>at<uchar> | 2002<br>iterator  | 3998<br>LUT     | 10<br>forEach   | 0.85<br>forEach lambda  | 1.15 </p>
<p><code>forEach</code> is 10x times faster than <code>LUT</code>, 240<del>340x times faster than <code>ptr []</code> and <code>ptr ++</code>, and 2000</del>4000x times faster than <code>at</code> and <code>iterator</code>.</p>
<h3 id="code"><a href="#code" class="headerlink" title="code"></a>code</h3><p><a target="_blank" rel="noopener" href="https://gist.github.com/kezunlin/8a8f1be7c0e101ce3f0e16e529288afc">code here</a></p>
<h2 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h2><h3 id="pure-python"><a href="#pure-python" class="headerlink" title="pure python"></a>pure python</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># import the necessary packages</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="built_in">print</span>(cv2.__version__)</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>

<pre><code>3.4.2
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load the original image, convert it to grayscale, and display</span></span><br><span class="line"><span class="comment"># it inline</span></span><br><span class="line">image = cv2.imread(<span class="string">&quot;cat.jpg&quot;</span>)</span><br><span class="line">image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)</span><br><span class="line"><span class="built_in">print</span>(image.shape)</span><br><span class="line"><span class="comment">#plt.imshow(image, cmap=&quot;gray&quot;)</span></span><br></pre></td></tr></table></figure>

<pre><code>(360, 480)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%load_ext cython</span><br></pre></td></tr></table></figure>

<pre><code>The cython extension is already loaded. To reload it, use:
  %reload_ext cython
</code></pre>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">%%cython -a</span><br><span class="line"> </span><br><span class="line">def threshold_python(T, image):</span><br><span class="line">    # grab the image dimensions</span><br><span class="line">    h = image.shape[0]</span><br><span class="line">    w = image.shape[1]</span><br><span class="line">    </span><br><span class="line">    # loop over the image, pixel by pixel</span><br><span class="line">    for y in range(0, h):</span><br><span class="line">        for x in range(0, w):</span><br><span class="line">            # threshold the pixel</span><br><span class="line">            image[y, x] = 255 if image[y, x] &gt;= T else 0</span><br><span class="line">            </span><br><span class="line">    # return the thresholded image</span><br><span class="line">    return image</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%timeit threshold_python(<span class="number">5</span>, image)</span><br></pre></td></tr></table></figure>

<pre><code>263 ms ± 20.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
</code></pre>
<h3 id="cython"><a href="#cython" class="headerlink" title="cython"></a>cython</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">%%cython -a</span><br><span class="line"> </span><br><span class="line">import cython</span><br><span class="line"> </span><br><span class="line">@cython.boundscheck(False)</span><br><span class="line">cpdef unsigned char[:, :] threshold_cython(int T, unsigned char [:, :] image):</span><br><span class="line">    # set the variable extension types</span><br><span class="line">    cdef int x, y, w, h</span><br><span class="line">    </span><br><span class="line">    # grab the image dimensions</span><br><span class="line">    h = image.shape[0]</span><br><span class="line">    w = image.shape[1]</span><br><span class="line">    </span><br><span class="line">    # loop over the image</span><br><span class="line">    for y in range(0, h):</span><br><span class="line">        for x in range(0, w):</span><br><span class="line">            # threshold the pixel</span><br><span class="line">            image[y, x] = 255 if image[y, x] &gt;= T else 0</span><br><span class="line">    </span><br><span class="line">    # return the thresholded image</span><br><span class="line">    return image</span><br></pre></td></tr></table></figure>

<h3 id="numba"><a href="#numba" class="headerlink" title="numba"></a>numba</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%timeit threshold_cython(<span class="number">5</span>, image)</span><br></pre></td></tr></table></figure>

<pre><code>150 µs ± 7.14 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numba <span class="keyword">import</span> njit</span><br><span class="line"></span><br><span class="line"><span class="meta">@njit</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">threshold_njit</span>(<span class="params">T, image</span>):</span><br><span class="line">    <span class="comment"># grab the image dimensions</span></span><br><span class="line">    h = image.shape[<span class="number">0</span>]</span><br><span class="line">    w = image.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># loop over the image, pixel by pixel</span></span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, h):</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, w):</span><br><span class="line">            <span class="comment"># threshold the pixel</span></span><br><span class="line">            image[y, x] = <span class="number">255</span> <span class="keyword">if</span> image[y, x] &gt;= T <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">            </span><br><span class="line">    <span class="comment"># return the thresholded image</span></span><br><span class="line">    <span class="keyword">return</span> image</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%timeit threshold_njit(<span class="number">5</span>, image)</span><br></pre></td></tr></table></figure>

<pre><code>43.5 µs ± 142 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
</code></pre>
<h3 id="numpy"><a href="#numpy" class="headerlink" title="numpy"></a>numpy</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">threshold_numpy</span>(<span class="params">T, image</span>):</span><br><span class="line">    image[image &gt; T] = <span class="number">255</span></span><br><span class="line">    <span class="keyword">return</span> image</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%timeit threshold_numpy(<span class="number">5</span>, image)</span><br></pre></td></tr></table></figure>

<pre><code>111 µs ± 334 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
</code></pre>
<h3 id="conclusions"><a href="#conclusions" class="headerlink" title="conclusions"></a>conclusions</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">image = cv2.imread(<span class="string">&quot;cat.jpg&quot;</span>)</span><br><span class="line">image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)</span><br><span class="line"><span class="built_in">print</span>(image.shape)</span><br><span class="line"></span><br><span class="line">%timeit threshold_python(<span class="number">5</span>, image)</span><br><span class="line">%timeit threshold_cython(<span class="number">5</span>, image)</span><br><span class="line">%timeit threshold_njit(<span class="number">5</span>, image)</span><br><span class="line">%timeit threshold_numpy(<span class="number">5</span>, image)</span><br></pre></td></tr></table></figure>

<pre><code>(360, 480)
251 ms ± 6.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
143 µs ± 1.19 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
43.8 µs ± 284 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
113 µs ± 957 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">image = cv2.imread(<span class="string">&quot;big.jpg&quot;</span>)</span><br><span class="line">image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)</span><br><span class="line"><span class="built_in">print</span>(image.shape)</span><br><span class="line"></span><br><span class="line">%timeit threshold_python(<span class="number">5</span>, image)</span><br><span class="line">%timeit threshold_cython(<span class="number">5</span>, image)</span><br><span class="line">%timeit threshold_njit(<span class="number">5</span>, image)</span><br><span class="line">%timeit threshold_numpy(<span class="number">5</span>, image)</span><br></pre></td></tr></table></figure>

<pre><code>(2880, 5120)
21.8 s ± 460 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
12.3 ms ± 231 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
3.91 ms ± 66.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
10.3 ms ± 179 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
</code></pre>
<p>60,480</p>
<ul>
<li>python: 251 ms</li>
<li>cython: 143 us</li>
<li>numba: 43 us</li>
<li>numpy: 113 us</li>
</ul>
<p>2880, 5120</p>
<ul>
<li>python: 21 s</li>
<li>cython: 12 ms</li>
<li>numba: 4 ms</li>
<li>numpy: 10 ms</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a target="_blank" rel="noopener" href="https://www.learnopencv.com/parallel-pixel-access-in-opencv-using-foreach/">parallel-pixel-access-in-opencv-using-foreach</a></li>
<li><a target="_blank" rel="noopener" href="https://www.pyimagesearch.com/2017/08/28/fast-optimized-for-pixel-loops-with-opencv-and-python/">fast-optimized-for-pixel-loops-with-opencv-and-python</a></li>
<li><a target="_blank" rel="noopener" href="https://wiki.python.org/moin/PythonSpeed/PerformanceTips">python performance tips</a></li>
</ul>
<h2 id="History"><a href="#History" class="headerlink" title="History"></a>History</h2><ul>
<li>20180823: created.</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/10/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><span class="page-number current">11</span><a class="page-number" href="/page/12/">12</a><span class="space">&hellip;</span><a class="page-number" href="/page/19/">19</a><a class="extend next" rel="next" href="/page/12/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">kezunlin</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">181</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">213</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">kezunlin</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
