<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"kezunlin.me","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Kezunlin&#39;s Blog">
<meta property="og:url" content="https://kezunlin.me/page/13/index.html">
<meta property="og:site_name" content="Kezunlin&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="kezunlin">
<meta property="article:tag" content="linux, c++, python, AI, LLM">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://kezunlin.me/page/13/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Kezunlin's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Kezunlin's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Kezunlin's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Live and Learn</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kezunlin.me/blog/Classification-Instant-Recognition-with-Caffe/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="kezunlin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kezunlin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/Classification-Instant-Recognition-with-Caffe/" class="post-title-link" itemprop="url">Classification: Instant Recognition with Caffe</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-08-07 17:33:00" itemprop="dateCreated datePublished" datetime="2018-08-07T17:33:00+08:00">2018-08-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-10-14 13:39:28" itemprop="dateModified" datetime="2024-10-14T13:39:28+08:00">2024-10-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Tutorial"><a href="#Tutorial" class="headerlink" title="Tutorial"></a>Tutorial</h2><p>In this example we’ll classify an image with the bundled CaffeNet model (which is based on the network architecture of Krizhevsky et al. for ImageNet).</p>
<p>We’ll compare CPU and GPU modes and then dig into the model to inspect features and the output.</p>
<h3 id="Setup"><a href="#Setup" class="headerlink" title="Setup"></a>Setup</h3><ul>
<li>First, set up Python, <code>numpy</code>, and <code>matplotlib</code>.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># set up Python environment: numpy for numerical routines, and matplotlib for plotting</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># display plots in this notebook</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment"># set display defaults</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] = (<span class="number">10</span>, <span class="number">10</span>)        <span class="comment"># large images</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;image.interpolation&#x27;</span>] = <span class="string">&#x27;nearest&#x27;</span>  <span class="comment"># don&#x27;t interpolate: show square pixels</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;image.cmap&#x27;</span>] = <span class="string">&#x27;gray&#x27;</span>  <span class="comment"># use grayscale output rather than a (potentially misleading) color heatmap</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># The caffe module needs to be on the Python path;</span></span><br><span class="line"><span class="comment">#  we&#x27;ll add it here explicitly.</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">caffe_root = <span class="string">&#x27;../&#x27;</span>  <span class="comment"># this file should be run from &#123;caffe_root&#125;/examples (otherwise change this line)</span></span><br><span class="line">sys.path.insert(<span class="number">0</span>, caffe_root + <span class="string">&#x27;python&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> caffe</span><br><span class="line"><span class="comment"># If you get &quot;No module named _caffe&quot;, either you have not built pycaffe or you have the wrong path.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">if</span> os.path.isfile(caffe_root + <span class="string">&#x27;models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel&#x27;</span>):</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&#x27;CaffeNet found.&#x27;</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&#x27;Downloading pre-trained CaffeNet model...&#x27;</span></span><br><span class="line">    !../scripts/download_model_binary.py ../models/bvlc_reference_caffenet</span><br></pre></td></tr></table></figure>

<pre><code>CaffeNet found.
</code></pre>
<h3 id="Load-net-and-set-up-input-preprocessing"><a href="#Load-net-and-set-up-input-preprocessing" class="headerlink" title="Load net and set up input preprocessing"></a>Load net and set up input preprocessing</h3><ul>
<li>Set Caffe to CPU mode and load the net from disk.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">caffe.set_mode_cpu()</span><br><span class="line"></span><br><span class="line">model_def = caffe_root + <span class="string">&#x27;models/bvlc_reference_caffenet/deploy.prototxt&#x27;</span></span><br><span class="line">model_weights = caffe_root + <span class="string">&#x27;models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel&#x27;</span></span><br><span class="line"></span><br><span class="line">net = caffe.Net(model_def,      <span class="comment"># defines the structure of the model</span></span><br><span class="line">                model_weights,  <span class="comment"># contains the trained weights</span></span><br><span class="line">                caffe.TEST)     <span class="comment"># use test mode (e.g., don&#x27;t perform dropout)</span></span><br></pre></td></tr></table></figure>

<ul>
<li><p>Set up input preprocessing. (We’ll use Caffe’s <code>caffe.io.Transformer</code> to do this, but this step is independent of other parts of Caffe, so any custom preprocessing code may be used).</p>
<p>  Our default CaffeNet is configured to take images in BGR format. Values are expected to start in the range [0, 255] and then have the mean ImageNet pixel value subtracted from them. In addition, the channel dimension is expected as the first (<em>outermost</em>) dimension.</p>
<p>  As matplotlib will load images with values in the range [0, 1] in RGB format with the channel as the <em>innermost</em> dimension, we are arranging for the needed transformations here.</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load the mean ImageNet image (as distributed with Caffe) for subtraction</span></span><br><span class="line">mu = np.load(caffe_root + <span class="string">&#x27;python/caffe/imagenet/ilsvrc_2012_mean.npy&#x27;</span>)</span><br><span class="line">mu = mu.mean(<span class="number">1</span>).mean(<span class="number">1</span>)  <span class="comment"># average over pixels to obtain the mean (BGR) pixel values</span></span><br><span class="line"><span class="comment">#mu = np.array([ 104.00698793,  116.66876762,  122.67891434]) </span></span><br><span class="line"><span class="built_in">print</span> <span class="string">&#x27;mean-subtracted values:&#x27;</span>, <span class="built_in">zip</span>(<span class="string">&#x27;BGR&#x27;</span>, mu)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create transformer for the input called &#x27;data&#x27;</span></span><br><span class="line">transformer = caffe.io.Transformer(&#123;<span class="string">&#x27;data&#x27;</span>: net.blobs[<span class="string">&#x27;data&#x27;</span>].data.shape&#125;)</span><br><span class="line"></span><br><span class="line">transformer.set_transpose(<span class="string">&#x27;data&#x27;</span>, (<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>))  <span class="comment"># move image channels to outermost dimension</span></span><br><span class="line">transformer.set_mean(<span class="string">&#x27;data&#x27;</span>, mu)            <span class="comment"># subtract the dataset-mean value in each channel</span></span><br><span class="line">transformer.set_raw_scale(<span class="string">&#x27;data&#x27;</span>, <span class="number">255</span>)      <span class="comment"># rescale from [0, 1] to [0, 255]</span></span><br><span class="line">transformer.set_channel_swap(<span class="string">&#x27;data&#x27;</span>, (<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>))  <span class="comment"># swap channels from RGB to BGR</span></span><br></pre></td></tr></table></figure>

<pre><code>mean-subtracted values: [(&#39;B&#39;, 104.0069879317889), (&#39;G&#39;, 116.66876761696767), (&#39;R&#39;, 122.6789143406786)]
</code></pre>
<h3 id="CPU-classification"><a href="#CPU-classification" class="headerlink" title="CPU classification"></a>CPU classification</h3><ul>
<li>Now we’re ready to perform classification. Even though we’ll only classify one image, we’ll set a batch size of 50 to demonstrate batching.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># set the size of the input (we can skip this if we&#x27;re happy</span></span><br><span class="line"><span class="comment">#  with the default; we can also change it later, e.g., for different batch sizes)</span></span><br><span class="line">net.blobs[<span class="string">&#x27;data&#x27;</span>].reshape(<span class="number">50</span>,        <span class="comment"># batch size</span></span><br><span class="line">                          <span class="number">3</span>,         <span class="comment"># 3-channel (BGR) images</span></span><br><span class="line">                          <span class="number">227</span>, <span class="number">227</span>)  <span class="comment"># image size is 227x227</span></span><br></pre></td></tr></table></figure>

<ul>
<li>Load an image (that comes with Caffe) and perform the preprocessing we’ve set up.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">image = caffe.io.load_image(caffe_root + <span class="string">&#x27;examples/images/cat.jpg&#x27;</span>)</span><br><span class="line">transformed_image = transformer.preprocess(<span class="string">&#x27;data&#x27;</span>, image)</span><br><span class="line">plt.imshow(image)</span><br></pre></td></tr></table></figure>

<pre><code>/usr/local/lib/python2.7/dist-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, &#39;constant&#39;, will be changed to &#39;reflect&#39; in skimage 0.15.
  warn(&quot;The default mode, &#39;constant&#39;, will be changed to &#39;reflect&#39; in &quot;





&lt;matplotlib.image.AxesImage at 0x7f2088044450&gt;
</code></pre>
<p><img src="https://kezunlin.me/images/posts/635233-20180807173641145-432603467.png" alt="png"></p>
<ul>
<li>Adorable! Let’s classify it!</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># copy the image data into the memory allocated for the net</span></span><br><span class="line">net.blobs[<span class="string">&#x27;data&#x27;</span>].data[...] = transformed_image</span><br><span class="line"></span><br><span class="line"><span class="comment">### perform classification</span></span><br><span class="line">output = net.forward()</span><br><span class="line"></span><br><span class="line">output_prob = output[<span class="string">&#x27;prob&#x27;</span>][<span class="number">0</span>]  <span class="comment"># the output probability vector for the first image in the batch</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> output_prob.shape</span><br><span class="line"><span class="built_in">print</span> <span class="string">&#x27;predicted class is:&#x27;</span>, output_prob.argmax()</span><br></pre></td></tr></table></figure>

<pre><code>(1000,)
predicted class is: 281
</code></pre>
<ul>
<li>The net gives us a vector of probabilities; the most probable class was the 281st one. But is that correct? Let’s check the ImageNet labels…</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load ImageNet labels</span></span><br><span class="line">labels_file = caffe_root + <span class="string">&#x27;data/ilsvrc12/synset_words.txt&#x27;</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(labels_file):</span><br><span class="line">    !../data/ilsvrc12/get_ilsvrc_aux.sh</span><br><span class="line">    </span><br><span class="line">labels = np.loadtxt(labels_file, <span class="built_in">str</span>, delimiter=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> labels.shape</span><br><span class="line"><span class="built_in">print</span> <span class="string">&#x27;output label:&#x27;</span>, labels[output_prob.argmax()]</span><br></pre></td></tr></table></figure>

<pre><code>(1000,)
output label: n02123045 tabby, tabby cat
</code></pre>
<ul>
<li>“Tabby cat” is correct! But let’s also look at other top (but less confident predictions).</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sort top five predictions from softmax output</span></span><br><span class="line">top_inds = output_prob.argsort()[::-<span class="number">1</span>][:<span class="number">5</span>]  <span class="comment"># reverse sort and take five largest items</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> <span class="string">&#x27;probabilities and labels:&#x27;</span></span><br><span class="line"><span class="built_in">zip</span>(output_prob[top_inds], labels[top_inds])</span><br></pre></td></tr></table></figure>

<pre><code>probabilities and labels:





[(0.31243625, &#39;n02123045 tabby, tabby cat&#39;),
 (0.23797157, &#39;n02123159 tiger cat&#39;),
 (0.12387245, &#39;n02124075 Egyptian cat&#39;),
 (0.10075716, &#39;n02119022 red fox, Vulpes vulpes&#39;),
 (0.070957333, &#39;n02127052 lynx, catamount&#39;)]
</code></pre>
<ul>
<li>We see that less confident predictions are sensible.</li>
</ul>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- kzl in-article ad -->
<p><ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-5653382914441020"
     data-ad-slot="7925631830"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>




<h3 id="Switching-to-GPU-mode"><a href="#Switching-to-GPU-mode" class="headerlink" title="Switching to GPU mode"></a>Switching to GPU mode</h3><ul>
<li>Let’s see how long classification took, and compare it to GPU mode.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%timeit net.forward()</span><br></pre></td></tr></table></figure>

<pre><code>1 loop, best of 3: 4.26 s per loop
</code></pre>
<ul>
<li>That’s a while, even for a batch of 50 images. Let’s switch to GPU mode.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">caffe.set_device(<span class="number">0</span>)  <span class="comment"># if we have multiple GPUs, pick the first one</span></span><br><span class="line">caffe.set_mode_gpu()</span><br><span class="line">net.forward()  <span class="comment"># run once before timing to set up memory</span></span><br><span class="line">%timeit net.forward()</span><br></pre></td></tr></table></figure>

<pre><code>10 loops, best of 3: 29.6 ms per loop
</code></pre>
<ul>
<li>That should be much faster!</li>
</ul>
<h3 id="Examining-intermediate-output"><a href="#Examining-intermediate-output" class="headerlink" title="Examining intermediate output"></a>Examining intermediate output</h3><ul>
<li>A net is not just a black box; let’s take a look at some of the parameters and intermediate activations.</li>
</ul>
<p>First we’ll see how to read out the structure of the net in terms of activation and parameter shapes.</p>
<ul>
<li><p>For each layer, let’s look at the activation shapes, which typically have the form <code>(batch_size, channel_dim, height, width)</code>.</p>
<p>  The activations are exposed as an <code>OrderedDict</code>, <code>net.blobs</code>.</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># for each layer, show the output shape</span></span><br><span class="line"><span class="keyword">for</span> layer_name, blob <span class="keyword">in</span> net.blobs.iteritems():</span><br><span class="line">    <span class="built_in">print</span> layer_name + <span class="string">&#x27;\t&#x27;</span> + <span class="built_in">str</span>(blob.data.shape)</span><br></pre></td></tr></table></figure>

<pre><code>data	(50, 3, 227, 227)
conv1	(50, 96, 55, 55)
pool1	(50, 96, 27, 27)
norm1	(50, 96, 27, 27)
conv2	(50, 256, 27, 27)
pool2	(50, 256, 13, 13)
norm2	(50, 256, 13, 13)
conv3	(50, 384, 13, 13)
conv4	(50, 384, 13, 13)
conv5	(50, 256, 13, 13)
pool5	(50, 256, 6, 6)
fc6	(50, 4096)
fc7	(50, 4096)
fc8	(50, 1000)
prob	(50, 1000)
</code></pre>
<ul>
<li><p>Now look at the parameter shapes. The parameters are exposed as another <code>OrderedDict</code>, <code>net.params</code>. We need to index the resulting values with either <code>[0]</code> for weights or <code>[1]</code> for biases.</p>
<p>  The param shapes typically have the form <code>(output_channels, input_channels, filter_height, filter_width)</code> (for the weights) and the 1-dimensional shape <code>(output_channels,)</code> (for the biases).</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> layer_name, param <span class="keyword">in</span> net.params.iteritems():</span><br><span class="line">    <span class="built_in">print</span> layer_name + <span class="string">&#x27;\t&#x27;</span> + <span class="built_in">str</span>(param[<span class="number">0</span>].data.shape), <span class="built_in">str</span>(param[<span class="number">1</span>].data.shape)</span><br></pre></td></tr></table></figure>

<pre><code>conv1	(96, 3, 11, 11) (96,)
conv2	(256, 48, 5, 5) (256,)
conv3	(384, 256, 3, 3) (384,)
conv4	(384, 192, 3, 3) (384,)
conv5	(256, 192, 3, 3) (256,)
fc6	(4096, 9216) (4096,)
fc7	(4096, 4096) (4096,)
fc8	(1000, 4096) (1000,)
</code></pre>
<ul>
<li>Since we’re dealing with four-dimensional data here, we’ll define a helper function for visualizing sets of rectangular heatmaps.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">vis_square</span>(<span class="params">data</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Take an array of shape (n, height, width) or (n, height, width, 3)</span></span><br><span class="line"><span class="string">       and visualize each (height, width) thing in a grid of size approx. sqrt(n) by sqrt(n)&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># normalize data for display</span></span><br><span class="line">    data = (data - data.<span class="built_in">min</span>()) / (data.<span class="built_in">max</span>() - data.<span class="built_in">min</span>())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># force the number of filters to be square</span></span><br><span class="line">    n = <span class="built_in">int</span>(np.ceil(np.sqrt(data.shape[<span class="number">0</span>])))</span><br><span class="line">    padding = (((<span class="number">0</span>, n ** <span class="number">2</span> - data.shape[<span class="number">0</span>]),</span><br><span class="line">               (<span class="number">0</span>, <span class="number">1</span>), (<span class="number">0</span>, <span class="number">1</span>))                 <span class="comment"># add some space between filters</span></span><br><span class="line">               + ((<span class="number">0</span>, <span class="number">0</span>),) * (data.ndim - <span class="number">3</span>))  <span class="comment"># don&#x27;t pad the last dimension (if there is one)</span></span><br><span class="line">    data = np.pad(data, padding, mode=<span class="string">&#x27;constant&#x27;</span>, constant_values=<span class="number">1</span>)  <span class="comment"># pad with ones (white)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># tile the filters into an image</span></span><br><span class="line">    data = data.reshape((n, n) + data.shape[<span class="number">1</span>:]).transpose((<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>) + <span class="built_in">tuple</span>(<span class="built_in">range</span>(<span class="number">4</span>, data.ndim + <span class="number">1</span>)))</span><br><span class="line">    data = data.reshape((n * data.shape[<span class="number">1</span>], n * data.shape[<span class="number">3</span>]) + data.shape[<span class="number">4</span>:])</span><br><span class="line">    </span><br><span class="line">    plt.imshow(data); plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>First we’ll look at the first layer filters, <code>conv1</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># the parameters are a list of [weights, biases]</span></span><br><span class="line">filters = net.params[<span class="string">&#x27;conv1&#x27;</span>][<span class="number">0</span>].data</span><br><span class="line">vis_square(filters.transpose(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>


<p><img src="https://kezunlin.me/images/posts/635233-20180807173745487-782490699.png" alt="png"></p>
<ul>
<li>The first layer output, <code>conv1</code> (rectified responses of the filters above, first 36 only)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">feat = net.blobs[<span class="string">&#x27;conv1&#x27;</span>].data[<span class="number">0</span>, :<span class="number">36</span>]</span><br><span class="line">vis_square(feat)</span><br></pre></td></tr></table></figure>


<p><img src="https://kezunlin.me/images/posts/635233-20180807173803768-454494158.png" alt="png"></p>
<ul>
<li>The fifth layer after pooling, <code>pool5</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">feat = net.blobs[<span class="string">&#x27;pool5&#x27;</span>].data[<span class="number">0</span>]</span><br><span class="line">vis_square(feat)</span><br></pre></td></tr></table></figure>


<p><img src="https://kezunlin.me/images/posts/635233-20180807173817099-163236065.png" alt="png"></p>
<ul>
<li><p>The first fully connected layer, <code>fc6</code> (rectified)</p>
<p>  We show the output values and the histogram of the positive values</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">feat = net.blobs[<span class="string">&#x27;fc6&#x27;</span>].data[<span class="number">0</span>]</span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">plt.plot(feat.flat)</span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">_ = plt.hist(feat.flat[feat.flat &gt; <span class="number">0</span>], bins=<span class="number">100</span>)</span><br></pre></td></tr></table></figure>


<p><img src="https://kezunlin.me/images/posts/635233-20180807173830706-986799321.png" alt="png"></p>
<ul>
<li>The final probability output, <code>prob</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">feat = net.blobs[<span class="string">&#x27;prob&#x27;</span>].data[<span class="number">0</span>]</span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">3</span>))</span><br><span class="line">plt.plot(feat.flat)</span><br></pre></td></tr></table></figure>




<pre><code>[&lt;matplotlib.lines.Line2D at 0x7f2060250650&gt;]
</code></pre>
<p><img src="https://kezunlin.me/images/posts/635233-20180807173851980-73956897.png" alt="png"></p>
<p>Note the cluster of strong predictions; the labels are sorted semantically. The top peaks correspond to the top predicted labels, as shown above.</p>
<h3 id="Try-your-own-image"><a href="#Try-your-own-image" class="headerlink" title="Try your own image"></a>Try your own image</h3><p>Now we’ll grab an image from the web and classify it using the steps above.</p>
<ul>
<li>Try setting <code>my_image_url</code> to any JPEG image URL.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># download an image</span></span><br><span class="line">my_image_url = <span class="string">&quot;...&quot;</span>  <span class="comment"># paste your URL here</span></span><br><span class="line"><span class="comment"># for example:</span></span><br><span class="line"><span class="comment"># my_image_url = &quot;https://upload.wikimedia.org/wikipedia/commons/b/be/Orang_Utan%2C_Semenggok_Forest_Reserve%2C_Sarawak%2C_Borneo%2C_Malaysia.JPG&quot;</span></span><br><span class="line">!wget -O image.jpg $my_image_url</span><br><span class="line"></span><br><span class="line"><span class="comment"># transform it and copy it into the net</span></span><br><span class="line">image = caffe.io.load_image(<span class="string">&#x27;image.jpg&#x27;</span>)</span><br><span class="line">net.blobs[<span class="string">&#x27;data&#x27;</span>].data[...] = transformer.preprocess(<span class="string">&#x27;data&#x27;</span>, image)</span><br><span class="line"></span><br><span class="line"><span class="comment"># perform classification</span></span><br><span class="line">net.forward()</span><br><span class="line"></span><br><span class="line"><span class="comment"># obtain the output probabilities</span></span><br><span class="line">output_prob = net.blobs[<span class="string">&#x27;prob&#x27;</span>].data[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># sort top five predictions from softmax output</span></span><br><span class="line">top_inds = output_prob.argsort()[::-<span class="number">1</span>][:<span class="number">5</span>]</span><br><span class="line"></span><br><span class="line">plt.imshow(image)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> <span class="string">&#x27;probabilities and labels:&#x27;</span></span><br><span class="line"><span class="built_in">zip</span>(output_prob[top_inds], labels[top_inds])</span><br></pre></td></tr></table></figure>

<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a target="_blank" rel="noopener" href="http://demo.vislab.berkeleyvision.org/">demo</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/BVLC/caffe">caffe git</a></li>
</ul>
<h2 id="History"><a href="#History" class="headerlink" title="History"></a>History</h2><ul>
<li>20180807: created.</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kezunlin.me/blog/Tutorial-for-Training-LeNet-on-MNIST-with-Caffe/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="kezunlin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kezunlin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/Tutorial-for-Training-LeNet-on-MNIST-with-Caffe/" class="post-title-link" itemprop="url">Tutorial for Training LeNet on MNIST with Caffe</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-08-07 15:56:00" itemprop="dateCreated datePublished" datetime="2018-08-07T15:56:00+08:00">2018-08-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-10-14 13:39:28" itemprop="dateModified" datetime="2024-10-14T13:39:28+08:00">2024-10-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="LeNet"><a href="#LeNet" class="headerlink" title="LeNet"></a>LeNet</h2><p>The design of LeNet contains the essence of CNNs that are still used in larger models such as the ones in ImageNet. In general, it consists of a convolutional layer followed by a pooling layer, another convolution layer followed by a pooling layer, and then two fully connected layers similar to the conventional multilayer perceptrons. </p>
<p>经典LeNet结构:</p>
<pre><code>input-&gt;conv1(20,)-pool1-conv2(50,)-pool2-f1(500,ReLU)-f2(10,softmax)-&gt;output
</code></pre>
<h3 id="lenet-train-test-prototxt"><a href="#lenet-train-test-prototxt" class="headerlink" title="lenet_train_test.prototxt"></a>lenet_train_test.prototxt</h3><ul>
<li>batch size设置在net.prototxt中而不是solver.prototxt中,用以明确blob的dims</li>
<li>bottom: layer的input blob; top: layer的output blob</li>
<li>对于0-255区间的pixel，需要归一化到0-1区间，scale &#x3D; 1&#x2F;256. &#x3D; 0.00390625</li>
<li>lr_mult: 1表示learning时，weight的learning rate需要x1;</li>
<li>lr_mult: 2表示learning时，bias的learning rate需要x2 (this usually leads to better convergence rates)</li>
<li>InnerProduct默认输出的是z,而不是a&#x3D;sigmoid(z)</li>
<li>ReLU是Inplace操作，输入输出blob都是ip1,对于其他Layer,input和output的blob不能是相同的</li>
</ul>
<h3 id="Input-Layer-types"><a href="#Input-Layer-types" class="headerlink" title="Input Layer types"></a>Input Layer types</h3><h4 id="Input-Layer-types-for-train-val-prototxt"><a href="#Input-Layer-types-for-train-val-prototxt" class="headerlink" title="Input Layer types for train_val.prototxt"></a>Input Layer types for <strong>train_val.prototxt</strong></h4><p>&#96;&#96;python<br>solver.net.forward() # load mini-batch images from training data</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">**Data** </span><br><span class="line">```prototxt</span><br><span class="line">name: &quot;mnist&quot;</span><br><span class="line">layer &#123;</span><br><span class="line">  name: &quot;mnist&quot;</span><br><span class="line">  type: &quot;Data&quot;</span><br><span class="line">  top: &quot;data&quot;</span><br><span class="line">  top: &quot;label&quot;</span><br><span class="line">  include &#123;</span><br><span class="line">    phase: TRAIN</span><br><span class="line">  &#125;</span><br><span class="line">  transform_param &#123;</span><br><span class="line">    scale: 0.00390625</span><br><span class="line">  &#125;</span><br><span class="line">  data_param &#123;</span><br><span class="line">    source: &quot;examples/mnist/mnist_train_lmdb&quot;</span><br><span class="line">    batch_size: 64</span><br><span class="line">    backend: LMDB</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">name: &quot;CaffeNet&quot;</span><br><span class="line">layer &#123;</span><br><span class="line">  name: &quot;data&quot;</span><br><span class="line">  type: &quot;Data&quot;</span><br><span class="line">  top: &quot;data&quot;</span><br><span class="line">  top: &quot;label&quot;</span><br><span class="line">  include &#123;</span><br><span class="line">    phase: TRAIN</span><br><span class="line">  &#125;</span><br><span class="line">  transform_param &#123;</span><br><span class="line">    mirror: true</span><br><span class="line">    crop_size: 227</span><br><span class="line">    mean_file: &quot;data/ilsvrc12/imagenet_mean.binaryproto&quot;</span><br><span class="line">  &#125;</span><br><span class="line"># mean pixel / channel-wise mean instead of mean image</span><br><span class="line">#  transform_param &#123;</span><br><span class="line">#    crop_size: 227</span><br><span class="line">#    mean_value: 104</span><br><span class="line">#    mean_value: 117</span><br><span class="line">#    mean_value: 123</span><br><span class="line">#    mirror: true</span><br><span class="line">#  &#125;</span><br><span class="line">  data_param &#123;</span><br><span class="line">    source: &quot;examples/imagenet/ilsvrc12_train_lmdb&quot;</span><br><span class="line">    batch_size: 256</span><br><span class="line">    backend: LMDB</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>** ImageData**: read raw images.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: &quot;data&quot;</span><br><span class="line">  type: &quot;ImageData&quot;</span><br><span class="line">  top: &quot;data&quot;</span><br><span class="line">  top: &quot;label&quot;</span><br><span class="line">  include &#123;</span><br><span class="line">    phase: TRAIN</span><br><span class="line">  &#125;</span><br><span class="line">  transform_param &#123;</span><br><span class="line">    mirror: true</span><br><span class="line">    crop_size: 227</span><br><span class="line">    mean_file: &quot;data/ilsvrc12/imagenet_mean.binaryproto&quot;</span><br><span class="line">  &#125;</span><br><span class="line">  image_data_param &#123;</span><br><span class="line">    source: &quot;data/flickr_style/train.txt&quot;</span><br><span class="line">    batch_size: 50</span><br><span class="line">    new_height: 256</span><br><span class="line">    new_width: 256</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="Input-Layer-types-for-deploy-prototxt"><a href="#Input-Layer-types-for-deploy-prototxt" class="headerlink" title="Input Layer types for deploy.prototxt"></a>Input Layer types for <strong>deploy.prototxt</strong></h4><p>** DummyData **: no labels, only for forward and get probs</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: &quot;data&quot;</span><br><span class="line">  type: &quot;DummyData&quot;</span><br><span class="line">  top: &quot;data&quot;</span><br><span class="line">  dummy_data_param &#123;</span><br><span class="line">    shape &#123;</span><br><span class="line">      dim: 1</span><br><span class="line">      dim: 3</span><br><span class="line">      dim: 227</span><br><span class="line">      dim: 227</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>** Input** :  typically used for networks that are being deployed.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: &quot;data&quot;</span><br><span class="line">  type: &quot;Input&quot;</span><br><span class="line">  top: &quot;data&quot;</span><br><span class="line">  input_param &#123; </span><br><span class="line">  shape &#123;</span><br><span class="line">      dim: 10</span><br><span class="line">      dim: 3</span><br><span class="line">      dim: 227</span><br><span class="line">      dim: 227</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="LeNet-train-val-prototxt"><a href="#LeNet-train-val-prototxt" class="headerlink" title="LeNet train_val.prototxt"></a>LeNet train_val.prototxt</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br></pre></td><td class="code"><pre><span class="line">name: &quot;LeNet&quot;</span><br><span class="line">layer &#123;</span><br><span class="line">  name: &quot;mnist&quot;</span><br><span class="line">  type: &quot;Data&quot;</span><br><span class="line">  top: &quot;data&quot;</span><br><span class="line">  top: &quot;label&quot;</span><br><span class="line">  include &#123;</span><br><span class="line">    phase: TRAIN</span><br><span class="line">  &#125;</span><br><span class="line">  transform_param &#123;</span><br><span class="line">    scale: 0.00390625</span><br><span class="line">  &#125;</span><br><span class="line">  data_param &#123;</span><br><span class="line">    source: &quot;examples/mnist/mnist_train_lmdb&quot;</span><br><span class="line">    batch_size: 64</span><br><span class="line">    backend: LMDB</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">layer &#123;</span><br><span class="line">  name: &quot;mnist&quot;</span><br><span class="line">  type: &quot;Data&quot;</span><br><span class="line">  top: &quot;data&quot;</span><br><span class="line">  top: &quot;label&quot;</span><br><span class="line">  include &#123;</span><br><span class="line">    phase: TEST</span><br><span class="line">  &#125;</span><br><span class="line">  transform_param &#123;</span><br><span class="line">    scale: 0.00390625</span><br><span class="line">  &#125;</span><br><span class="line">  data_param &#123;</span><br><span class="line">    source: &quot;examples/mnist/mnist_test_lmdb&quot;</span><br><span class="line">    batch_size: 100</span><br><span class="line">    backend: LMDB</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">layer &#123;</span><br><span class="line">  name: &quot;conv1&quot;</span><br><span class="line">  type: &quot;Convolution&quot;</span><br><span class="line">  bottom: &quot;data&quot;</span><br><span class="line">  top: &quot;conv1&quot;</span><br><span class="line">  param &#123;</span><br><span class="line">    lr_mult: 1</span><br><span class="line">  &#125;</span><br><span class="line">  param &#123;</span><br><span class="line">    lr_mult: 2</span><br><span class="line">  &#125;</span><br><span class="line">  convolution_param &#123;</span><br><span class="line">    num_output: 20</span><br><span class="line">    kernel_size: 5</span><br><span class="line">    stride: 1</span><br><span class="line">    weight_filler &#123;</span><br><span class="line">      type: &quot;xavier&quot;</span><br><span class="line">    &#125;</span><br><span class="line">    bias_filler &#123;</span><br><span class="line">      type: &quot;constant&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">layer &#123;</span><br><span class="line">  name: &quot;pool1&quot;</span><br><span class="line">  type: &quot;Pooling&quot;</span><br><span class="line">  bottom: &quot;conv1&quot;</span><br><span class="line">  top: &quot;pool1&quot;</span><br><span class="line">  pooling_param &#123;</span><br><span class="line">    pool: MAX</span><br><span class="line">    kernel_size: 2</span><br><span class="line">    stride: 2</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">layer &#123;</span><br><span class="line">  name: &quot;conv2&quot;</span><br><span class="line">  type: &quot;Convolution&quot;</span><br><span class="line">  bottom: &quot;pool1&quot;</span><br><span class="line">  top: &quot;conv2&quot;</span><br><span class="line">  param &#123;</span><br><span class="line">    lr_mult: 1</span><br><span class="line">  &#125;</span><br><span class="line">  param &#123;</span><br><span class="line">    lr_mult: 2</span><br><span class="line">  &#125;</span><br><span class="line">  convolution_param &#123;</span><br><span class="line">    num_output: 50</span><br><span class="line">    kernel_size: 5</span><br><span class="line">    stride: 1</span><br><span class="line">    weight_filler &#123;</span><br><span class="line">      type: &quot;xavier&quot;</span><br><span class="line">    &#125;</span><br><span class="line">    bias_filler &#123;</span><br><span class="line">      type: &quot;constant&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">layer &#123;</span><br><span class="line">  name: &quot;pool2&quot;</span><br><span class="line">  type: &quot;Pooling&quot;</span><br><span class="line">  bottom: &quot;conv2&quot;</span><br><span class="line">  top: &quot;pool2&quot;</span><br><span class="line">  pooling_param &#123;</span><br><span class="line">    pool: MAX</span><br><span class="line">    kernel_size: 2</span><br><span class="line">    stride: 2</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">layer &#123;</span><br><span class="line">  name: &quot;ip1&quot;</span><br><span class="line">  type: &quot;InnerProduct&quot;</span><br><span class="line">  bottom: &quot;pool2&quot;</span><br><span class="line">  top: &quot;ip1&quot;</span><br><span class="line">  param &#123;</span><br><span class="line">    lr_mult: 1</span><br><span class="line">  &#125;</span><br><span class="line">  param &#123;</span><br><span class="line">    lr_mult: 2</span><br><span class="line">  &#125;</span><br><span class="line">  inner_product_param &#123;</span><br><span class="line">    num_output: 500</span><br><span class="line">    weight_filler &#123;</span><br><span class="line">      type: &quot;xavier&quot;</span><br><span class="line">    &#125;</span><br><span class="line">    bias_filler &#123;</span><br><span class="line">      type: &quot;constant&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">layer &#123;</span><br><span class="line">  name: &quot;relu1&quot;</span><br><span class="line">  type: &quot;ReLU&quot;</span><br><span class="line">  bottom: &quot;ip1&quot;</span><br><span class="line">  top: &quot;ip1&quot;</span><br><span class="line">&#125;</span><br><span class="line">layer &#123;</span><br><span class="line">  name: &quot;ip2&quot;</span><br><span class="line">  type: &quot;InnerProduct&quot;</span><br><span class="line">  bottom: &quot;ip1&quot;</span><br><span class="line">  top: &quot;ip2&quot;</span><br><span class="line">  param &#123;</span><br><span class="line">    lr_mult: 1</span><br><span class="line">  &#125;</span><br><span class="line">  param &#123;</span><br><span class="line">    lr_mult: 2</span><br><span class="line">  &#125;</span><br><span class="line">  inner_product_param &#123;</span><br><span class="line">    num_output: 10</span><br><span class="line">    weight_filler &#123;</span><br><span class="line">      type: &quot;xavier&quot;</span><br><span class="line">    &#125;</span><br><span class="line">    bias_filler &#123;</span><br><span class="line">      type: &quot;constant&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">layer &#123;</span><br><span class="line">  name: &quot;accuracy&quot;</span><br><span class="line">  type: &quot;Accuracy&quot;</span><br><span class="line">  bottom: &quot;ip2&quot;</span><br><span class="line">  bottom: &quot;label&quot;</span><br><span class="line">  top: &quot;accuracy&quot;</span><br><span class="line">  include &#123;</span><br><span class="line">    phase: TEST</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">layer &#123;</span><br><span class="line">  name: &quot;loss&quot;</span><br><span class="line">  type: &quot;SoftmaxWithLoss&quot;</span><br><span class="line">  bottom: &quot;ip2&quot;</span><br><span class="line">  bottom: &quot;label&quot;</span><br><span class="line">  top: &quot;loss&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- kzl in-article ad -->
<p><ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-5653382914441020"
     data-ad-slot="7925631830"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h3 id="LeNet-solver-prototxt"><a href="#LeNet-solver-prototxt" class="headerlink" title="LeNet solver.prototxt"></a>LeNet solver.prototxt</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"># The train/test net protocol buffer definition</span><br><span class="line">net: &quot;examples/mnist/lenet_train_test.prototxt&quot;</span><br><span class="line">    </span><br><span class="line"># batch_size定义在net.prototxt中,train_mini_batch_size = 64,test_mini_batch_size = 100</span><br><span class="line"></span><br><span class="line"># test_iter specifies how many forward passes the test should carry out.</span><br><span class="line"># In the case of MNIST, we have test batch size 100 and 100 test iterations,</span><br><span class="line"># covering the full 10,000 testing images.</span><br><span class="line">test_iter: 100 # test_iter = num_test_images/test_mini_batch_size = 10000/100</span><br><span class="line"># Carry out testing every 500 training iterations.</span><br><span class="line">test_interval: 500</span><br><span class="line"># The base learning rate, momentum and the weight decay of the network.</span><br><span class="line">base_lr: 0.01</span><br><span class="line">momentum: 0.9</span><br><span class="line">weight_decay: 0.0005</span><br><span class="line"># The learning rate policy</span><br><span class="line">lr_policy: &quot;inv&quot;</span><br><span class="line">gamma: 0.0001</span><br><span class="line">power: 0.75</span><br><span class="line"># Display every 100 iterations</span><br><span class="line">display: 100</span><br><span class="line"># The maximum number of iterations</span><br><span class="line">max_iter: 10000  # epoch = </span><br><span class="line"># snapshot intermediate results</span><br><span class="line">snapshot: 5000</span><br><span class="line">snapshot_prefix: &quot;examples/mnist/lenet&quot;</span><br><span class="line"># solver mode: CPU or GPU</span><br><span class="line">solver_mode: GPU</span><br></pre></td></tr></table></figure>

<h4 id="learning-rate-policy-todo…"><a href="#learning-rate-policy-todo…" class="headerlink" title="learning rate policy (todo…)"></a>learning rate policy (todo…)</h4><p>This is the same policy as our default LeNet.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">s.lr_policy = &#x27;inv&#x27;</span><br><span class="line">s.gamma = 0.0001</span><br><span class="line">s.power = 0.75</span><br></pre></td></tr></table></figure>

<p> EDIT HERE to try the fixed rate (and compare with adaptive solvers)<br><code>fixed</code> is the simplest policy that keeps the learning rate constant.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">s.lr_policy = &#x27;fixed&#x27;</span><br></pre></td></tr></table></figure>

<p>Set <code>lr_policy</code> to define how the learning rate changes during training.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Here, we &#x27;step&#x27; the learning rate by multiplying it by a factor `gamma`</span><br><span class="line"># every `stepsize` iterations.</span><br><span class="line">s.lr_policy = &#x27;step&#x27;</span><br><span class="line">s.gamma = 0.1</span><br><span class="line">s.stepsize = 20000</span><br></pre></td></tr></table></figure>

<h4 id="solver-types-todo…"><a href="#solver-types-todo…" class="headerlink" title="solver types (todo…)"></a>solver types (todo…)</h4><p>solver types include “SGD”, “Adam”, and “Nesterov” among others.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">s.type = &quot;SGD&quot;</span><br></pre></td></tr></table></figure>

<h3 id="Train-LeNet"><a href="#Train-LeNet" class="headerlink" title="Train LeNet"></a>Train LeNet</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> <span class="variable">$CAFFE_ROOT</span></span><br><span class="line">./examples/mnist/train_lenet.sh</span><br></pre></td></tr></table></figure>

<pre><code>#!/usr/bin/env sh
set -e

./build/tools/caffe train --solver=examples/mnist/lenet_solver.prototxt $@
</code></pre>
<p>train output </p>
<pre><code>I0807 16:15:29.555564  4273 solver.cpp:310] Iteration 10000, loss = 0.00251452
I0807 16:15:29.555619  4273 solver.cpp:330] Iteration 10000, Testing net (#0)
I0807 16:15:29.634243  4281 data_layer.cpp:73] Restarting data prefetching from start.
I0807 16:15:29.635372  4273 solver.cpp:397]     Test net output #0: accuracy = 0.9909
I0807 16:15:29.635409  4273 solver.cpp:397]     Test net output #1: loss = 0.0302912 (* 1 = 0.0302912 loss)
I0807 16:15:29.635416  4273 solver.cpp:315] Optimization Done.
I0807 16:15:29.635439  4273 caffe.cpp:259] Optimization Done.
</code></pre>
<h3 id="Deploy-model"><a href="#Deploy-model" class="headerlink" title="Deploy model"></a>Deploy model</h3><ul>
<li>for train, <code>train_test.prototxt</code> + <code>solver.prototxt</code></li>
<li>for deploy, <code>deploy.prototxt</code>+ <code>model.caffemodel</code></li>
</ul>
<blockquote>
<p>depoly: no weight_filler,bias_filler, loaded from weights.caffemodel. if not set weights file, w,b default to 0s</p>
</blockquote>
<h3 id="PyCaffe"><a href="#PyCaffe" class="headerlink" title="PyCaffe"></a>PyCaffe</h3><p><a target="_blank" rel="noopener" href="http://caffe.berkeleyvision.org/tutorial/interfaces.html">pycaffe interfaces</a></p>
<p>The Python interface – <code>pycaffe</code> – is the caffe module and its scripts in caffe&#x2F;python. import caffe to load models, do forward and backward, handle IO, visualize networks, and even instrument model solving. All model data, derivatives, and parameters are exposed for reading and writing.</p>
<ul>
<li><code>caffe.Net</code> is the central interface for loading, configuring, and running models. </li>
<li><code>caffe.Classifier</code> and caffe.Detector provide convenience interfaces for common tasks.</li>
<li><code>caffe.SGDSolver</code> exposes the solving interface.</li>
<li><code>caffe.io</code> handles input &#x2F; output with preprocessing and protocol buffers.</li>
<li><code>caffe.draw</code> visualizes network architectures.</li>
<li>Caffe blobs are exposed as numpy ndarrays for ease-of-use and efficiency.</li>
</ul>
<p>Tutorial IPython notebooks are found in caffe&#x2F;examples: do <code>ipython notebook caffe/examples</code> to try them. For developer reference docstrings can be found throughout the code.</p>
<p>Compile pycaffe by make pycaffe. Add the module directory to your <code>$PYTHONPATH</code> by <code>export PYTHONPATH=/path/to/caffe/python:$PYTHONPATH</code> or the like for <code>import caffe</code>.</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a target="_blank" rel="noopener" href="http://caffe.berkeleyvision.org/gathered/examples/mnist.html">minist</a></li>
<li><a target="_blank" rel="noopener" href="http://caffe.berkeleyvision.org/tutorial/layers.html">caffe layers</a></li>
<li><a target="_blank" rel="noopener" href="http://caffe.berkeleyvision.org/tutorial/layers/python.html">caffe python layer</a></li>
</ul>
<h2 id="History"><a href="#History" class="headerlink" title="History"></a>History</h2><ul>
<li>20180807: created.</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kezunlin.me/blog/Install-and-Configure-Caffe-on-ubuntu-16-04/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="kezunlin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kezunlin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/Install-and-Configure-Caffe-on-ubuntu-16-04/" class="post-title-link" itemprop="url">Install and Configure Caffe on ubuntu 16.04</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-08-07 11:27:00" itemprop="dateCreated datePublished" datetime="2018-08-07T11:27:00+08:00">2018-08-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-10-14 13:39:28" itemprop="dateModified" datetime="2024-10-14T13:39:28+08:00">2024-10-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Series"><a href="#Series" class="headerlink" title="Series"></a>Series</h2><ul>
<li><a href="https://kezunlin.me/post/1739694c/">Part 1: Install and Configure Caffe on windows 10</a></li>
<li><strong><a href="https://kezunlin.me/post/b90033a9/">Part 2: Install and Configure Caffe on ubuntu 16.04</a></strong></li>
</ul>
<h2 id="Guide"><a href="#Guide" class="headerlink" title="Guide"></a>Guide</h2><p>requirements:</p>
<ul>
<li>NVIDIA driver 396.54</li>
<li><del>CUDA 8.0 + cudnn 6.0.21</del></li>
<li>CUDA 9.2 +cudnn 7.1.4</li>
<li>opencv 3.1.0 —&gt;3.3.0</li>
<li>python 2.7 + numpy 1.15.1</li>
<li><del>python 3.5.2 + numpy 1.16.2</del></li>
<li>protobuf 3.6.1 （static）</li>
<li>caffe latest</li>
</ul>
<blockquote>
<p>默认的protobuf,2.6.1测试通过。<br>此处，使用最新的3.6.1 也可以，编译caffe需要加上<code>-std=c++11</code></p>
</blockquote>
<h3 id="install-CUDA-cudnn"><a href="#install-CUDA-cudnn" class="headerlink" title="install CUDA + cudnn"></a>install CUDA + cudnn</h3><blockquote>
<p>see <a href="https://kezunlin.me/post/3f33896b/">install and configure cuda 9.2 with cudnn 7.1 on ubuntu 16.04</a><br>tips: we need to recompile caffe with cudnn 7.1</p>
</blockquote>
<p>before we compile caffe, move <code>caffe/python/caffe/selective_search_ijcv_with_python</code> folder outside caffe source folder, otherwise error occurs.</p>
<p><img src="https://kezunlin.me/images/posts/635233-20180912154818552-851608997.png" alt="recompile caffe with cudnn"></p>
<h3 id="install-protobuf"><a href="#install-protobuf" class="headerlink" title="install protobuf"></a>install protobuf</h3><blockquote>
<p>see <a href="https://kezunlin.me/post/d60ff6fe/">Part 1: compile protobuf-cpp on ubuntu 16.04</a></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">which</span> protoc </span><br><span class="line">/usr/local/bin/protoc</span><br><span class="line"></span><br><span class="line">protoc --version</span><br><span class="line">libprotoc 3.6.1</span><br></pre></td></tr></table></figure>

<blockquote>
<p>caffe使用static的libprotoc 3.6.1</p>
</blockquote>
<h3 id="install-opencv"><a href="#install-opencv" class="headerlink" title="install opencv"></a>install opencv</h3><blockquote>
<p>see <a href="https://kezunlin.me/post/15f5c3e8/">compile opencv on ubuntu 16.04</a></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">which</span> opencv_version</span><br><span class="line">/usr/local/bin/opencv_version</span><br><span class="line"></span><br><span class="line">opencv_version </span><br><span class="line">3.3.0</span><br></pre></td></tr></table></figure>

<h3 id="python"><a href="#python" class="headerlink" title="python"></a>python</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python --version</span><br><span class="line">Python 2.7.12</span><br></pre></td></tr></table></figure>

<p>check <code>numpy</code> version</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import numpy</span><br><span class="line">numpy.__version__</span><br><span class="line"><span class="string">&#x27;1.15.1&#x27;</span></span><br><span class="line"></span><br><span class="line">import numpy</span><br><span class="line">import inspect</span><br><span class="line">inspect.getfile(numpy)</span><br><span class="line"><span class="string">&#x27;/usr/local/lib/python2.7/dist-packages/numpy/__init__.pyc&#x27;</span></span><br></pre></td></tr></table></figure>

<h3 id="compile-caffe"><a href="#compile-caffe" class="headerlink" title="compile caffe"></a>compile caffe</h3><h4 id="clone-repo"><a href="#clone-repo" class="headerlink" title="clone repo"></a>clone repo</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/BVLC/caffe.git </span><br><span class="line"><span class="built_in">cd</span> caffe</span><br></pre></td></tr></table></figure>

<h4 id="update-repo"><a href="#update-repo" class="headerlink" title="update repo"></a>update repo</h4><p>update at 20180822.</p>
<p>if you change your local Makefile and <code>git pull origin master</code> merge conflict, solution</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git checkout HEAD Makefile</span><br><span class="line">git pull origin master</span><br></pre></td></tr></table></figure>

<h4 id="configure"><a href="#configure" class="headerlink" title="configure"></a>configure</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> build &amp;&amp; <span class="built_in">cd</span> build &amp;&amp; cmake-gui ..</span><br></pre></td></tr></table></figure>

<p>cmake-gui options</p>
<pre><code>USE_CUDNN ON
USE_OPENCV ON
Build_python ON
Build_python_layer ON

BLAS atlas
CMAKE_CXX_FLGAS -std=c++11

CMAKE_INSTALL_PREFIX /home/kezunlin/program/caffe/build/install
</code></pre>
<blockquote>
<p>使用<code>-std=c++11</code></p>
</blockquote>
<p>configure output </p>
<pre><code>Dependencies:
  BLAS              :   Yes (Atlas)
  Boost             :   Yes (ver. 1.66)
  glog              :   Yes
  gflags            :   Yes
  protobuf          :   Yes (ver. 3.6.1)
  lmdb              :   Yes (ver. 0.9.17)
  LevelDB           :   Yes (ver. 1.18)
  Snappy            :   Yes (ver. 1.1.3)
  OpenCV            :   Yes (ver. 3.1.0)
  CUDA              :   Yes (ver. 9.2)

NVIDIA CUDA:
  Target GPU(s)     :   Auto
  GPU arch(s)       :   sm_61
  cuDNN             :   Yes (ver. 7.1.4)

Python:
  Interpreter       :   /usr/bin/python2.7 (ver. 2.7.12)
  Libraries         :   /usr/lib/x86_64-linux-gnu/libpython2.7.so (ver 2.7.12)
  NumPy             :   /usr/lib/python2.7/dist-packages/numpy/core/include (ver 1.51.1)

Documentaion:
  Doxygen           :   /usr/bin/doxygen (1.8.11)
  config_file       :   /home/kezunlin/program/caffe/.Doxyfile

Install:
  Install path      :   /home/kezunlin/program/caffe/build/install

Configuring done
</code></pre>
<blockquote>
<p>we can also use <code>python3.5</code>  and <code>numpy 1.16.2</code></p>
</blockquote>
<pre><code>Python:
  Interpreter       :   /usr/bin/python3 (ver. 3.5.2)
  Libraries         :   /usr/lib/x86_64-linux-gnu/libpython3.5m.so (ver 3.5.2)
  NumPy             :   /home/kezunlin/.local/lib/python3.5/site-packages/numpy/core/include (ver 1.16.2)
</code></pre>
<p>use <code>-std=c++11</code>, otherwise errors occur</p>
<pre><code>make -j8
[  1%] Running C++/Python protocol buffer compiler on /home/kezunlin/program/caffe/src/caffe/proto/caffe.proto
Scanning dependencies of target caffeproto
[  1%] Building CXX object src/caffe/CMakeFiles/caffeproto.dir/__/__/include/caffe/proto/caffe.pb.cc.o
In file included from /usr/include/c++/5/mutex:35:0,
                 from /usr/local/include/google/protobuf/stubs/mutex.h:33,
                 from /usr/local/include/google/protobuf/stubs/common.h:52,
                 from /home/kezunlin/program/caffe/build/include/caffe/proto/caffe.pb.h:9,
                 from /home/kezunlin/program/caffe/build/include/caffe/proto/caffe.pb.cc:4:
/usr/include/c++/5/bits/c++0x_warning.h:32:2: error: #error This file requires compiler and library support for the ISO C++ 2011 standard. This support must be enabled with the -std=c++11 or -std=gnu++11 compiler options.
 #error This file requires compiler and library support \
</code></pre>
<h4 id="fix-gcc-error"><a href="#fix-gcc-error" class="headerlink" title="fix gcc error"></a>fix gcc error</h4><p>edit <code>/usr/local/cuda/include/host_config.h</code></p>
<p>将其中的第115行注释掉：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">error</span>-- unsupported GNU version! gcc versions later than 4.9 are not supported!</span></span><br><span class="line">======&gt;</span><br><span class="line"><span class="comment">//#error-- unsupported GNU version! gcc versions later than 4.9 are not supported!</span></span><br></pre></td></tr></table></figure>

<h4 id="fix-gflags-error"><a href="#fix-gflags-error" class="headerlink" title="fix gflags error"></a>fix gflags error</h4><ul>
<li>caffe&#x2F;include&#x2F;caffe&#x2F;common.hpp</li>
<li>caffe&#x2F;examples&#x2F;mnist&#x2F;convert_mnist_data.cpp</li>
</ul>
<p>Comment out the ifndef</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// #ifndef GFLAGS_GFLAGS_H_</span></span><br><span class="line"><span class="keyword">namespace</span> gflags = google;</span><br><span class="line"><span class="comment">// #endif  // GFLAGS_GFLAGS_H_</span></span><br></pre></td></tr></table></figure>

<h4 id="compile"><a href="#compile" class="headerlink" title="compile"></a>compile</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">make clean</span><br><span class="line">make -j8 </span><br><span class="line">make pycaffe</span><br></pre></td></tr></table></figure>

<p>output</p>
<pre><code>[  1%] Running C++/Python protocol buffer compiler on /home/kezunlin/program/caffe/src/caffe/proto/caffe.proto
Scanning dependencies of target caffeproto
[  1%] Building CXX object src/caffe/CMakeFiles/caffeproto.dir/__/__/include/caffe/proto/caffe.pb.cc.o
[  1%] Linking CXX static library ../../lib/libcaffeproto.a
[  1%] Built target caffeproto
</code></pre>
<blockquote>
<p><code>libcaffeproto.a</code>　static library</p>
</blockquote>
<h4 id="install"><a href="#install" class="headerlink" title="install"></a>install</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">make install</span><br><span class="line"></span><br><span class="line"><span class="built_in">ls</span> build/install</span><br><span class="line">bin  include  lib  python  share</span><br></pre></td></tr></table></figure>

<blockquote>
<p>install to <code>build/install</code> folder</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ls</span> build/install/lib</span><br><span class="line">libcaffeproto.a  libcaffe.so  libcaffe.so.1.0.0</span><br></pre></td></tr></table></figure>

<h4 id="advanced"><a href="#advanced" class="headerlink" title="advanced"></a>advanced</h4><ul>
<li>INTERFACE_INCLUDE_DIRECTORIES</li>
<li>INTERFACE_LINK_LIBRARIES</li>
</ul>
<blockquote>
<p>Target “caffe” has an INTERFACE_LINK_LIBRARIES property which differs from   its LINK_INTERFACE_LIBRARIES properties.</p>
</blockquote>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- kzl in-article ad -->
<p><ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-5653382914441020"
     data-ad-slot="7925631830"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h2 id="Play-with-Caffe"><a href="#Play-with-Caffe" class="headerlink" title="Play with Caffe"></a>Play with Caffe</h2><h3 id="python-caffe"><a href="#python-caffe" class="headerlink" title="python caffe"></a>python caffe</h3><h4 id="fix-python-caffe"><a href="#fix-python-caffe" class="headerlink" title="fix python caffe"></a>fix python caffe</h4><p>fix ipython 6.1 version conflict</p>
<p>vim <code>caffe/python/requirements.txt</code></p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ipython&gt;=3.0.0</span><br><span class="line">====&gt;</span><br><span class="line">ipython==5.4.1</span><br></pre></td></tr></table></figure>

<p>reinstall ipython</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> caffe/python</span><br><span class="line">python</span><br><span class="line">&gt;&gt;&gt;import caffe </span><br></pre></td></tr></table></figure>

<h4 id="python-draw-net"><a href="#python-draw-net" class="headerlink" title="python draw net"></a>python draw net</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> apt-get install graphviz</span><br><span class="line"><span class="built_in">sudo</span> pip install theano=0.9</span><br><span class="line"></span><br><span class="line"><span class="comment"># for theano d3viz</span></span><br><span class="line"><span class="built_in">sudo</span> pip install pydot==1.1.0</span><br><span class="line"><span class="built_in">sudo</span> pip install pydot-ng</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># other usefull tools</span></span><br><span class="line"><span class="built_in">sudo</span> pip install jupyter</span><br><span class="line"><span class="built_in">sudo</span> pip install seaborn</span><br></pre></td></tr></table></figure>

<blockquote>
<p>we need to install graphviz, otherwise we get ERROR:”dot” not found in path </p>
</blockquote>
<p>draw net </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> <span class="variable">$CAFFE_HOME</span></span><br><span class="line">./python/draw_net.py ./examples/mnist/lenet.prototxt ./examples/mnist/lenet.png</span><br><span class="line"></span><br><span class="line">eog ./examples/mnist/lenet.png</span><br></pre></td></tr></table></figure>

<p><img src="https://kezunlin.me/images/posts/635233-20180807151218024-1073147705.png" alt="lenet"></p>
<h3 id="cpp-caffe"><a href="#cpp-caffe" class="headerlink" title="cpp caffe"></a>cpp caffe</h3><h4 id="train-net"><a href="#train-net" class="headerlink" title="train net"></a>train net</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> caffe</span><br><span class="line">./examples/mnist/create_mnist.sh</span><br><span class="line">./examples/mnist/train_lenet.sh </span><br><span class="line"></span><br><span class="line"><span class="built_in">cat</span> ./examples/mnist/train_lenet.sh </span><br><span class="line">./build/tools/caffe train --solver=examples/mnist/lenet_solver.prototxt <span class="variable">$@</span></span><br></pre></td></tr></table></figure>

<p>output results</p>
<pre><code>I0912 15:57:28.812655 14094 solver.cpp:327] Iteration 10000, loss = 0.00272129
I0912 15:57:28.812675 14094 solver.cpp:347] Iteration 10000, Testing net (#0)
I0912 15:57:28.891481 14100 data_layer.cpp:73] Restarting data prefetching from start.
I0912 15:57:28.893678 14094 solver.cpp:414]     Test net output #0: accuracy = 0.9904
I0912 15:57:28.893707 14094 solver.cpp:414]     Test net output #1: loss = 0.0276084 (* 1 = 0.0276084 loss)
I0912 15:57:28.893714 14094 solver.cpp:332] Optimization Done.
I0912 15:57:28.893719 14094 caffe.cpp:250] Optimization Done.
</code></pre>
<blockquote>
<p>tips, for <code>caffe</code>, errors because no imdb data.</p>
</blockquote>
<pre><code>I0417 13:28:17.764714 35030 layer_factory.hpp:77] Creating layer mnist
F0417 13:28:17.765067 35030 db_lmdb.hpp:15] Check failed: mdb_status == 0 (2 vs. 0) No such file or directory
--------------------- 
</code></pre>
<h4 id="upgrade-net"><a href="#upgrade-net" class="headerlink" title="upgrade net"></a>upgrade net</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">./tools/upgrade_net_proto_text  old.prototxt new.prototxt</span><br><span class="line">./tools/upgrade_net_proto_binary  old.caffemodel new.caffemodel</span><br></pre></td></tr></table></figure>

<h4 id="caffe-time"><a href="#caffe-time" class="headerlink" title="caffe time"></a>caffe time</h4><p>yolov3</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">./build/tools/caffe time --model=<span class="string">&#x27;det/yolov3/yolov3.prototxt&#x27;</span> --iterations=100 --gpu=0</span><br><span class="line"></span><br><span class="line">I0313 10:15:41.888208 12527 caffe.cpp:408] Average Forward pass: 49.7012 ms.</span><br><span class="line">I0313 10:15:41.888213 12527 caffe.cpp:410] Average Backward pass: 84.946 ms.</span><br><span class="line">I0313 10:15:41.888248 12527 caffe.cpp:412] Average Forward-Backward: 134.85 ms.</span><br></pre></td></tr></table></figure>

<p>yolov3 5 class</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">./build/tools/caffe time --model=<span class="string">&#x27;det/autotrain/yolo3-5c.prototxt&#x27;</span> --iterations=100 --gpu=0</span><br><span class="line"></span><br><span class="line">I0313 10:19:27.283625 12894 caffe.cpp:408] Average Forward pass: 38.4823 ms.</span><br><span class="line">I0313 10:19:27.283630 12894 caffe.cpp:410] Average Backward pass: 74.1656 ms.</span><br><span class="line">I0313 10:19:27.283638 12894 caffe.cpp:412] Average Forward-Backward: 112.732 ms.</span><br></pre></td></tr></table></figure>

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- kzl in-article ad -->
<p><ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-5653382914441020"
     data-ad-slot="7925631830"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><h3 id="Caffe-Classifier"><a href="#Caffe-Classifier" class="headerlink" title="Caffe Classifier"></a>Caffe Classifier</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;caffe/caffe.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">ifdef</span> USE_OPENCV</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;opencv2/core/core.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;opencv2/highgui/highgui.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;opencv2/imgproc/imgproc.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">endif</span>  <span class="comment">// USE_OPENCV</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iosfwd&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;memory&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;utility&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">ifdef</span> USE_OPENCV</span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> caffe;  <span class="comment">// NOLINT(build/namespaces)</span></span><br><span class="line"><span class="keyword">using</span> std::string;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Pair (label, confidence) representing a prediction. */</span></span><br><span class="line"><span class="keyword">typedef</span> std::pair&lt;string, <span class="type">float</span>&gt; Prediction;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Classifier</span> &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="built_in">Classifier</span>(<span class="type">const</span> string&amp; model_file,</span><br><span class="line">             <span class="type">const</span> string&amp; trained_file,</span><br><span class="line">             <span class="type">const</span> string&amp; mean_file,</span><br><span class="line">             <span class="type">const</span> string&amp; label_file);</span><br><span class="line"></span><br><span class="line">  <span class="function">std::vector&lt;Prediction&gt; <span class="title">Classify</span><span class="params">(<span class="type">const</span> cv::Mat&amp; img, <span class="type">int</span> N = <span class="number">5</span>)</span></span>;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">private</span>:</span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">SetMean</span><span class="params">(<span class="type">const</span> string&amp; mean_file)</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="function">std::vector&lt;<span class="type">float</span>&gt; <span class="title">Predict</span><span class="params">(<span class="type">const</span> cv::Mat&amp; img)</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">WrapInputLayer</span><span class="params">(std::vector&lt;cv::Mat&gt;* input_channels)</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">Preprocess</span><span class="params">(<span class="type">const</span> cv::Mat&amp; img,</span></span></span><br><span class="line"><span class="params"><span class="function">                  std::vector&lt;cv::Mat&gt;* input_channels)</span></span>;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">private</span>:</span><br><span class="line">  shared_ptr&lt;Net&lt;<span class="type">float</span>&gt; &gt; net_;</span><br><span class="line">  cv::Size input_geometry_;</span><br><span class="line">  <span class="type">int</span> num_channels_;</span><br><span class="line">  cv::Mat mean_;</span><br><span class="line">  std::vector&lt;string&gt; labels_;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">Classifier::<span class="built_in">Classifier</span>(<span class="type">const</span> string&amp; model_file,</span><br><span class="line">                       <span class="type">const</span> string&amp; trained_file,</span><br><span class="line">                       <span class="type">const</span> string&amp; mean_file,</span><br><span class="line">                       <span class="type">const</span> string&amp; label_file) &#123;</span><br><span class="line"><span class="meta">#<span class="keyword">ifdef</span> CPU_ONLY</span></span><br><span class="line">  Caffe::<span class="built_in">set_mode</span>(Caffe::CPU);</span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line">  Caffe::<span class="built_in">set_mode</span>(Caffe::GPU);</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/* Load the network. */</span></span><br><span class="line">  net_.<span class="built_in">reset</span>(<span class="keyword">new</span> <span class="built_in">Net</span>&lt;<span class="type">float</span>&gt;(model_file, TEST));</span><br><span class="line">  net_-&gt;<span class="built_in">CopyTrainedLayersFrom</span>(trained_file);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">CHECK_EQ</span>(net_-&gt;<span class="built_in">num_inputs</span>(), <span class="number">1</span>) &lt;&lt; <span class="string">&quot;Network should have exactly one input.&quot;</span>;</span><br><span class="line">  <span class="built_in">CHECK_EQ</span>(net_-&gt;<span class="built_in">num_outputs</span>(), <span class="number">1</span>) &lt;&lt; <span class="string">&quot;Network should have exactly one output.&quot;</span>;</span><br><span class="line"></span><br><span class="line">  Blob&lt;<span class="type">float</span>&gt;* input_layer = net_-&gt;<span class="built_in">input_blobs</span>()[<span class="number">0</span>];</span><br><span class="line">  num_channels_ = input_layer-&gt;<span class="built_in">channels</span>();</span><br><span class="line">  <span class="built_in">CHECK</span>(num_channels_ == <span class="number">3</span> || num_channels_ == <span class="number">1</span>)</span><br><span class="line">    &lt;&lt; <span class="string">&quot;Input layer should have 1 or 3 channels.&quot;</span>;</span><br><span class="line">  input_geometry_ = cv::<span class="built_in">Size</span>(input_layer-&gt;<span class="built_in">width</span>(), input_layer-&gt;<span class="built_in">height</span>());</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* Load the binaryproto mean file. */</span></span><br><span class="line">  <span class="built_in">SetMean</span>(mean_file);</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* Load labels. */</span></span><br><span class="line">  <span class="function">std::ifstream <span class="title">labels</span><span class="params">(label_file.c_str())</span></span>;</span><br><span class="line">  <span class="built_in">CHECK</span>(labels) &lt;&lt; <span class="string">&quot;Unable to open labels file &quot;</span> &lt;&lt; label_file;</span><br><span class="line">  string line;</span><br><span class="line">  <span class="keyword">while</span> (std::<span class="built_in">getline</span>(labels, line))</span><br><span class="line">    labels_.<span class="built_in">push_back</span>(<span class="built_in">string</span>(line));</span><br><span class="line"></span><br><span class="line">  Blob&lt;<span class="type">float</span>&gt;* output_layer = net_-&gt;<span class="built_in">output_blobs</span>()[<span class="number">0</span>];</span><br><span class="line">  <span class="built_in">CHECK_EQ</span>(labels_.<span class="built_in">size</span>(), output_layer-&gt;<span class="built_in">channels</span>())</span><br><span class="line">    &lt;&lt; <span class="string">&quot;Number of labels is different from the output layer dimension.&quot;</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">static</span> <span class="type">bool</span> <span class="title">PairCompare</span><span class="params">(<span class="type">const</span> std::pair&lt;<span class="type">float</span>, <span class="type">int</span>&gt;&amp; lhs,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="type">const</span> std::pair&lt;<span class="type">float</span>, <span class="type">int</span>&gt;&amp; rhs)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> lhs.first &gt; rhs.first;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Return the indices of the top N values of vector v. */</span></span><br><span class="line"><span class="function"><span class="type">static</span> std::vector&lt;<span class="type">int</span>&gt; <span class="title">Argmax</span><span class="params">(<span class="type">const</span> std::vector&lt;<span class="type">float</span>&gt;&amp; v, <span class="type">int</span> N)</span> </span>&#123;</span><br><span class="line">  std::vector&lt;std::pair&lt;<span class="type">float</span>, <span class="type">int</span>&gt; &gt; pairs;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">size_t</span> i = <span class="number">0</span>; i &lt; v.<span class="built_in">size</span>(); ++i)</span><br><span class="line">    pairs.<span class="built_in">push_back</span>(std::<span class="built_in">make_pair</span>(v[i], i));</span><br><span class="line">  std::<span class="built_in">partial_sort</span>(pairs.<span class="built_in">begin</span>(), pairs.<span class="built_in">begin</span>() + N, pairs.<span class="built_in">end</span>(), PairCompare);</span><br><span class="line"></span><br><span class="line">  std::vector&lt;<span class="type">int</span>&gt; result;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; N; ++i)</span><br><span class="line">    result.<span class="built_in">push_back</span>(pairs[i].second);</span><br><span class="line">  <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Return the top N predictions. */</span></span><br><span class="line"><span class="function">std::vector&lt;Prediction&gt; <span class="title">Classifier::Classify</span><span class="params">(<span class="type">const</span> cv::Mat&amp; img, <span class="type">int</span> N)</span> </span>&#123;</span><br><span class="line">  std::vector&lt;<span class="type">float</span>&gt; output = <span class="built_in">Predict</span>(img);</span><br><span class="line"></span><br><span class="line">  N = std::<span class="built_in">min</span>&lt;<span class="type">int</span>&gt;(labels_.<span class="built_in">size</span>(), N);</span><br><span class="line">  std::vector&lt;<span class="type">int</span>&gt; maxN = <span class="built_in">Argmax</span>(output, N);</span><br><span class="line">  std::vector&lt;Prediction&gt; predictions;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; N; ++i) &#123;</span><br><span class="line">    <span class="type">int</span> idx = maxN[i];</span><br><span class="line">    predictions.<span class="built_in">push_back</span>(std::<span class="built_in">make_pair</span>(labels_[idx], output[idx]));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> predictions;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Load the mean file in binaryproto format. */</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Classifier::SetMean</span><span class="params">(<span class="type">const</span> string&amp; mean_file)</span> </span>&#123;</span><br><span class="line">  BlobProto blob_proto;</span><br><span class="line">  <span class="built_in">ReadProtoFromBinaryFileOrDie</span>(mean_file.<span class="built_in">c_str</span>(), &amp;blob_proto);</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* Convert from BlobProto to Blob&lt;float&gt; */</span></span><br><span class="line">  Blob&lt;<span class="type">float</span>&gt; mean_blob;</span><br><span class="line">  mean_blob.<span class="built_in">FromProto</span>(blob_proto);</span><br><span class="line">  <span class="built_in">CHECK_EQ</span>(mean_blob.<span class="built_in">channels</span>(), num_channels_)</span><br><span class="line">    &lt;&lt; <span class="string">&quot;Number of channels of mean file doesn&#x27;t match input layer.&quot;</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* The format of the mean file is planar 32-bit float BGR or grayscale. */</span></span><br><span class="line">  std::vector&lt;cv::Mat&gt; channels;</span><br><span class="line">  <span class="type">float</span>* data = mean_blob.<span class="built_in">mutable_cpu_data</span>();</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; num_channels_; ++i) &#123;</span><br><span class="line">    <span class="comment">/* Extract an individual channel. */</span></span><br><span class="line">    <span class="function">cv::Mat <span class="title">channel</span><span class="params">(mean_blob.height(), mean_blob.width(), CV_32FC1, data)</span></span>;</span><br><span class="line">    channels.<span class="built_in">push_back</span>(channel);</span><br><span class="line">    data += mean_blob.<span class="built_in">height</span>() * mean_blob.<span class="built_in">width</span>();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* Merge the separate channels into a single image. */</span></span><br><span class="line">  cv::Mat mean;</span><br><span class="line">  cv::<span class="built_in">merge</span>(channels, mean);</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* Compute the global mean pixel value and create a mean image</span></span><br><span class="line"><span class="comment">   * filled with this value. */</span></span><br><span class="line">  cv::Scalar channel_mean = cv::<span class="built_in">mean</span>(mean);</span><br><span class="line">  mean_ = cv::<span class="built_in">Mat</span>(input_geometry_, mean.<span class="built_in">type</span>(), channel_mean);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">std::vector&lt;<span class="type">float</span>&gt; <span class="title">Classifier::Predict</span><span class="params">(<span class="type">const</span> cv::Mat&amp; img)</span> </span>&#123;</span><br><span class="line">  Blob&lt;<span class="type">float</span>&gt;* input_layer = net_-&gt;<span class="built_in">input_blobs</span>()[<span class="number">0</span>];</span><br><span class="line">  input_layer-&gt;<span class="built_in">Reshape</span>(<span class="number">1</span>, num_channels_,</span><br><span class="line">                       input_geometry_.height, input_geometry_.width);</span><br><span class="line">  <span class="comment">/* Forward dimension change to all layers. */</span></span><br><span class="line">  net_-&gt;<span class="built_in">Reshape</span>();</span><br><span class="line"></span><br><span class="line">  std::vector&lt;cv::Mat&gt; input_channels;</span><br><span class="line">  <span class="built_in">WrapInputLayer</span>(&amp;input_channels);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">Preprocess</span>(img, &amp;input_channels);</span><br><span class="line"></span><br><span class="line">  net_-&gt;<span class="built_in">Forward</span>();</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* Copy the output layer to a std::vector */</span></span><br><span class="line">  Blob&lt;<span class="type">float</span>&gt;* output_layer = net_-&gt;<span class="built_in">output_blobs</span>()[<span class="number">0</span>];</span><br><span class="line">  <span class="type">const</span> <span class="type">float</span>* begin = output_layer-&gt;<span class="built_in">cpu_data</span>();</span><br><span class="line">  <span class="type">const</span> <span class="type">float</span>* end = begin + output_layer-&gt;<span class="built_in">channels</span>();</span><br><span class="line">  <span class="keyword">return</span> std::<span class="built_in">vector</span>&lt;<span class="type">float</span>&gt;(begin, end);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Wrap the input layer of the network in separate cv::Mat objects</span></span><br><span class="line"><span class="comment"> * (one per channel). This way we save one memcpy operation and we</span></span><br><span class="line"><span class="comment"> * don&#x27;t need to rely on cudaMemcpy2D. The last preprocessing</span></span><br><span class="line"><span class="comment"> * operation will write the separate channels directly to the input</span></span><br><span class="line"><span class="comment"> * layer. */</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Classifier::WrapInputLayer</span><span class="params">(std::vector&lt;cv::Mat&gt;* input_channels)</span> </span>&#123;</span><br><span class="line">  Blob&lt;<span class="type">float</span>&gt;* input_layer = net_-&gt;<span class="built_in">input_blobs</span>()[<span class="number">0</span>];</span><br><span class="line"></span><br><span class="line">  <span class="type">int</span> width = input_layer-&gt;<span class="built_in">width</span>();</span><br><span class="line">  <span class="type">int</span> height = input_layer-&gt;<span class="built_in">height</span>();</span><br><span class="line">  <span class="type">float</span>* input_data = input_layer-&gt;<span class="built_in">mutable_cpu_data</span>();</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; input_layer-&gt;<span class="built_in">channels</span>(); ++i) &#123;</span><br><span class="line">    <span class="function">cv::Mat <span class="title">channel</span><span class="params">(height, width, CV_32FC1, input_data)</span></span>;</span><br><span class="line">    input_channels-&gt;<span class="built_in">push_back</span>(channel);</span><br><span class="line">    input_data += width * height;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Classifier::Preprocess</span><span class="params">(<span class="type">const</span> cv::Mat&amp; img,</span></span></span><br><span class="line"><span class="params"><span class="function">                            std::vector&lt;cv::Mat&gt;* input_channels)</span> </span>&#123;</span><br><span class="line">  <span class="comment">/* Convert the input image to the input image format of the network. */</span></span><br><span class="line">  cv::Mat sample;</span><br><span class="line">  <span class="keyword">if</span> (img.<span class="built_in">channels</span>() == <span class="number">3</span> &amp;&amp; num_channels_ == <span class="number">1</span>)</span><br><span class="line">    cv::<span class="built_in">cvtColor</span>(img, sample, cv::COLOR_BGR2GRAY);</span><br><span class="line">  <span class="keyword">else</span> <span class="keyword">if</span> (img.<span class="built_in">channels</span>() == <span class="number">4</span> &amp;&amp; num_channels_ == <span class="number">1</span>)</span><br><span class="line">    cv::<span class="built_in">cvtColor</span>(img, sample, cv::COLOR_BGRA2GRAY);</span><br><span class="line">  <span class="keyword">else</span> <span class="keyword">if</span> (img.<span class="built_in">channels</span>() == <span class="number">4</span> &amp;&amp; num_channels_ == <span class="number">3</span>)</span><br><span class="line">    cv::<span class="built_in">cvtColor</span>(img, sample, cv::COLOR_BGRA2BGR);</span><br><span class="line">  <span class="keyword">else</span> <span class="keyword">if</span> (img.<span class="built_in">channels</span>() == <span class="number">1</span> &amp;&amp; num_channels_ == <span class="number">3</span>)</span><br><span class="line">    cv::<span class="built_in">cvtColor</span>(img, sample, cv::COLOR_GRAY2BGR);</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    sample = img;</span><br><span class="line"></span><br><span class="line">  cv::Mat sample_resized;</span><br><span class="line">  <span class="keyword">if</span> (sample.<span class="built_in">size</span>() != input_geometry_)</span><br><span class="line">    cv::<span class="built_in">resize</span>(sample, sample_resized, input_geometry_);</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    sample_resized = sample;</span><br><span class="line"></span><br><span class="line">  cv::Mat sample_float;</span><br><span class="line">  <span class="keyword">if</span> (num_channels_ == <span class="number">3</span>)</span><br><span class="line">    sample_resized.<span class="built_in">convertTo</span>(sample_float, CV_32FC3);</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    sample_resized.<span class="built_in">convertTo</span>(sample_float, CV_32FC1);</span><br><span class="line"></span><br><span class="line">  cv::Mat sample_normalized;</span><br><span class="line">  cv::<span class="built_in">subtract</span>(sample_float, mean_, sample_normalized);</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* This operation will write the separate BGR planes directly to the</span></span><br><span class="line"><span class="comment">   * input layer of the network because it is wrapped by the cv::Mat</span></span><br><span class="line"><span class="comment">   * objects in input_channels. */</span></span><br><span class="line">  cv::<span class="built_in">split</span>(sample_normalized, *input_channels);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">reinterpret_cast</span>&lt;<span class="type">float</span>*&gt;(input_channels-&gt;<span class="built_in">at</span>(<span class="number">0</span>).data)</span><br><span class="line">        == net_-&gt;<span class="built_in">input_blobs</span>()[<span class="number">0</span>]-&gt;<span class="built_in">cpu_data</span>())</span><br><span class="line">    &lt;&lt; <span class="string">&quot;Input channels are not wrapping the input layer of the network.&quot;</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span>** argv)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (argc != <span class="number">6</span>) &#123;</span><br><span class="line">    std::cerr &lt;&lt; <span class="string">&quot;Usage: &quot;</span> &lt;&lt; argv[<span class="number">0</span>]</span><br><span class="line">              &lt;&lt; <span class="string">&quot; deploy.prototxt network.caffemodel&quot;</span></span><br><span class="line">              &lt;&lt; <span class="string">&quot; mean.binaryproto labels.txt img.jpg&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  ::google::<span class="built_in">InitGoogleLogging</span>(argv[<span class="number">0</span>]);</span><br><span class="line"></span><br><span class="line">  string model_file   = argv[<span class="number">1</span>];</span><br><span class="line">  string trained_file = argv[<span class="number">2</span>];</span><br><span class="line">  string mean_file    = argv[<span class="number">3</span>];</span><br><span class="line">  string label_file   = argv[<span class="number">4</span>];</span><br><span class="line">  <span class="function">Classifier <span class="title">classifier</span><span class="params">(model_file, trained_file, mean_file, label_file)</span></span>;</span><br><span class="line"></span><br><span class="line">  string file = argv[<span class="number">5</span>];</span><br><span class="line"></span><br><span class="line">  std::cout &lt;&lt; <span class="string">&quot;---------- Prediction for &quot;</span></span><br><span class="line">            &lt;&lt; file &lt;&lt; <span class="string">&quot; ----------&quot;</span> &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">  cv::Mat img = cv::<span class="built_in">imread</span>(file, <span class="number">-1</span>);</span><br><span class="line">  <span class="built_in">CHECK</span>(!img.<span class="built_in">empty</span>()) &lt;&lt; <span class="string">&quot;Unable to decode image &quot;</span> &lt;&lt; file;</span><br><span class="line">  std::vector&lt;Prediction&gt; predictions = classifier.<span class="built_in">Classify</span>(img);</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* Print the top N predictions. */</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="type">size_t</span> i = <span class="number">0</span>; i &lt; predictions.<span class="built_in">size</span>(); ++i) &#123;</span><br><span class="line">    Prediction p = predictions[i];</span><br><span class="line">    std::cout &lt;&lt; std::fixed &lt;&lt; std::<span class="built_in">setprecision</span>(<span class="number">4</span>) &lt;&lt; p.second &lt;&lt; <span class="string">&quot; - \&quot;&quot;</span></span><br><span class="line">              &lt;&lt; p.first &lt;&lt; <span class="string">&quot;\&quot;&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span>** argv)</span> </span>&#123;</span><br><span class="line">  <span class="built_in">LOG</span>(FATAL) &lt;&lt; <span class="string">&quot;This example requires OpenCV; compile with USE_OPENCV.&quot;</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span>  <span class="comment">// USE_OPENCV</span></span></span><br></pre></td></tr></table></figure>

<h3 id="CMakeLists-txt"><a href="#CMakeLists-txt" class="headerlink" title="CMakeLists.txt"></a>CMakeLists.txt</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">find_package(OpenCV REQUIRED)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">set</span>(Caffe_DIR <span class="string">&quot;/home/kezunlin/program/caffe/build/install/share/Caffe&quot;</span>)  <span class="comment"># caffe caffe</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># for CaffeConfig.cmake/ caffe-config.cmake</span></span><br><span class="line">find_package(Caffe)</span><br><span class="line"><span class="comment"># offical caffe  : There is no Caffe_INCLUDE_DIRS and Caffe_DEFINITIONS</span></span><br><span class="line"><span class="comment"># refinedet caffe: OK.</span></span><br><span class="line"></span><br><span class="line">add_definitions(<span class="variable">$&#123;Caffe_DEFINITIONS&#125;</span>)</span><br><span class="line"></span><br><span class="line">MESSAGE( [Main] <span class="string">&quot; Caffe_INCLUDE_DIRS = <span class="variable">$&#123;Caffe_INCLUDE_DIRS&#125;</span>&quot;</span>)</span><br><span class="line">MESSAGE( [Main] <span class="string">&quot; Caffe_DEFINITIONS = <span class="variable">$&#123;Caffe_DEFINITIONS&#125;</span>&quot;</span>)</span><br><span class="line">MESSAGE( [Main] <span class="string">&quot; Caffe_LIBRARIES = <span class="variable">$&#123;Caffe_LIBRARIES&#125;</span>&quot;</span>) <span class="comment"># caffe</span></span><br><span class="line">MESSAGE( [Main] <span class="string">&quot; Caffe_CPU_ONLY = <span class="variable">$&#123;Caffe_CPU_ONLY&#125;</span>&quot;</span>)</span><br><span class="line">MESSAGE( [Main] <span class="string">&quot; Caffe_HAVE_CUDA = <span class="variable">$&#123;Caffe_HAVE_CUDA&#125;</span>&quot;</span>)</span><br><span class="line">MESSAGE( [Main] <span class="string">&quot; Caffe_HAVE_CUDNN = <span class="variable">$&#123;Caffe_HAVE_CUDNN&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">include_directories(<span class="variable">$&#123;Caffe_INCLUDE_DIRS&#125;</span>)</span><br><span class="line"></span><br><span class="line">target_link_libraries(demo </span><br><span class="line">    <span class="variable">$&#123;OpenCV_LIBS&#125;</span></span><br><span class="line">    <span class="variable">$&#123;Caffe_LIBRARIES&#125;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h3 id="run"><a href="#run" class="headerlink" title="run"></a>run</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./demo</span><br></pre></td></tr></table></figure>

<p>if error occurs:</p>
<pre><code>libcaffe.so.1.0.0 =&gt; not found
</code></pre>
<p>edit <code>.bashrc</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># for caffe</span></span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=/home/kezunlin/program/caffe/build/install/lib:<span class="variable">$LD_LIBRARY_PATH</span></span><br></pre></td></tr></table></figure>

<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a target="_blank" rel="noopener" href="http://caffe.berkeleyvision.org/install_apt.html">caffe offical</a> </li>
<li><a target="_blank" rel="noopener" href="https://www.pyimagesearch.com/2017/09/27/setting-up-ubuntu-16-04-cuda-gpu-for-deep-learning-with-python/">setting-up-ubuntu-16-04-cuda-gpu-for-deep-learning-with-python</a></li>
<li><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/30426734/caffe-recompile-libgflags-a-with-fpic-error">gflags error</a></li>
</ul>
<h2 id="History"><a href="#History" class="headerlink" title="History"></a>History</h2><ul>
<li>20180807: created.</li>
<li>20180822: update cmake-gui for caffe</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kezunlin.me/blog/tutorials-on-deep-learning-for-image-classification/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="kezunlin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kezunlin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/tutorials-on-deep-learning-for-image-classification/" class="post-title-link" itemprop="url">tutorials on deep learning for image classification</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-08-07 11:11:00" itemprop="dateCreated datePublished" datetime="2018-08-07T11:11:00+08:00">2018-08-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-10-14 13:39:28" itemprop="dateModified" datetime="2024-10-14T13:39:28+08:00">2024-10-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h2><h3 id="Shared-variables-tips"><a href="#Shared-variables-tips" class="headerlink" title="Shared variables tips"></a>Shared variables tips</h3><p>We encourage you to store the dataset into shared variables and access it based on the minibatch index, given a fixed and known batch size. The reason behind shared variables is related to using the GPU. There is a large overhead when copying data into the GPU memory. </p>
<blockquote>
<p>将数据存储在shared variable便于加速GPU计算，避免数据从CPU拷贝到GPU。</p>
</blockquote>
<p>If you have your data in Theano shared variables though, you give Theano the possibility to copy the entire data on the GPU in a single call when the shared variables are constructed.</p>
<blockquote>
<p>shared构建的时候，theano一次性讲所有数据拷贝至GPU.</p>
</blockquote>
<p>Because the datapoints and their labels are usually of different nature (labels are usually integers while datapoints are real numbers) we suggest to use different variables for label and data. Also we recommend using different variables for the training set, validation set and testing set to make the code more readable (resulting in 6 different shared variables).</p>
<blockquote>
<p>data,label使用２个shared variables，train,valid,test使用不同的shared variables，总计6个</p>
</blockquote>
<h3 id="Mini-batch-data"><a href="#Mini-batch-data" class="headerlink" title="Mini-batch data"></a>Mini-batch data</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">shared_dataset</span>(<span class="params">data_xy</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Function that loads the dataset into shared variables</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The reason we store our dataset in shared variables is to allow</span></span><br><span class="line"><span class="string">    Theano to copy it into the GPU memory (when code is run on GPU).</span></span><br><span class="line"><span class="string">    Since copying data into the GPU is slow, copying a minibatch everytime</span></span><br><span class="line"><span class="string">    is needed (the default behaviour if the data is not in a shared</span></span><br><span class="line"><span class="string">    variable) would lead to a large decrease in performance.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    data_x, data_y = data_xy</span><br><span class="line">    shared_x = theano.shared(numpy.asarray(data_x, dtype=theano.config.floatX))</span><br><span class="line">    shared_y = theano.shared(numpy.asarray(data_y, dtype=theano.config.floatX))</span><br><span class="line">    <span class="comment"># shared变量中的数据在GPU上必须是float32类型，但是计算阶段可能需要int类型(y)，所以需要</span></span><br><span class="line">    <span class="comment"># 将float32---&gt;int.</span></span><br><span class="line">    <span class="comment"># When storing data on the GPU it has to be stored as floats</span></span><br><span class="line">    <span class="comment"># therefore we will store the labels as ``floatX`` as well</span></span><br><span class="line">    <span class="comment"># (``shared_y`` does exactly that). But during our computations</span></span><br><span class="line">    <span class="comment"># we need them as ints (we use labels as index, and if they are</span></span><br><span class="line">    <span class="comment"># floats it doesn&#x27;t make sense) therefore instead of returning</span></span><br><span class="line">    <span class="comment"># ``shared_y`` we will have to cast it to int. This little hack</span></span><br><span class="line">    <span class="comment"># lets us get around this issue</span></span><br><span class="line">    <span class="keyword">return</span> shared_x, T.cast(shared_y, <span class="string">&#x27;int32&#x27;</span>)</span><br><span class="line"></span><br><span class="line">test_set_x, test_set_y = shared_dataset(test_set)</span><br><span class="line">valid_set_x, valid_set_y = shared_dataset(valid_set)</span><br><span class="line">train_set_x, train_set_y = shared_dataset(train_set)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">500</span>    <span class="comment"># size of the minibatch</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># accessing the third minibatch of the training set</span></span><br><span class="line"></span><br><span class="line">data  = train_set_x[<span class="number">2</span> * batch_size: <span class="number">3</span> * batch_size]</span><br><span class="line">label = train_set_y[<span class="number">2</span> * batch_size: <span class="number">3</span> * batch_size]</span><br></pre></td></tr></table></figure>

<h3 id="SGD-pseudo-code-in-theano"><a href="#SGD-pseudo-code-in-theano" class="headerlink" title="SGD pseudo code in theano"></a>SGD pseudo code in theano</h3><h4 id="Traditional-GD-m-N"><a href="#Traditional-GD-m-N" class="headerlink" title="Traditional GD (m&#x3D;N)"></a>Traditional GD (m&#x3D;N)</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADIENT DESCENT</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    loss = f(params)</span><br><span class="line">    d_loss_wrt_params = ... <span class="comment"># compute gradient</span></span><br><span class="line">    params -= learning_rate * d_loss_wrt_params</span><br><span class="line">    <span class="keyword">if</span> &lt;stopping condition <span class="keyword">is</span> met&gt;:</span><br><span class="line">        <span class="keyword">return</span> params</span><br></pre></td></tr></table></figure>

<p>Stochastic gradient descent (SGD) works according to the same principles as ordinary gradient descent, but proceeds more quickly by estimating the gradient from just a few examples at a time instead of the entire training set. In its purest form, we estimate the gradient from just a single example at a time.</p>
<h4 id="Online-Learning-SGD-m-1"><a href="#Online-Learning-SGD-m-1" class="headerlink" title="Online Learning SGD (m&#x3D;1)"></a>Online Learning SGD (m&#x3D;1)</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># STOCHASTIC GRADIENT DESCENT</span></span><br><span class="line"><span class="keyword">for</span> (x_i,y_i) <span class="keyword">in</span> training_set:</span><br><span class="line">                            <span class="comment"># imagine an infinite generator</span></span><br><span class="line">                            <span class="comment"># that may repeat examples (if there is only a finite training set)</span></span><br><span class="line">    loss = f(params, x_i, y_i)</span><br><span class="line">    d_loss_wrt_params = ... <span class="comment"># compute gradient</span></span><br><span class="line">    params -= learning_rate * d_loss_wrt_params</span><br><span class="line">    <span class="keyword">if</span> &lt;stopping condition <span class="keyword">is</span> met&gt;:</span><br><span class="line">        <span class="keyword">return</span> params</span><br></pre></td></tr></table></figure>

<p>The variant that we recommend for deep learning is a further twist on stochastic gradient descent using so-called “minibatches”. Minibatch SGD (MSGD) works identically to SGD, except that we use more than one training example to make each estimate of the gradient. This technique reduces variance in the estimate of the gradient, and often makes better use of the hierarchical memory organization in modern computers.</p>
<h4 id="Minibatch-SGD-m"><a href="#Minibatch-SGD-m" class="headerlink" title="Minibatch SGD (m)"></a>Minibatch SGD (m)</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (x_batch,y_batch) <span class="keyword">in</span> train_batches:</span><br><span class="line">                            <span class="comment"># imagine an infinite generator</span></span><br><span class="line">                            <span class="comment"># that may repeat examples</span></span><br><span class="line">    loss = f(params, x_batch, y_batch)</span><br><span class="line">    d_loss_wrt_params = ... <span class="comment"># compute gradient using theano</span></span><br><span class="line">    params -= learning_rate * d_loss_wrt_params</span><br><span class="line">    <span class="keyword">if</span> &lt;stopping condition <span class="keyword">is</span> met&gt;:</span><br><span class="line">        <span class="keyword">return</span> params</span><br></pre></td></tr></table></figure>

<h4 id="Theano-pseudo-code-of-Minibatch-SGD"><a href="#Theano-pseudo-code-of-Minibatch-SGD" class="headerlink" title="Theano pseudo code of Minibatch SGD"></a>Theano pseudo code of Minibatch SGD</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Minibatch Stochastic Gradient Descent</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># assume loss is a symbolic description of the loss function given</span></span><br><span class="line"><span class="comment"># the symbolic variables params (shared variable), x_batch, y_batch;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># compute gradient of loss with respect to params</span></span><br><span class="line">d_loss_wrt_params = T.grad(loss, params)</span><br><span class="line"></span><br><span class="line"><span class="comment"># compile the MSGD step into a theano function</span></span><br><span class="line">updates = [(params, params - learning_rate * d_loss_wrt_params)]</span><br><span class="line">MSGD = theano.function([x_batch,y_batch], loss, updates=updates)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (x_batch, y_batch) <span class="keyword">in</span> train_batches:</span><br><span class="line">    <span class="comment"># here x_batch and y_batch are elements of train_batches and</span></span><br><span class="line">    <span class="comment"># therefore numpy arrays; function MSGD also updates the params</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Current loss is &#x27;</span>, MSGD(x_batch, y_batch))</span><br><span class="line">    <span class="keyword">if</span> stopping_condition_is_met:</span><br><span class="line">        <span class="keyword">return</span> params</span><br></pre></td></tr></table></figure>

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- kzl in-article ad -->
<p><ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-5653382914441020"
     data-ad-slot="7925631830"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>




<h3 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h3><h4 id="L1-L2-regularization"><a href="#L1-L2-regularization" class="headerlink" title="L1&#x2F;L2 regularization"></a>L1&#x2F;L2 regularization</h4><p>L1&#x2F;L2 regularization and early-stopping.</p>
<p>Commonly used values for p are 1 and 2, hence the L1&#x2F;L2 nomenclature. If p&#x3D;2, then the regularizer is also called “weight decay”.</p>
<p> To follow Occam’s razor principle, this minimization should find us the simplest solution (as measured by our simplicity criterion) that fits the training data.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># symbolic Theano variable that represents the L1 regularization term</span></span><br><span class="line">L1  = T.<span class="built_in">sum</span>(<span class="built_in">abs</span>(param))</span><br><span class="line"></span><br><span class="line"><span class="comment"># symbolic Theano variable that represents the squared L2 term</span></span><br><span class="line">L2 = T.<span class="built_in">sum</span>(param ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># the loss</span></span><br><span class="line">loss = NLL + lambda_1 * L1 + lambda_2 * L2</span><br></pre></td></tr></table></figure>

<h4 id="Early-Stopping"><a href="#Early-Stopping" class="headerlink" title="Early-Stopping"></a>Early-Stopping</h4><p>Early-stopping combats overfitting by monitoring the model’s performance on a validation set. A validation set is a set of examples that we never use for gradient descent, but which is also not a part of the test set. </p>
<p>所谓early stopping，即在每一个epoch结束时（一个epoch即对所有训练数据的一轮遍历）计算 validation data的accuracy，当accuracy不再提高时，就停止训练。这是很自然的做法，因为accuracy不再提高了，训练下去也没用。另外，这样做还能防止overfitting。</p>
<p>那么，怎么样才算是validation accuracy不再提高呢？并不是说validation accuracy一降下来，它就是“不再提高”，因为可能经过这个epoch后，accuracy降低了，但是随后的epoch又让accuracy升上去了，所以不能根据一两次的连续降低就判断“不再提高”。正确的做法是，在训练的过程中，记录最佳的validation accuracy，当连续10次epoch（或者更多次）没达到最佳accuracy时，你可以认为“不再提高”，此时使用early stopping。这个策略就叫“ no-improvement-in-n”，n即epoch的次数，可以根据实际情况取10、20、30….</p>
<h4 id="Variable-learning-rate"><a href="#Variable-learning-rate" class="headerlink" title="Variable learning rate"></a>Variable learning rate</h4><ul>
<li><p>Decreasing the learning rate over time is sometimes a good idea. eta &#x3D; eta0&#x2F;(1+d*epoch)  (d: eta decrease constant, d&#x3D;0.001)</p>
</li>
<li><p>Early stopping + decrease learning rate. eta &#x3D; eta0&#x2F;2 until eta&#x3D; eta0&#x2F;1024</p>
</li>
</ul>
<p>一个简单有效的做法就是，当validation accuracy满足 no-improvement-in-n规则时，本来我们是要early stopping的，但是我们可以不stop，而是让learning rate减半，之后让程序继续跑。下一次validation accuracy又满足no-improvement-in-n规则时，我们同样再将learning rate减半（此时变为原始learni rate的四分之一）…继续这个过程，直到learning rate变为原来的1&#x2F;1024再终止程序。（1&#x2F;1024还是1&#x2F;512还是其他可以根据实际确定）。【PS：也可以选择每一次将learning rate除以10，而不是除以2.】</p>
<p>实践中，eta&#x2F;2变化太快，eta0&#x2F;(1+d*epoch),d&#x3D;0.001比较合适。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># early-stopping parameters</span></span><br><span class="line">patience = <span class="number">5000</span>  <span class="comment"># look as this many examples regardless</span></span><br><span class="line">patience_increase = <span class="number">2</span>     <span class="comment"># wait this much longer when a new best is</span></span><br><span class="line">                              <span class="comment"># found</span></span><br><span class="line">improvement_threshold = <span class="number">0.995</span>  <span class="comment"># a relative improvement of this much is</span></span><br><span class="line">                               <span class="comment"># considered significant</span></span><br><span class="line">validation_frequency = <span class="built_in">min</span>(n_train_batches, patience/<span class="number">2</span>) = <span class="number">2500</span> <span class="comment"># for iters</span></span><br><span class="line">                              <span class="comment"># go through this many</span></span><br><span class="line">                              <span class="comment"># minibatches before checking the network</span></span><br><span class="line">                              <span class="comment"># on the validation set; in this case we</span></span><br><span class="line">                              <span class="comment"># check every epoch</span></span><br><span class="line"></span><br><span class="line">best_params = <span class="literal">None</span></span><br><span class="line">best_validation_loss = numpy.inf</span><br><span class="line">test_score = <span class="number">0.</span></span><br><span class="line">start_time = time.clock()</span><br><span class="line"></span><br><span class="line">done_looping = <span class="literal">False</span></span><br><span class="line">epoch = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> (epoch &lt; n_epochs) <span class="keyword">and</span> (<span class="keyword">not</span> done_looping):</span><br><span class="line">    <span class="comment"># Report &quot;1&quot; for first epoch, &quot;n_epochs&quot; for last epoch</span></span><br><span class="line">    epoch = epoch + <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> minibatch_index <span class="keyword">in</span> <span class="built_in">range</span>(n_train_batches):</span><br><span class="line"></span><br><span class="line">        d_loss_wrt_params = ... <span class="comment"># compute gradient</span></span><br><span class="line">        params -= learning_rate * d_loss_wrt_params <span class="comment"># gradient descent</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># iteration number. We want it to start at 0.</span></span><br><span class="line">        <span class="built_in">iter</span> = (epoch - <span class="number">1</span>) * n_train_batches + minibatch_index</span><br><span class="line">        <span class="comment"># note that if we do `iter % validation_frequency` it will be</span></span><br><span class="line">        <span class="comment"># true for iter = 0 which we do not want. We want it true for</span></span><br><span class="line">        <span class="comment"># iter = validation_frequency - 1.</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">iter</span> + <span class="number">1</span>) % validation_frequency == <span class="number">0</span>:</span><br><span class="line"></span><br><span class="line">            this_validation_loss = ... <span class="comment"># compute zero-one loss on validation set</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> this_validation_loss &lt; best_validation_loss:</span><br><span class="line"></span><br><span class="line">                <span class="comment"># improve patience if loss improvement is good enough</span></span><br><span class="line">                <span class="keyword">if</span> this_validation_loss &lt; best_validation_loss * improvement_threshold:</span><br><span class="line">                    patience = <span class="built_in">max</span>(patience, <span class="built_in">iter</span> * patience_increase)</span><br><span class="line">                    </span><br><span class="line">                best_params = copy.deepcopy(params)</span><br><span class="line">                best_validation_loss = this_validation_loss</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> patience &lt;= <span class="built_in">iter</span>:</span><br><span class="line">            done_looping = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># POSTCONDITION:</span></span><br><span class="line"><span class="comment"># best_params refers to the best out-of-sample parameters observed during the optimization</span></span><br></pre></td></tr></table></figure>

<h3 id="Theano-Python-Tips"><a href="#Theano-Python-Tips" class="headerlink" title="Theano&#x2F;Python Tips"></a>Theano&#x2F;Python Tips</h3><h4 id="Loading-and-Saving-Models"><a href="#Loading-and-Saving-Models" class="headerlink" title="Loading and Saving Models"></a>Loading and Saving Models</h4><ul>
<li><p>DO: <strong>Pickle the numpy ndarrays from your shared variables</strong></p>
</li>
<li><p>DON’T: <strong>Do not pickle your training or test functions for long-term storage</strong></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cPickle</span><br><span class="line">save_file = <span class="built_in">open</span>(<span class="string">&#x27;path&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>)  <span class="comment"># this will overwrite current contents</span></span><br><span class="line">cPickle.dump(w.get_value(borrow=<span class="literal">True</span>), save_file, -<span class="number">1</span>)  <span class="comment"># the -1 is for HIGHEST_PROTOCOL</span></span><br><span class="line">cPickle.dump(v.get_value(borrow=<span class="literal">True</span>), save_file, -<span class="number">1</span>)  <span class="comment"># .. and it triggers much more efficient</span></span><br><span class="line">cPickle.dump(u.get_value(borrow=<span class="literal">True</span>), save_file, -<span class="number">1</span>)  <span class="comment"># .. storage than numpy&#x27;s default</span></span><br><span class="line">save_file.close()</span><br></pre></td></tr></table></figure>

<p>Then later, you can load your data back like this:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">save_file = <span class="built_in">open</span>(<span class="string">&#x27;path&#x27;</span>)</span><br><span class="line">w.set_value(cPickle.load(save_file), borrow=<span class="literal">True</span>)</span><br><span class="line">v.set_value(cPickle.load(save_file), borrow=<span class="literal">True</span>)</span><br><span class="line">u.set_value(cPickle.load(save_file), borrow=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h4 id="Plotting-Intermediate-Results"><a href="#Plotting-Intermediate-Results" class="headerlink" title="Plotting Intermediate Results"></a>Plotting Intermediate Results</h4><p>If you have enough disk space, your training script should save intermediate models and a visualization script should process those saved models.</p>
<h2 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a>MLP</h2><blockquote>
<p>see <a target="_blank" rel="noopener" href="http://deeplearning.net/tutorial/mlp.html">here</a></p>
</blockquote>
<p>An MLP can be viewed as a logistic regression classifier where the input is first transformed using a learnt non-linear transformation sigmoid. This transformation projects the input data into a space where it becomes linearly separable. This intermediate layer is referred to as a hidden layer. A single hidden layer is sufficient to make MLPs a universal approximator. </p>
<h3 id="weight-initializations"><a href="#weight-initializations" class="headerlink" title="weight initializations"></a>weight initializations</h3><ul>
<li>old version: 1&#x2F;sqrt(n_in)</li>
</ul>
<p>The initial values for the weights of a hidden layer i should be uniformly sampled from a symmetric interval that depends on the activation function. weight的初始化依赖于activation</p>
<ul>
<li>tanh: uniformely sampled from -sqrt(6.&#x2F;(n_in+n_hidden)) and sqrt(6.&#x2F;(n_in+n_hidden)) </li>
<li>sigmoid : use 4 times larger initial weights for sigmoid compared to tanh</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># `W` is initialized with `W_values` which is uniformely sampled</span></span><br><span class="line">        <span class="comment"># from sqrt(-6./(n_in+n_hidden)) and sqrt(6./(n_in+n_hidden))</span></span><br><span class="line">        <span class="comment"># for tanh activation function</span></span><br><span class="line">        <span class="comment"># the output of uniform if converted using asarray to dtype</span></span><br><span class="line">        <span class="comment"># theano.config.floatX so that the code is runable on GPU</span></span><br><span class="line">        <span class="comment"># Note : optimal initialization of weights is dependent on the</span></span><br><span class="line">        <span class="comment">#        activation function used (among other things).</span></span><br><span class="line">        <span class="comment">#        For example, results presented in [Xavier10] suggest that you</span></span><br><span class="line">        <span class="comment">#        should use 4 times larger initial weights for sigmoid</span></span><br><span class="line">        <span class="comment">#        compared to tanh</span></span><br><span class="line">        <span class="comment">#        We have no info for other function, so we use the same as</span></span><br><span class="line">        <span class="comment">#        tanh.</span></span><br><span class="line">        <span class="keyword">if</span> W <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            W_values = numpy.asarray(</span><br><span class="line">                rng.uniform(</span><br><span class="line">                    low=-numpy.sqrt(<span class="number">6.</span> / (n_in + n_out)),</span><br><span class="line">                    high=numpy.sqrt(<span class="number">6.</span> / (n_in + n_out)),</span><br><span class="line">                    size=(n_in, n_out)</span><br><span class="line">                ),</span><br><span class="line">                dtype=theano.config.floatX</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">if</span> activation == theano.tensor.nnet.sigmoid:</span><br><span class="line">                W_values *= <span class="number">4</span></span><br><span class="line"></span><br><span class="line">            W = theano.shared(value=W_values, name=<span class="string">&#x27;W&#x27;</span>, borrow=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> b <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)</span><br><span class="line">            b = theano.shared(value=b_values, name=<span class="string">&#x27;b&#x27;</span>, borrow=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.W = W</span><br><span class="line">        <span class="variable language_">self</span>.b = b</span><br></pre></td></tr></table></figure>

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- kzl in-article ad -->
<p><ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-5653382914441020"
     data-ad-slot="7925631830"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h3 id="Tips-and-Tricks-for-training-MLPs"><a href="#Tips-and-Tricks-for-training-MLPs" class="headerlink" title="Tips and Tricks for training MLPs"></a>Tips and Tricks for training MLPs</h3><h4 id="Nonlinearity"><a href="#Nonlinearity" class="headerlink" title="Nonlinearity"></a>Nonlinearity</h4><p>Two of the most common ones are the <strong>sigmoid</strong> and the <strong>tanh</strong> function. nonlinearities that are symmetric around the origin are preferred because they tend to produce zero-mean inputs to the next layer (which is a desirable property). Empirically, we have observed that the tanh has better convergence properties.</p>
<h4 id="Weight-initialization"><a href="#Weight-initialization" class="headerlink" title="Weight initialization"></a>Weight initialization</h4><p>At initialization we want the weights to be small enough around the origin so that the activation function operates in its linear regime, where gradients are the largest. <strong>weight的初始化依赖于activation</strong></p>
<h4 id="Learning-rate"><a href="#Learning-rate" class="headerlink" title="Learning rate"></a>Learning rate</h4><ul>
<li><p>The simplest solution is to simply have a <strong>constant rate</strong>. Rule of thumb: try several log-spaced values (10^{-1},10^{-2},\ldots) and narrow the (logarithmic) grid search to the region where you obtain the lowest validation error.</p>
</li>
<li><p>Decreasing the learning rate over time is sometimes a good idea. eta &#x3D; eta0&#x2F;(1+d*epoch)  (d: decrease constant, 0.001)</p>
</li>
<li><p>Early stopping + decrease learning rate. eta &#x3D; eta0&#x2F;2 until eta&#x3D; eta0&#x2F;1024</p>
</li>
</ul>
<h4 id="Regularization-parameter"><a href="#Regularization-parameter" class="headerlink" title="Regularization parameter"></a>Regularization parameter</h4><p>Typical values to try for the L1&#x2F;L2 regularization parameter \lambda are 10^{-2},10^{-3},\ldots. In the framework that we described so far, optimizing this parameter will not lead to significantly better solutions, but is worth exploring nonetheless.</p>
<h4 id="Number-of-hidden-units"><a href="#Number-of-hidden-units" class="headerlink" title="Number of hidden units"></a>Number of hidden units</h4><p>This hyper-parameter is very much dataset-dependent. <strong>hidden neurons的数量依赖于具体的数据集</strong>。Unless we employ some regularization scheme (early stopping or L1&#x2F;L2 penalties), a typical number of hidden units vs. generalization performance graph will be U-shaped.</p>
<h2 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h2><ul>
<li><a target="_blank" rel="noopener" href="http://deeplearning.net/tutorial/lenet.html">LeNet</a></li>
</ul>
<h3 id="The-Convolution-and-Pool-Operator"><a href="#The-Convolution-and-Pool-Operator" class="headerlink" title="The Convolution and Pool Operator"></a>The Convolution and Pool Operator</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> theano</span><br><span class="line"><span class="keyword">from</span> theano <span class="keyword">import</span> tensor <span class="keyword">as</span> T</span><br><span class="line"><span class="keyword">from</span> theano.tensor.nnet <span class="keyword">import</span> conv2d,sigmoid</span><br><span class="line"><span class="keyword">from</span> theano.tensor.signal.pool <span class="keyword">import</span> pool_2d</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt  </span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">rng = numpy.random.RandomState(<span class="number">23455</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># instantiate 4D tensor for input</span></span><br><span class="line"><span class="built_in">input</span> = T.tensor4(name=<span class="string">&#x27;input&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># initialize shared variable for weights.</span></span><br><span class="line">w_shp = (<span class="number">2</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">9</span>)</span><br><span class="line">w_bound = numpy.sqrt(<span class="number">3</span> * <span class="number">9</span> * <span class="number">9</span>)</span><br><span class="line">W = theano.shared( numpy.asarray(</span><br><span class="line">            rng.uniform(</span><br><span class="line">                low=-<span class="number">1.0</span> / w_bound,</span><br><span class="line">                high=<span class="number">1.0</span> / w_bound,</span><br><span class="line">                size=w_shp),</span><br><span class="line">            dtype=<span class="built_in">input</span>.dtype), name =<span class="string">&#x27;W&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># initialize shared variable for bias (1D tensor) with random values</span></span><br><span class="line"><span class="comment"># IMPORTANT: biases are usually initialized to zero. However in this</span></span><br><span class="line"><span class="comment"># particular application, we simply apply the convolutional layer to</span></span><br><span class="line"><span class="comment"># an image without learning the parameters. We therefore initialize</span></span><br><span class="line"><span class="comment"># them to random values to &quot;simulate&quot; learning.</span></span><br><span class="line">b_shp = (<span class="number">2</span>,)</span><br><span class="line">b = theano.shared(numpy.asarray(</span><br><span class="line">            rng.uniform(low=-<span class="number">.5</span>, high=<span class="number">.5</span>, size=b_shp),</span><br><span class="line">            dtype=<span class="built_in">input</span>.dtype), name =<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># build symbolic expression that computes the convolution of input with filters in w</span></span><br><span class="line">conv_out = conv2d(<span class="built_in">input</span>, W)</span><br><span class="line"></span><br><span class="line">poolsize=(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">pooled_out = pool_2d( <span class="built_in">input</span>=conv_out, ws=poolsize, ignore_border=<span class="literal">True</span>) </span><br><span class="line"></span><br><span class="line">conv_activations = sigmoid(conv_out + b.dimshuffle(<span class="string">&#x27;x&#x27;</span>, <span class="number">0</span>, <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;x&#x27;</span>))</span><br><span class="line"><span class="comment"># create theano function to compute filtered images</span></span><br><span class="line">f = theano.function([<span class="built_in">input</span>], conv_activations)</span><br><span class="line"></span><br><span class="line">pooled_activations = sigmoid(pooled_out + b.dimshuffle(<span class="string">&#x27;x&#x27;</span>, <span class="number">0</span>, <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;x&#x27;</span>))</span><br><span class="line">f2 = theano.function([<span class="built_in">input</span>], pooled_activations)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#===========================================================</span></span><br><span class="line"><span class="comment"># processing image file</span></span><br><span class="line"><span class="comment">#===========================================================</span></span><br><span class="line"><span class="comment"># open random image of dimensions 639x516</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="built_in">open</span>(<span class="string">&#x27;./3wolfmoon.jpg&#x27;</span>))</span><br><span class="line"><span class="comment"># dimensions are (height, width, channel)</span></span><br><span class="line">img = numpy.asarray(img, dtype=theano.config.floatX) / <span class="number">256.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># put image in 4D tensor of shape (1, 3, height, width)</span></span><br><span class="line">input_img_ = img.transpose(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>).reshape(<span class="number">1</span>, <span class="number">3</span>, <span class="number">639</span>, <span class="number">516</span>)</span><br><span class="line">filtered_img = f(input_img_)</span><br><span class="line">pooled_img = f2(input_img_)</span><br><span class="line"><span class="built_in">print</span> filtered_img.shape <span class="comment"># (1, 2, 631, 508) 2 feature maps</span></span><br><span class="line"><span class="built_in">print</span> pooled_img.shape   <span class="comment"># (1, 2, 315, 254) 2 feature maps</span></span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">16</span>,<span class="number">8</span>))</span><br><span class="line"><span class="comment"># (1)</span></span><br><span class="line"><span class="comment"># plot original image and first and second components of output</span></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>); plt.axis(<span class="string">&#x27;off&#x27;</span>); plt.imshow(img)</span><br><span class="line">plt.gray();</span><br><span class="line"><span class="comment"># recall that the convOp output (filtered image) is actually a &quot;minibatch&quot;,</span></span><br><span class="line"><span class="comment"># of size 1 here, so we take index 0 in the first dimension:</span></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>); plt.axis(<span class="string">&#x27;off&#x27;</span>); plt.imshow(filtered_img[<span class="number">0</span>, <span class="number">0</span>, :, :])</span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>); plt.axis(<span class="string">&#x27;off&#x27;</span>); plt.imshow(filtered_img[<span class="number">0</span>, <span class="number">1</span>, :, :])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># (2)</span></span><br><span class="line"><span class="comment"># plot original image and first and second components of output</span></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>); plt.axis(<span class="string">&#x27;off&#x27;</span>); plt.imshow(img)</span><br><span class="line">plt.gray();</span><br><span class="line"><span class="comment"># recall that the convOp output (filtered image) is actually a &quot;minibatch&quot;,</span></span><br><span class="line"><span class="comment"># of size 1 here, so we take index 0 in the first dimension:</span></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>); plt.axis(<span class="string">&#x27;off&#x27;</span>); plt.imshow(pooled_img[<span class="number">0</span>, <span class="number">0</span>, :, :])</span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">3</span>, <span class="number">6</span>); plt.axis(<span class="string">&#x27;off&#x27;</span>); plt.imshow(pooled_img[<span class="number">0</span>, <span class="number">1</span>, :, :])</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Notice that a randomly initialized filter acts very much like an edge detector!</span></span><br></pre></td></tr></table></figure>

<pre><code>WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:
 https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29

Using gpu device 0: GeForce GTX 1060 (CNMeM is enabled with initial size: 80.0% of memory, cuDNN 5105)


(1, 2, 631, 508)
(1, 2, 315, 254)
</code></pre>
<p><img src="https://kezunlin.me/images/posts/635233-20180807105128016-1414614810.png" alt="png"></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a target="_blank" rel="noopener" href="http://deeplearning.net/tutorial/gettingstarted.html#gettingstarted">deeplearning getting started</a></li>
<li><a target="_blank" rel="noopener" href="http://deeplearning.net/tutorial/mlp.html">mlp</a></li>
<li><a target="_blank" rel="noopener" href="http://deeplearning.net/tutorial/lenet.html">lenet</a></li>
</ul>
<h2 id="History"><a href="#History" class="headerlink" title="History"></a>History</h2><ul>
<li>20180807: created.</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kezunlin.me/blog/theano-conv-pool-example/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="kezunlin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kezunlin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/theano-conv-pool-example/" class="post-title-link" itemprop="url">theano conv pool example</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-08-07 10:47:00" itemprop="dateCreated datePublished" datetime="2018-08-07T10:47:00+08:00">2018-08-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-10-14 13:39:28" itemprop="dateModified" datetime="2024-10-14T13:39:28+08:00">2024-10-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Theano-conv-pool-example"><a href="#Theano-conv-pool-example" class="headerlink" title="Theano conv pool example"></a>Theano conv pool example</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> theano</span><br><span class="line"><span class="keyword">from</span> theano <span class="keyword">import</span> tensor <span class="keyword">as</span> T</span><br><span class="line"><span class="keyword">from</span> theano.tensor.nnet <span class="keyword">import</span> conv2d,sigmoid</span><br><span class="line"><span class="keyword">from</span> theano.tensor.signal.pool <span class="keyword">import</span> pool_2d</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt  </span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">rng = numpy.random.RandomState(<span class="number">23455</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># instantiate 4D tensor for input</span></span><br><span class="line"><span class="built_in">input</span> = T.tensor4(name=<span class="string">&#x27;input&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># initialize shared variable for weights.</span></span><br><span class="line">w_shp = (<span class="number">2</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">9</span>)</span><br><span class="line">w_bound = numpy.sqrt(<span class="number">3</span> * <span class="number">9</span> * <span class="number">9</span>)</span><br><span class="line">W = theano.shared( numpy.asarray(</span><br><span class="line">            rng.uniform(</span><br><span class="line">                low=-<span class="number">1.0</span> / w_bound,</span><br><span class="line">                high=<span class="number">1.0</span> / w_bound,</span><br><span class="line">                size=w_shp),</span><br><span class="line">            dtype=<span class="built_in">input</span>.dtype), name =<span class="string">&#x27;W&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># initialize shared variable for bias (1D tensor) with random values</span></span><br><span class="line"><span class="comment"># IMPORTANT: biases are usually initialized to zero. However in this</span></span><br><span class="line"><span class="comment"># particular application, we simply apply the convolutional layer to</span></span><br><span class="line"><span class="comment"># an image without learning the parameters. We therefore initialize</span></span><br><span class="line"><span class="comment"># them to random values to &quot;simulate&quot; learning.</span></span><br><span class="line">b_shp = (<span class="number">2</span>,)</span><br><span class="line">b = theano.shared(numpy.asarray(</span><br><span class="line">            rng.uniform(low=-<span class="number">.5</span>, high=<span class="number">.5</span>, size=b_shp),</span><br><span class="line">            dtype=<span class="built_in">input</span>.dtype), name =<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># build symbolic expression that computes the convolution of input with filters in w</span></span><br><span class="line">conv_out = conv2d(<span class="built_in">input</span>, W)</span><br><span class="line"></span><br><span class="line">poolsize=(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">pooled_out = pool_2d( <span class="built_in">input</span>=conv_out, ws=poolsize, ignore_border=<span class="literal">True</span>) </span><br><span class="line"></span><br><span class="line">conv_activations = sigmoid(conv_out + b.dimshuffle(<span class="string">&#x27;x&#x27;</span>, <span class="number">0</span>, <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;x&#x27;</span>))</span><br><span class="line"><span class="comment"># create theano function to compute filtered images</span></span><br><span class="line">f = theano.function([<span class="built_in">input</span>], conv_activations)</span><br><span class="line"></span><br><span class="line">pooled_activations = sigmoid(pooled_out + b.dimshuffle(<span class="string">&#x27;x&#x27;</span>, <span class="number">0</span>, <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;x&#x27;</span>))</span><br><span class="line">f2 = theano.function([<span class="built_in">input</span>], pooled_activations)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#===========================================================</span></span><br><span class="line"><span class="comment"># processing image file</span></span><br><span class="line"><span class="comment">#===========================================================</span></span><br><span class="line"><span class="comment"># open random image of dimensions 639x516</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="built_in">open</span>(<span class="string">&#x27;./3wolfmoon.jpg&#x27;</span>))</span><br><span class="line"><span class="comment"># dimensions are (height, width, channel)</span></span><br><span class="line">img = numpy.asarray(img, dtype=theano.config.floatX) / <span class="number">256.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># put image in 4D tensor of shape (1, 3, height, width)</span></span><br><span class="line">input_img_ = img.transpose(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>).reshape(<span class="number">1</span>, <span class="number">3</span>, <span class="number">639</span>, <span class="number">516</span>)</span><br><span class="line">filtered_img = f(input_img_)</span><br><span class="line">pooled_img = f2(input_img_)</span><br><span class="line"><span class="built_in">print</span> filtered_img.shape <span class="comment"># (1, 2, 631, 508) 2 feature maps</span></span><br><span class="line"><span class="built_in">print</span> pooled_img.shape   <span class="comment"># (1, 2, 315, 254) 2 feature maps</span></span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">16</span>,<span class="number">8</span>))</span><br><span class="line"><span class="comment"># (1)</span></span><br><span class="line"><span class="comment"># plot original image and first and second components of output</span></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>); plt.axis(<span class="string">&#x27;off&#x27;</span>); plt.imshow(img)</span><br><span class="line">plt.gray();</span><br><span class="line"><span class="comment"># recall that the convOp output (filtered image) is actually a &quot;minibatch&quot;,</span></span><br><span class="line"><span class="comment"># of size 1 here, so we take index 0 in the first dimension:</span></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>); plt.axis(<span class="string">&#x27;off&#x27;</span>); plt.imshow(filtered_img[<span class="number">0</span>, <span class="number">0</span>, :, :])</span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>); plt.axis(<span class="string">&#x27;off&#x27;</span>); plt.imshow(filtered_img[<span class="number">0</span>, <span class="number">1</span>, :, :])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># (2)</span></span><br><span class="line"><span class="comment"># plot original image and first and second components of output</span></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>); plt.axis(<span class="string">&#x27;off&#x27;</span>); plt.imshow(img)</span><br><span class="line">plt.gray();</span><br><span class="line"><span class="comment"># recall that the convOp output (filtered image) is actually a &quot;minibatch&quot;,</span></span><br><span class="line"><span class="comment"># of size 1 here, so we take index 0 in the first dimension:</span></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>); plt.axis(<span class="string">&#x27;off&#x27;</span>); plt.imshow(pooled_img[<span class="number">0</span>, <span class="number">0</span>, :, :])</span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">3</span>, <span class="number">6</span>); plt.axis(<span class="string">&#x27;off&#x27;</span>); plt.imshow(pooled_img[<span class="number">0</span>, <span class="number">1</span>, :, :])</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Notice that a randomly initialized filter acts very much like an edge detector!</span></span><br></pre></td></tr></table></figure>

<pre><code>(1, 2, 631, 508)
(1, 2, 315, 254)
</code></pre>
<p><img src="https://kezunlin.me/images/posts/635233-20180807105128016-1414614810.png" alt="png"></p>
<h2 id="Pool-example"><a href="#Pool-example" class="headerlink" title="Pool example"></a>Pool example</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> theano.tensor.signal <span class="keyword">import</span> pool</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = T.dtensor4(<span class="string">&#x27;input&#x27;</span>)</span><br><span class="line">maxpool_shape = (<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">pool_out = pool.pool_2d(<span class="built_in">input</span>, maxpool_shape, ignore_border=<span class="literal">True</span>)</span><br><span class="line">f = theano.function([<span class="built_in">input</span>],pool_out)</span><br><span class="line"></span><br><span class="line"><span class="comment">#invals = numpy.random.RandomState(1).rand(3, 2, 5, 5)</span></span><br><span class="line">invals = np.arange(<span class="number">50</span>).reshape(<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span> <span class="string">&#x27;With ignore_border set to True:&#x27;</span></span><br><span class="line"><span class="built_in">print</span> <span class="string">&#x27;invals[0, 0, :, :] =\n&#x27;</span>, invals[<span class="number">0</span>, <span class="number">0</span>, :, :]</span><br><span class="line"><span class="built_in">print</span> <span class="string">&#x27;output[0, 0, :, :] =\n&#x27;</span>, f(invals)[<span class="number">0</span>, <span class="number">0</span>, :, :]</span><br><span class="line"></span><br><span class="line">pool_out = pool.pool_2d(<span class="built_in">input</span>, maxpool_shape, ignore_border=<span class="literal">False</span>)</span><br><span class="line">f = theano.function([<span class="built_in">input</span>],pool_out)</span><br><span class="line"><span class="built_in">print</span> </span><br><span class="line"><span class="built_in">print</span> <span class="string">&#x27;With ignore_border set to False:&#x27;</span></span><br><span class="line"><span class="built_in">print</span> <span class="string">&#x27;invals[1, 0, :, :] =\n &#x27;</span>, invals[<span class="number">0</span>, <span class="number">0</span>, :, :]</span><br><span class="line"><span class="built_in">print</span> <span class="string">&#x27;output[1, 0, :, :] =\n &#x27;</span>, f(invals)[<span class="number">0</span>, <span class="number">0</span>, :, :]</span><br></pre></td></tr></table></figure>

<pre><code>With ignore_border set to True:
invals[0, 0, :, :] =
[[ 0  1  2  3  4]
 [ 5  6  7  8  9]
 [10 11 12 13 14]
 [15 16 17 18 19]
 [20 21 22 23 24]]
output[0, 0, :, :] =
[[  6.   8.]
 [ 16.  18.]]

With ignore_border set to False:
invals[1, 0, :, :] =
  [[ 0  1  2  3  4]
 [ 5  6  7  8  9]
 [10 11 12 13 14]
 [15 16 17 18 19]
 [20 21 22 23 24]]
output[1, 0, :, :] =
  [[  6.   8.   9.]
 [ 16.  18.  19.]
 [ 21.  23.  24.]]
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a target="_blank" rel="noopener" href="http://deeplearning.net/software/theano/library/tensor/nnet/conv.html">theano conv</a></li>
</ul>
<h2 id="History"><a href="#History" class="headerlink" title="History"></a>History</h2><ul>
<li>20180807: created.</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kezunlin.me/blog/network3-py/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="kezunlin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kezunlin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/network3-py/" class="post-title-link" itemprop="url">network3.py</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-08-07 10:36:00" itemprop="dateCreated datePublished" datetime="2018-08-07T10:36:00+08:00">2018-08-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-10-14 13:39:28" itemprop="dateModified" datetime="2024-10-14T13:39:28+08:00">2024-10-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="network3-py"><a href="#network3-py" class="headerlink" title="network3.py"></a>network3.py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br><span class="line">715</span><br><span class="line">716</span><br><span class="line">717</span><br><span class="line">718</span><br><span class="line">719</span><br><span class="line">720</span><br><span class="line">721</span><br><span class="line">722</span><br><span class="line">723</span><br><span class="line">724</span><br><span class="line">725</span><br><span class="line">726</span><br><span class="line">727</span><br><span class="line">728</span><br><span class="line">729</span><br><span class="line">730</span><br><span class="line">731</span><br><span class="line">732</span><br><span class="line">733</span><br><span class="line">734</span><br><span class="line">735</span><br><span class="line">736</span><br><span class="line">737</span><br><span class="line">738</span><br><span class="line">739</span><br><span class="line">740</span><br><span class="line">741</span><br><span class="line">742</span><br><span class="line">743</span><br><span class="line">744</span><br><span class="line">745</span><br><span class="line">746</span><br><span class="line">747</span><br><span class="line">748</span><br><span class="line">749</span><br><span class="line">750</span><br><span class="line">751</span><br><span class="line">752</span><br><span class="line">753</span><br><span class="line">754</span><br><span class="line">755</span><br><span class="line">756</span><br><span class="line">757</span><br><span class="line">758</span><br><span class="line">759</span><br><span class="line">760</span><br><span class="line">761</span><br><span class="line">762</span><br><span class="line">763</span><br><span class="line">764</span><br><span class="line">765</span><br><span class="line">766</span><br><span class="line">767</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;network3.py</span></span><br><span class="line"><span class="string">~~~~~~~~~~~~~~</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">A Theano-based program for training and running simple neural</span></span><br><span class="line"><span class="string">networks.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Supports several layer types (fully connected, convolutional, max</span></span><br><span class="line"><span class="string">pooling, softmax), and activation functions (sigmoid, tanh, and</span></span><br><span class="line"><span class="string">rectified linear units, with more easily added).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">When run on a CPU, this program is much faster than network.py and</span></span><br><span class="line"><span class="string">network2.py.  However, unlike network.py and network2.py it can also</span></span><br><span class="line"><span class="string">be run on a GPU, which makes it faster still.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Because the code is based on Theano, the code is different in many</span></span><br><span class="line"><span class="string">ways from network.py and network2.py.  However, where possible I have</span></span><br><span class="line"><span class="string">tried to maintain consistency with the earlier programs.  In</span></span><br><span class="line"><span class="string">particular, the API is similar to network2.py.  Note that I have</span></span><br><span class="line"><span class="string">focused on making the code simple, easily readable, and easily</span></span><br><span class="line"><span class="string">modifiable.  It is not optimized, and omits many desirable features.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">This program incorporates ideas from the Theano documentation on</span></span><br><span class="line"><span class="string">convolutional neural nets (notably,</span></span><br><span class="line"><span class="string">http://deeplearning.net/tutorial/lenet.html ), from Misha Denil&#x27;s</span></span><br><span class="line"><span class="string">implementation of dropout (https://github.com/mdenil/dropout ), and</span></span><br><span class="line"><span class="string">from Chris Olah (http://colah.github.io ).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Written for Theano 0.6 and 0.7, needs some changes for more recent</span></span><br><span class="line"><span class="string">versions of Theano.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">对于N=50000数据全部参与训练，time(python) = 7分钟; time(theano) = 1分钟。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">But the big win is the ability to do fast symbolic differentiation, </span></span><br><span class="line"><span class="string">using a very general form of the backpropagation algorithm. </span></span><br><span class="line"><span class="string">This is extremely useful for applying stochastic gradient</span></span><br><span class="line"><span class="string">descent to a wide variety of network architectures.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#### Libraries</span></span><br><span class="line"><span class="comment"># Standard library</span></span><br><span class="line"><span class="keyword">import</span> cPickle</span><br><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"></span><br><span class="line"><span class="comment"># Third-party libraries</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> theano</span><br><span class="line"><span class="keyword">import</span> theano.tensor <span class="keyword">as</span> T</span><br><span class="line"><span class="keyword">from</span> theano.tensor.nnet <span class="keyword">import</span> conv</span><br><span class="line"><span class="keyword">from</span> theano.tensor.nnet <span class="keyword">import</span> softmax</span><br><span class="line"><span class="keyword">from</span> theano.tensor <span class="keyword">import</span> shared_randomstreams</span><br><span class="line"><span class="comment">#from theano.tensor.signal.downsample import max_pool_2d  # for version theano-0.7</span></span><br><span class="line"><span class="keyword">from</span> theano.tensor.signal.pool <span class="keyword">import</span> pool_2d <span class="comment"># for version theano-0.9</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Activation functions for neurons</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">linear</span>(<span class="params">z</span>): <span class="keyword">return</span> z</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ReLU</span>(<span class="params">z</span>): <span class="keyword">return</span> T.maximum(<span class="number">0.0</span>, z)</span><br><span class="line"><span class="keyword">from</span> theano.tensor.nnet <span class="keyword">import</span> sigmoid</span><br><span class="line"><span class="keyword">from</span> theano.tensor <span class="keyword">import</span> tanh</span><br><span class="line"></span><br><span class="line"><span class="comment">#### Load the MNIST data</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_shared</span>(<span class="params">filename=<span class="string">&quot;../data/mnist.pkl.gz&quot;</span>,training_set_size=<span class="number">1000</span></span>):</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&#x27;loading data from &#123;0&#125; of #&#123;1&#125;&#x27;</span>.<span class="built_in">format</span>(filename,training_set_size)</span><br><span class="line">    f = gzip.<span class="built_in">open</span>(filename, <span class="string">&#x27;rb&#x27;</span>)</span><br><span class="line">    training_data, validation_data, test_data = cPickle.load(f) <span class="comment"># float32(N,784); int64(N,)</span></span><br><span class="line">    f.close()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">shared</span>(<span class="params">data</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Place the data into shared variables.  This allows Theano to copy</span></span><br><span class="line"><span class="string">        the data to the GPU, if one is available.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        shared_x.get_value().shape   float32(50000, 784)</span></span><br><span class="line"><span class="string">        shared_y.get_value().shape   float32(50000,)   </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        y_cast = T.cast(shared_y, &quot;int8&quot;) # float32---&gt;int8</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        shared_x.type  TensorType(float32, matrix)  theano.tensor.sharedvar.TensorSharedVariable</span></span><br><span class="line"><span class="string">        shared_y.type  TensorType(float32, vector)  theano.tensor.sharedvar.TensorSharedVariable</span></span><br><span class="line"><span class="string">        y_cast.type    TensorType(int32, vector)    theano.tensor.var.TensorVariable  (y_cast不是shared变量)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 默认floatX = float64,在运行的时候需要设置floatX = float32</span></span><br><span class="line">        <span class="comment"># 取x[N,784],y[N]的前training_set_size个样本参与训练</span></span><br><span class="line">        shared_x = theano.shared(np.asarray(data[<span class="number">0</span>][:training_set_size,],dtype=theano.config.floatX), borrow=<span class="literal">True</span>)  </span><br><span class="line">        shared_y = theano.shared(np.asarray(data[<span class="number">1</span>][:training_set_size], dtype=theano.config.floatX), borrow=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># shared变量中的数据在GPU上必须是float32类型，但是计算阶段可能需要int类型(y)，所以需要将float32---&gt;int.</span></span><br><span class="line">        <span class="comment"># 并且int8类型需要和　self.y = T.bvector(&quot;y&quot;)的b类型一样。</span></span><br><span class="line">        <span class="comment"># When storing data on the GPU it has to be stored as floats</span></span><br><span class="line">        <span class="comment"># therefore we will store the labels as ``floatX`` as well</span></span><br><span class="line">        <span class="comment"># (``shared_y`` does exactly that). But during our computations</span></span><br><span class="line">        <span class="comment"># we need them as ints (we use labels as index, and if they are</span></span><br><span class="line">        <span class="comment"># floats it doesn&#x27;t make sense) therefore instead of returning</span></span><br><span class="line">        <span class="comment"># ``shared_y`` we will have to cast it to int. This little hack</span></span><br><span class="line">        <span class="comment"># lets us get around this issue</span></span><br><span class="line">        <span class="keyword">return</span> shared_x, T.cast(shared_y, <span class="string">&#x27;int8&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> [shared(training_data), shared(validation_data), shared(test_data)]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_expanded</span>(<span class="params">filename=<span class="string">&quot;../data/mnist_expanded.pkl.gz&quot;</span>,training_set_size=<span class="number">1000</span></span>):</span><br><span class="line">    <span class="keyword">return</span> load_data_shared(filename=filename,training_set_size=training_set_size)</span><br><span class="line"></span><br><span class="line"><span class="comment">#### Main class used to construct and train networks</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Network</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layers, mini_batch_size</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Takes a list of `layers`, describing the network architecture, and</span></span><br><span class="line"><span class="string">        a value for the `mini_batch_size` to be used during training</span></span><br><span class="line"><span class="string">        by stochastic gradient descent.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.layers = layers</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.layers)&gt;=<span class="number">2</span></span><br><span class="line">        <span class="variable language_">self</span>.mini_batch_size = mini_batch_size</span><br><span class="line">        <span class="variable language_">self</span>.params = [param <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="variable language_">self</span>.layers <span class="keyword">for</span> param <span class="keyword">in</span> layer.params]</span><br><span class="line">        <span class="variable language_">self</span>.x = T.matrix(<span class="string">&quot;x&quot;</span>)  <span class="comment"># batch x  float32,(m,784) 不需要指定fmatrix</span></span><br><span class="line">        <span class="variable language_">self</span>.y = T.bvector(<span class="string">&quot;y&quot;</span>)  <span class="comment"># batch y   int8,(m,)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># first layer init with inpt=x,inpt_dropout=x</span></span><br><span class="line">        init_layer = <span class="variable language_">self</span>.layers[<span class="number">0</span>]</span><br><span class="line">        init_layer.set_inpt(<span class="variable language_">self</span>.x, <span class="variable language_">self</span>.x, <span class="variable language_">self</span>.mini_batch_size)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> xrange(<span class="number">1</span>, <span class="built_in">len</span>(<span class="variable language_">self</span>.layers)):</span><br><span class="line">            prev_layer, layer  = <span class="variable language_">self</span>.layers[j-<span class="number">1</span>], <span class="variable language_">self</span>.layers[j]</span><br><span class="line">            layer.set_inpt(prev_layer.output, prev_layer.output_dropout, <span class="variable language_">self</span>.mini_batch_size)</span><br><span class="line">            </span><br><span class="line">        <span class="variable language_">self</span>.output = <span class="variable language_">self</span>.layers[-<span class="number">1</span>].output</span><br><span class="line">        <span class="variable language_">self</span>.output_dropout = <span class="variable language_">self</span>.layers[-<span class="number">1</span>].output_dropout</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">SGD</span>(<span class="params">self, training_data, epochs, mini_batch_size, eta,</span></span><br><span class="line"><span class="params">            validation_data, test_data, lmbda=<span class="number">0.0</span>,</span></span><br><span class="line"><span class="params">            no_improvement_in_n=<span class="number">20</span>,use_constant_eta=<span class="literal">True</span>, <span class="comment"># default not vary eta because accuracy not imporved too much</span></span></span><br><span class="line"><span class="params">            eta_shrink_times=<span class="number">10</span>,eta_descrease_factor = <span class="number">0.0001</span></span>):</span><br><span class="line">        </span><br><span class="line">        <span class="string">&quot;&quot;&quot;Train the network using mini-batch stochastic gradient descent.&quot;&quot;&quot;</span></span><br><span class="line">        training_x, training_y = training_data       <span class="comment"># (N,784) (N,)</span></span><br><span class="line">        validation_x, validation_y = validation_data</span><br><span class="line">        test_x, test_y = test_data</span><br><span class="line"></span><br><span class="line">        <span class="comment"># compute number of minibatches for training, validation and testing</span></span><br><span class="line">        num_training_batches = size(training_data)/mini_batch_size</span><br><span class="line">        num_validation_batches = size(validation_data)/mini_batch_size</span><br><span class="line">        num_test_batches = size(test_data)/mini_batch_size</span><br><span class="line"></span><br><span class="line">        <span class="comment"># define the (regularized) cost function, symbolic gradients, and updates</span></span><br><span class="line">        l2_norm_squared = <span class="built_in">sum</span>([(layer.w**<span class="number">2</span>).<span class="built_in">sum</span>() <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="variable language_">self</span>.layers])</span><br><span class="line">        cost0 = <span class="variable language_">self</span>.layers[-<span class="number">1</span>].cost(<span class="variable language_">self</span>) <span class="comment"># 计算最后一层的输出代价，传递Network作为net参数</span></span><br><span class="line">        </span><br><span class="line">        cost = cost0 + <span class="number">0.5</span>*lmbda*l2_norm_squared/size(training_data)  <span class="comment"># ??? N instead of num_training_batches</span></span><br><span class="line">        grads = T.grad(cost, <span class="variable language_">self</span>.params)</span><br><span class="line">        </span><br><span class="line">        shared_eta = theano.shared(eta,borrow=<span class="literal">True</span>) <span class="comment">#(same as shared_b) use SharedVariable instead of value</span></span><br><span class="line">        </span><br><span class="line">        updates = [(param, param-T.cast(shared_eta*grad,dtype=theano.config.floatX)) <span class="keyword">for</span> param, grad <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="variable language_">self</span>.params, grads)] </span><br><span class="line">        </span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        grad(float32),没有指定floatX=float32,则eta*grad(float64),指定之后eta*grad(float32)，无需cast</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        #for param, grad in zip(self.params, grads):</span></span><br><span class="line"><span class="string">        #    print param.type,grad.type,(eta*grad).type</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        # updates = [(param, T.cast(param-eta*grad,&#x27;float32&#x27;) ) for param, grad in zip(self.params, grads)]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># define functions to train a mini-batch, and to compute the</span></span><br><span class="line">        <span class="comment"># accuracy in validation and test mini-batches.</span></span><br><span class="line">        i = T.lscalar() <span class="comment"># mini-batch index</span></span><br><span class="line">        train_mb = theano.function(</span><br><span class="line">            [i], cost, updates=updates, <span class="comment"># 给定i,===&gt;x,y===&gt;cost中的x,y被替换掉，从而计算mini-batch的代价，最后updates</span></span><br><span class="line">            givens=&#123;</span><br><span class="line">                <span class="variable language_">self</span>.x:</span><br><span class="line">                training_x[i*<span class="variable language_">self</span>.mini_batch_size: (i+<span class="number">1</span>)*<span class="variable language_">self</span>.mini_batch_size],</span><br><span class="line">                <span class="variable language_">self</span>.y:</span><br><span class="line">                training_y[i*<span class="variable language_">self</span>.mini_batch_size: (i+<span class="number">1</span>)*<span class="variable language_">self</span>.mini_batch_size]</span><br><span class="line">            &#125;)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># cost and accuracy for train,val,test</span></span><br><span class="line">        <span class="comment"># (1) train</span></span><br><span class="line">        train_mb_cost = theano.function(</span><br><span class="line">            [i], cost,</span><br><span class="line">            givens=&#123;</span><br><span class="line">                <span class="variable language_">self</span>.x:</span><br><span class="line">                training_x[i*<span class="variable language_">self</span>.mini_batch_size: (i+<span class="number">1</span>)*<span class="variable language_">self</span>.mini_batch_size],</span><br><span class="line">                <span class="variable language_">self</span>.y:</span><br><span class="line">                training_y[i*<span class="variable language_">self</span>.mini_batch_size: (i+<span class="number">1</span>)*<span class="variable language_">self</span>.mini_batch_size]</span><br><span class="line">            &#125;) </span><br><span class="line">        train_mb_accuracy = theano.function(</span><br><span class="line">            [i], <span class="variable language_">self</span>.layers[-<span class="number">1</span>].accuracy(<span class="variable language_">self</span>.y), <span class="comment"># y(m,)</span></span><br><span class="line">            givens=&#123;</span><br><span class="line">                <span class="variable language_">self</span>.x:</span><br><span class="line">                training_x[i*<span class="variable language_">self</span>.mini_batch_size: (i+<span class="number">1</span>)*<span class="variable language_">self</span>.mini_batch_size],</span><br><span class="line">                <span class="variable language_">self</span>.y:</span><br><span class="line">                training_y[i*<span class="variable language_">self</span>.mini_batch_size: (i+<span class="number">1</span>)*<span class="variable language_">self</span>.mini_batch_size]</span><br><span class="line">            &#125;) </span><br><span class="line">        <span class="comment"># (2) val</span></span><br><span class="line">        validate_mb_cost = theano.function(</span><br><span class="line">            [i], cost,</span><br><span class="line">            givens=&#123;</span><br><span class="line">                <span class="variable language_">self</span>.x:</span><br><span class="line">                validation_x[i*<span class="variable language_">self</span>.mini_batch_size: (i+<span class="number">1</span>)*<span class="variable language_">self</span>.mini_batch_size],</span><br><span class="line">                <span class="variable language_">self</span>.y:</span><br><span class="line">                validation_y[i*<span class="variable language_">self</span>.mini_batch_size: (i+<span class="number">1</span>)*<span class="variable language_">self</span>.mini_batch_size]</span><br><span class="line">            &#125;)</span><br><span class="line">        validate_mb_accuracy = theano.function(</span><br><span class="line">            [i], <span class="variable language_">self</span>.layers[-<span class="number">1</span>].accuracy(<span class="variable language_">self</span>.y), <span class="comment"># y(m,)</span></span><br><span class="line">            givens=&#123;</span><br><span class="line">                <span class="variable language_">self</span>.x:</span><br><span class="line">                validation_x[i*<span class="variable language_">self</span>.mini_batch_size: (i+<span class="number">1</span>)*<span class="variable language_">self</span>.mini_batch_size],</span><br><span class="line">                <span class="variable language_">self</span>.y:</span><br><span class="line">                validation_y[i*<span class="variable language_">self</span>.mini_batch_size: (i+<span class="number">1</span>)*<span class="variable language_">self</span>.mini_batch_size]</span><br><span class="line">            &#125;)</span><br><span class="line">        <span class="comment"># (3) test</span></span><br><span class="line">        test_mb_cost = theano.function(</span><br><span class="line">            [i], cost,</span><br><span class="line">            givens=&#123;</span><br><span class="line">                <span class="variable language_">self</span>.x:</span><br><span class="line">                test_x[i*<span class="variable language_">self</span>.mini_batch_size: (i+<span class="number">1</span>)*<span class="variable language_">self</span>.mini_batch_size],</span><br><span class="line">                <span class="variable language_">self</span>.y:</span><br><span class="line">                test_y[i*<span class="variable language_">self</span>.mini_batch_size: (i+<span class="number">1</span>)*<span class="variable language_">self</span>.mini_batch_size]</span><br><span class="line">            &#125;)</span><br><span class="line">        test_mb_accuracy = theano.function(</span><br><span class="line">            [i], <span class="variable language_">self</span>.layers[-<span class="number">1</span>].accuracy(<span class="variable language_">self</span>.y), <span class="comment"># y(m,)</span></span><br><span class="line">            givens=&#123;</span><br><span class="line">                <span class="variable language_">self</span>.x:</span><br><span class="line">                test_x[i*<span class="variable language_">self</span>.mini_batch_size: (i+<span class="number">1</span>)*<span class="variable language_">self</span>.mini_batch_size],</span><br><span class="line">                <span class="variable language_">self</span>.y:</span><br><span class="line">                test_y[i*<span class="variable language_">self</span>.mini_batch_size: (i+<span class="number">1</span>)*<span class="variable language_">self</span>.mini_batch_size]</span><br><span class="line">            &#125;)</span><br><span class="line">        <span class="variable language_">self</span>.test_mb_predictions = theano.function(</span><br><span class="line">            [i], <span class="variable language_">self</span>.layers[-<span class="number">1</span>].y_out,   <span class="comment"># y(m,)　m个样本的预测结果</span></span><br><span class="line">            givens=&#123;</span><br><span class="line">                <span class="variable language_">self</span>.x:</span><br><span class="line">                test_x[i*<span class="variable language_">self</span>.mini_batch_size: (i+<span class="number">1</span>)*<span class="variable language_">self</span>.mini_batch_size]</span><br><span class="line">            &#125;)</span><br><span class="line">        </span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">         def shuffle_data(x,y):</span></span><br><span class="line"><span class="string">            seed = int(time.time()) </span></span><br><span class="line"><span class="string">            np.random.seed(seed)</span></span><br><span class="line"><span class="string">            np.random.shuffle(x)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            np.random.seed(seed)</span></span><br><span class="line"><span class="string">            np.random.shuffle(y)</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        def shuffle_training_data(training_x,training_y):</span></span><br><span class="line"><span class="string">            # CPU, OK; GPU, FAILED (在GPU中borrow失效)</span></span><br><span class="line"><span class="string">            originX = training_x.get_value(borrow=True) # shared---&gt; nparray</span></span><br><span class="line"><span class="string">            originY = training_y.get_value(borrow=True) # shared---&gt; nparray</span></span><br><span class="line"><span class="string">            shuffle_data(originX,originY)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        evaluation_costs, evaluation_accuracys = [], []</span><br><span class="line">        training_costs, training_accuracys = [], []</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># use no-improvement-in-n early stopping</span></span><br><span class="line">        <span class="comment"># 记录best_validation_accuracy,best_epoch，如果epoch-best_epoch&gt;=no_improvement_in_n,stop</span></span><br><span class="line">        best_epoch = <span class="number">0</span></span><br><span class="line">        cur_eta_shrink_times = <span class="number">0</span> <span class="comment"># if cur_eta_shrink_times&gt;=eta_shrink_times,stop</span></span><br><span class="line">        best_validation_accuracy = <span class="number">0.0</span> <span class="comment"># with gpu, numpy.float64</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> xrange(epochs):</span><br><span class="line">            <span class="comment">#random.shuffle(training_data) # for list[(x1,y1),(x2,y2),...] 此处training_data是(X,Y)</span></span><br><span class="line">            <span class="comment"># shuffle_training_data(training_x,training_y) # FAILED on GPU</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> minibatch_index <span class="keyword">in</span> xrange(num_training_batches):</span><br><span class="line">                <span class="comment"># iteration记录训练次数，每训练1000次输出一次</span></span><br><span class="line">                iteration = num_training_batches*epoch+minibatch_index</span><br><span class="line">                <span class="keyword">if</span> iteration % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&quot;Training mini-batch number &#123;0&#125;&quot;</span>.<span class="built_in">format</span>(iteration))</span><br><span class="line">                cost_ij = train_mb(minibatch_index)</span><br><span class="line">           </span><br><span class="line">            <span class="comment"># 一个epoch训练结束，训练了num_training_batches次，iterration=4999。利用w,b计算一次验证accuracy</span></span><br><span class="line">            <span class="comment">#if (iteration+1) % num_training_batches == 0:</span></span><br><span class="line">            validation_cost = np.mean( [validate_mb_cost(j) <span class="keyword">for</span> j <span class="keyword">in</span> xrange(num_validation_batches)] )</span><br><span class="line">            validation_accuracy = np.mean( [validate_mb_accuracy(j) <span class="keyword">for</span> j <span class="keyword">in</span> xrange(num_validation_batches)] )</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;\nEpoch &#123;0&#125;: validation accuracy &#123;1:.2%&#125;&quot;</span>.<span class="built_in">format</span>(epoch, validation_accuracy))</span><br><span class="line"></span><br><span class="line">            train_cost = np.mean( [train_mb_cost(j) <span class="keyword">for</span> j <span class="keyword">in</span> xrange(num_training_batches)] )</span><br><span class="line">            train_accuracy = np.mean( [train_mb_accuracy(j) <span class="keyword">for</span> j <span class="keyword">in</span> xrange(num_training_batches)] )</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># save 4 return lists</span></span><br><span class="line">            evaluation_costs.append(validation_cost)</span><br><span class="line">            evaluation_accuracys.append(validation_accuracy)</span><br><span class="line">            training_costs.append(train_cost)</span><br><span class="line">            training_accuracys.append(train_accuracy)</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#记录best_validation_accuracy</span></span><br><span class="line">            <span class="comment"># 关键在于&lt;,满足足够多的NIIN,才能满足eta_shrink_times&gt;=10</span></span><br><span class="line">            <span class="keyword">if</span> best_validation_accuracy - validation_accuracy &lt; <span class="number">0.0</span>:  <span class="comment"># &lt;=</span></span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;This is the best validation accuracy to date.&quot;</span>)</span><br><span class="line">                best_validation_accuracy = validation_accuracy</span><br><span class="line">                best_epoch = epoch</span><br><span class="line">                best_iteration = iteration</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># save best network</span></span><br><span class="line">                best_net = copy.deepcopy(<span class="variable language_">self</span>)</span><br><span class="line">                </span><br><span class="line">                <span class="comment">#计算在val取得最佳accuracy情况下，test数据集的accuracy</span></span><br><span class="line">                <span class="keyword">if</span> test_data:</span><br><span class="line">                    test_accuracy = np.mean( [test_mb_accuracy(j) <span class="keyword">for</span> j <span class="keyword">in</span> xrange(num_test_batches)] )</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&#x27;The corresponding test accuracy is &#123;0:.2%&#125;&#x27;</span>.<span class="built_in">format</span>(test_accuracy))</span><br><span class="line">                    </span><br><span class="line">            <span class="comment">#============================================================================================</span></span><br><span class="line">            <span class="comment"># early stopping with variable learning rate</span></span><br><span class="line">            <span class="comment"># (1) (epoch - best_epoch) &gt;= no_improvement_in_n: stop   NIIN = 20</span></span><br><span class="line">            <span class="comment"># (2) new_eta = 1/2*eta until new_eta&lt;=1/1024*eta         ETA_SHRINK_TIME = 10 </span></span><br><span class="line">            <span class="comment">#============================================================================================</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># check in last epoch of NIIN stage</span></span><br><span class="line">            <span class="keyword">if</span> (epoch+<span class="number">1</span>) % no_improvement_in_n == <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># (1) check NIIN</span></span><br><span class="line">                <span class="keyword">if</span> (epoch - best_epoch) &gt;= no_improvement_in_n:</span><br><span class="line">                    <span class="comment"># stop learning</span></span><br><span class="line">                    <span class="built_in">print</span> <span class="string">&#x27;!&#x27;</span>*<span class="number">100</span></span><br><span class="line">                    <span class="built_in">print</span> <span class="string">&#x27;[HIT] Early stopping at epoch #&#123;0&#125;,best_epoch #&#123;1&#125;,iteration #&#123;2&#125;,validation accuracy &#123;3:.2%&#125;&#x27;</span>.<span class="built_in">format</span>(epoch,best_epoch,best_iteration,best_validation_accuracy)</span><br><span class="line">                    <span class="built_in">print</span> <span class="string">&#x27;!&#x27;</span>*<span class="number">100</span></span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment">#******************************************************************************</span></span><br><span class="line">                    <span class="keyword">if</span> use_constant_eta:</span><br><span class="line">                        <span class="keyword">break</span> <span class="comment"># goto (2) instead of break</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        <span class="comment"># (2) shrink eta to 1/2*eta　　(accuracy not improved too much)</span></span><br><span class="line">                        <span class="built_in">print</span> <span class="string">&#x27;cur_eta_shrink_times = &#123;0&#125;&#x27;</span>.<span class="built_in">format</span>(cur_eta_shrink_times)</span><br><span class="line">                        <span class="keyword">if</span> cur_eta_shrink_times &gt;= eta_shrink_times:</span><br><span class="line">                            <span class="built_in">print</span> <span class="string">&#x27;+&#x27;</span>*<span class="number">100</span></span><br><span class="line">                            <span class="built_in">print</span> <span class="string">&#x27;[HIT] Eta shrink OK. at epoch #&#123;0&#125;,best_epoch #&#123;1&#125;,iteration #&#123;2&#125;,validation accuracy &#123;3:.2%&#125;&#x27;</span>.<span class="built_in">format</span>(epoch,best_epoch,best_iteration,best_validation_accuracy)</span><br><span class="line">                            <span class="built_in">print</span> <span class="string">&#x27;+&#x27;</span>*<span class="number">100</span></span><br><span class="line">                            <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">                        cur_eta_shrink_times +=<span class="number">1</span> </span><br><span class="line"></span><br><span class="line">                        <span class="comment"># update eta every epoch</span></span><br><span class="line">                        eta_descrease_factor = <span class="number">0.0001</span></span><br><span class="line">                        new_eta = eta/(<span class="number">1.0</span>+eta_descrease_factor*(epoch+<span class="number">1</span>))</span><br><span class="line">                        shared_eta.set_value(np.asarray(new_eta,dtype=theano.config.floatX),borrow=<span class="literal">True</span>) <span class="comment"># update eta</span></span><br><span class="line"></span><br><span class="line">                        <span class="comment">#eta = eta/2.0 </span></span><br><span class="line">                        <span class="comment">#shared_eta.set_value(np.asarray(eta,dtype=theano.config.floatX),borrow=True) # update eta</span></span><br><span class="line">                    <span class="comment">#******************************************************************************</span></span><br><span class="line">            <span class="comment">#============================================================================================</span></span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">        <span class="comment"># once early stopping, we save the best model to file</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;best_model.pkl&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            <span class="built_in">print</span> <span class="string">&#x27;Saving best mode to best_model.pkl...&#x27;</span></span><br><span class="line">            cPickle.dump(best_net, fp)</span><br><span class="line">                    </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\nFinished training network.&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Best validation accuracy of &#123;0:.2%&#125; obtained at best_epoch &#123;1&#125;&quot;</span>.<span class="built_in">format</span>(best_validation_accuracy, best_epoch))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Corresponding test accuracy of &#123;0:.2%&#125;&quot;</span>.<span class="built_in">format</span>(test_accuracy))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> evaluation_costs, evaluation_accuracys, training_costs, training_accuracys,best_epoch <span class="comment"># for plot</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#********************************************************</span></span><br><span class="line"><span class="comment"># load model and predict on test data</span></span><br><span class="line"><span class="comment">#********************************************************</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_network_and_predict</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    An example of how to load a trained model and use it</span></span><br><span class="line"><span class="string">    to predict labels.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># load the saved model</span></span><br><span class="line">    net = cPickle.load(<span class="built_in">open</span>(<span class="string">&#x27;best_model.pkl&#x27;</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># predict</span></span><br><span class="line">    training_set_size = <span class="number">50000</span></span><br><span class="line">    train_data,val_data,test_data = load_data_shared(training_set_size=training_set_size)</span><br><span class="line">    test_x,test_y = test_data</span><br><span class="line">    </span><br><span class="line">    mini_batch_size = <span class="number">10</span></span><br><span class="line">    num_test_batches = size(test_data)/mini_batch_size </span><br><span class="line">    </span><br><span class="line">    i = T.lscalar()</span><br><span class="line">    <span class="comment"># test predict</span></span><br><span class="line">    test_mb_predictions = theano.function(</span><br><span class="line">        [i], net.layers[-<span class="number">1</span>].y_out,   <span class="comment"># y(m,)　m个样本的预测结果</span></span><br><span class="line">        givens=&#123;</span><br><span class="line">            net.x:</span><br><span class="line">            test_x[i*mini_batch_size: (i+<span class="number">1</span>)*mini_batch_size]</span><br><span class="line">        &#125;)</span><br><span class="line">    <span class="comment"># test accuracy</span></span><br><span class="line">    test_mb_accuracy = theano.function(</span><br><span class="line">        [i], net.layers[-<span class="number">1</span>].accuracy(net.y), <span class="comment"># y(m,)</span></span><br><span class="line">        givens=&#123;</span><br><span class="line">            net.x:</span><br><span class="line">            test_x[i*mini_batch_size: (i+<span class="number">1</span>)*mini_batch_size],</span><br><span class="line">            net.y:</span><br><span class="line">            test_y[i*mini_batch_size: (i+<span class="number">1</span>)*mini_batch_size]</span><br><span class="line">        &#125;)</span><br><span class="line">    </span><br><span class="line">    test_predictions = test_mb_predictions(<span class="number">0</span>)</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&#x27;real values of first 10: &#x27;</span>,test_y[:<span class="number">10</span>].<span class="built_in">eval</span>()</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&#x27;predictions of first 10: &#x27;</span>,test_predictions</span><br><span class="line">    </span><br><span class="line">    test_accuracy = np.mean( [test_mb_accuracy(j) <span class="keyword">for</span> j <span class="keyword">in</span> xrange(num_test_batches)] )</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&#x27;test_accuracy &#x27;</span>,test_accuracy</span><br><span class="line">    </span><br><span class="line"><span class="comment">#********************************************************</span></span><br><span class="line"><span class="comment"># end of predict</span></span><br><span class="line"><span class="comment">#********************************************************</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#### Define layer types</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ConvPoolLayer</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Used to create a combination of a convolutional and a max-pooling</span></span><br><span class="line"><span class="string">    layer.  A more sophisticated implementation would separate the</span></span><br><span class="line"><span class="string">    two, but for our purposes we&#x27;ll always use them together, and it</span></span><br><span class="line"><span class="string">    simplifies the code, so it makes sense to combine them.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, filter_shape, image_shape, poolsize=(<span class="params"><span class="number">2</span>, <span class="number">2</span></span>),</span></span><br><span class="line"><span class="params">                 activation_fn=sigmoid</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;`filter_shape` is a tuple of length 4, whose entries are the number</span></span><br><span class="line"><span class="string">        of filters, the number of input feature maps, the filter height, and the</span></span><br><span class="line"><span class="string">        filter width.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        `image_shape` is a tuple of length 4, whose entries are the</span></span><br><span class="line"><span class="string">        mini-batch size, the number of input feature maps, the image</span></span><br><span class="line"><span class="string">        height, and the image width.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        `poolsize` is a tuple of length 2, whose entries are the y and</span></span><br><span class="line"><span class="string">        x pooling sizes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        np.prod((2,2)) = 4 # int64</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        ConvPoolLayer1</span></span><br><span class="line"><span class="string">        image_shape=(m,1,28,28)  1*28*28   (1 input feature map)</span></span><br><span class="line"><span class="string">        filter_shape=(20,1,5,5)  20*24*24 </span></span><br><span class="line"><span class="string">        poolsize=(2,2)           20*12*12 </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        ConvPoolLayer2</span></span><br><span class="line"><span class="string">        image_shape=(m,20,12,12) 20*12*12  (20 input feature map)</span></span><br><span class="line"><span class="string">        filter_shape=(40,20,5,5) 40*8*8</span></span><br><span class="line"><span class="string">        poolsize=(2,2)           40*4*4</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        ConvPoolLayer1</span></span><br><span class="line"><span class="string">        (20,1,5,5) </span></span><br><span class="line"><span class="string">        20指定当前ConvLayer1的features的数量: c1_f1,c1_f2,....c1_f19,c1_f20。</span></span><br><span class="line"><span class="string">        (1,5,5)指定feature的一个pixel所对应的local receptive field(LRF),此处对应1个input feature的5*5区域。</span></span><br><span class="line"><span class="string">        对应的w: w1,w2,...w19,w20 of size(1,5,5)===&gt;w(20,1,5,5) filter_shape</span></span><br><span class="line"><span class="string">        对应的b: b1,b2,...b19,b20 of size()     ===&gt;b(20,)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        ConvPoolLayer2</span></span><br><span class="line"><span class="string">        (40,20,5,5) </span></span><br><span class="line"><span class="string">        40指定当前ConvLayer2的features的数量: c2_f1,c2_f2,....c2_f39,c2_f40。</span></span><br><span class="line"><span class="string">        (20,5,5)指定feature的一个pixel所对应的local receptive field(LRF),此处对应20个input feature的5*5区域。</span></span><br><span class="line"><span class="string">        对应的w: w1,w2,...w39,w40 of size(20,5,5)===&gt;w(40,20,5,5) filter_shape</span></span><br><span class="line"><span class="string">        对应的b: b1,b2,...b39,b40 of size()     ===&gt;b(40,)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">assert</span> image_shape[<span class="number">1</span>] == filter_shape[<span class="number">1</span>] <span class="comment"># input feature maps</span></span><br><span class="line">        <span class="variable language_">self</span>.filter_shape = filter_shape</span><br><span class="line">        <span class="variable language_">self</span>.image_shape = image_shape</span><br><span class="line">        <span class="variable language_">self</span>.poolsize = poolsize</span><br><span class="line">        <span class="variable language_">self</span>.activation_fn=activation_fn</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># initialize weights and biases</span></span><br><span class="line">        <span class="comment"># 20*(5*5)/(2*2) = 500/4 = 125</span></span><br><span class="line">        <span class="comment"># 40*(5*5)/(2*2) = 1000/4 = 250</span></span><br><span class="line">        <span class="comment">#n_out = (filter_shape[0]*np.prod(filter_shape[2:])/np.prod(poolsize)) # 125  250 (why???)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># for tanh: w_bound = numpy.sqrt(6./(n_in+n_out))</span></span><br><span class="line">        <span class="comment"># for sigmoid: w_bound = 4*w_bound(tanh)</span></span><br><span class="line">        <span class="comment"># for ReLU: w = 0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># there are &quot;num input feature maps * filter height * filter width&quot; inputs to each hidden unit</span></span><br><span class="line">        n_in = np.prod(filter_shape[<span class="number">1</span>:]) <span class="comment"># LRF</span></span><br><span class="line">        <span class="comment"># each unit in the lower layer receives a gradient from:</span></span><br><span class="line">        <span class="comment"># &quot;num output feature maps * filter height * filter width&quot; / pooling size</span></span><br><span class="line">        n_out = (filter_shape[<span class="number">0</span>] * np.prod(filter_shape[<span class="number">2</span>:]) // np.prod(poolsize))</span><br><span class="line">        </span><br><span class="line">        w_bound = np.sqrt(<span class="number">6.</span>/(n_in+n_out))</span><br><span class="line">        <span class="keyword">if</span> activation_fn == sigmoid:</span><br><span class="line">            w_bound = <span class="number">4</span>*w_bound</span><br><span class="line">            </span><br><span class="line">        <span class="variable language_">self</span>.w = theano.shared(</span><br><span class="line">            np.asarray(</span><br><span class="line">                <span class="comment">#np.random.normal(loc=0, scale=np.sqrt(1.0/n_out), </span></span><br><span class="line">                np.random.uniform(low=-w_bound,high=w_bound, </span><br><span class="line">                                 size=filter_shape), </span><br><span class="line">                <span class="comment"># w(20,1,5,5) w(40,20,5,5)</span></span><br><span class="line">                dtype=theano.config.floatX),</span><br><span class="line">            borrow=<span class="literal">True</span>) </span><br><span class="line">        <span class="variable language_">self</span>.b = theano.shared(</span><br><span class="line">            np.asarray(</span><br><span class="line">                np.random.normal(loc=<span class="number">0</span>, scale=<span class="number">1.0</span>, size=(filter_shape[<span class="number">0</span>],)), </span><br><span class="line">                <span class="comment"># b(20,) b(40,)</span></span><br><span class="line">                dtype=theano.config.floatX),</span><br><span class="line">            borrow=<span class="literal">True</span>)</span><br><span class="line">        <span class="variable language_">self</span>.params = [<span class="variable language_">self</span>.w, <span class="variable language_">self</span>.b]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">set_inpt</span>(<span class="params">self, inpt, inpt_dropout, mini_batch_size</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        inpt = x:  fmatrix(m,784)</span></span><br><span class="line"><span class="string">        ConvPoolLayer1</span></span><br><span class="line"><span class="string">        image_shape=(m,1,28,28)  m,1*28*28   (1 input feature map)</span></span><br><span class="line"><span class="string">        filter_shape=(20,1,5,5)  m,20*24*24   w(20,1,5,5) b(20,)</span></span><br><span class="line"><span class="string">        poolsize=(2,2)           m,20*12*12 </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        ConvPoolLayer2</span></span><br><span class="line"><span class="string">        image_shape=(m,20,12,12) m,20*12*12  (20 input feature map)</span></span><br><span class="line"><span class="string">        filter_shape=(40,20,5,5) m,40*8*8     w(40,20,5,5) b(40,)</span></span><br><span class="line"><span class="string">        poolsize=(2,2)           m,40*4*4</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        ConvPoolLayer1</span></span><br><span class="line"><span class="string">        inpt(m,784)---&gt;inpt(m,1,28,28)</span></span><br><span class="line"><span class="string">        conv_out(m,20,24,24)</span></span><br><span class="line"><span class="string">        pooled_out(m,20,12,12)</span></span><br><span class="line"><span class="string">        output(m,20,12,12)</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        ConvPoolLayer2</span></span><br><span class="line"><span class="string">        inpt(m,20,12,12)</span></span><br><span class="line"><span class="string">        conv_out(m,40,8,8)</span></span><br><span class="line"><span class="string">        pooled_out(m,40,4,4)</span></span><br><span class="line"><span class="string">        output(m,40,4,4)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.inpt = inpt.reshape(<span class="variable language_">self</span>.image_shape)</span><br><span class="line">        conv_out = conv.conv2d( <span class="built_in">input</span>=<span class="variable language_">self</span>.inpt, image_shape=<span class="variable language_">self</span>.image_shape, </span><br><span class="line">                               filters=<span class="variable language_">self</span>.w, filter_shape=<span class="variable language_">self</span>.filter_shape) </span><br><span class="line">        </span><br><span class="line">        <span class="comment">#conv_out = conv.conv2d(input=self.inpt,filters=self.w) </span></span><br><span class="line">        <span class="comment">#theano.tensor.var.TensorVariable float32 TensorType(float32, 4D)</span></span><br><span class="line">        </span><br><span class="line">        pooled_out = pool_2d( <span class="built_in">input</span>=conv_out, ws=<span class="variable language_">self</span>.poolsize, ignore_border=<span class="literal">True</span>) </span><br><span class="line">        <span class="comment">#theano.tensor.var.TensorVariable float32 TensorType(float32, 4D)</span></span><br><span class="line">        </span><br><span class="line">        b_shuffle = <span class="variable language_">self</span>.b.dimshuffle(<span class="string">&#x27;x&#x27;</span>, <span class="number">0</span>, <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;x&#x27;</span>)  </span><br><span class="line">        <span class="comment"># TensorVariable TensorType(float32, (True, False, True, True))</span></span><br><span class="line">        <span class="comment"># ConvPoolLayer1: b(20,) 20个feature map分别增加b0,b1,...b19,b20</span></span><br><span class="line">        <span class="comment"># 对于pooled_out=(m,20,12,12)而言，(&#x27;x&#x27;, 0, &#x27;x&#x27;, &#x27;x&#x27;)的dim2=0，其他为x</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ConvPoolLayer2: b(40,) 40个feature map分别增加b0,b1,...b39,b40</span></span><br><span class="line">        <span class="comment"># 对于pooled_out=(m,40,4,4)而言，(&#x27;x&#x27;, 0, &#x27;x&#x27;, &#x27;x&#x27;)的dim2=0，其他为x</span></span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.output = <span class="variable language_">self</span>.activation_fn( pooled_out + b_shuffle )</span><br><span class="line">        <span class="comment">#theano.tensor.var.TensorVariable float32 TensorType(float32, 4D)</span></span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.output_dropout = <span class="variable language_">self</span>.output <span class="comment"># no dropout in the convolutional layers</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FullyConnectedLayer</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_in, n_out, activation_fn=sigmoid, p_dropout=<span class="number">0.0</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.n_in = n_in</span><br><span class="line">        <span class="variable language_">self</span>.n_out = n_out</span><br><span class="line">        <span class="variable language_">self</span>.activation_fn = activation_fn</span><br><span class="line">        <span class="variable language_">self</span>.p_dropout = p_dropout</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#rng = numpy.random.RandomState(1234) # for w initialization</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># for tanh: w_bound = numpy.sqrt(6./(n_in+n_out))</span></span><br><span class="line">        <span class="comment"># for sigmoid: w_bound = 4*w_bound(tanh)</span></span><br><span class="line">        <span class="comment"># for ReLU: w = 0</span></span><br><span class="line">        </span><br><span class="line">        w_bound = np.sqrt(<span class="number">6.</span>/(n_in+n_out))</span><br><span class="line">        <span class="keyword">if</span> activation_fn == sigmoid:</span><br><span class="line">            w_bound = <span class="number">4</span>*w_bound</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Initialize weights and biases</span></span><br><span class="line">        <span class="variable language_">self</span>.w = theano.shared(</span><br><span class="line">            np.asarray(</span><br><span class="line">                <span class="comment">#np.random.normal(loc=0.0, scale=np.sqrt(1.0/n_in),</span></span><br><span class="line">                np.random.uniform(low=-w_bound,high=w_bound,          </span><br><span class="line">                size=(n_in, n_out)),</span><br><span class="line">                dtype=theano.config.floatX),</span><br><span class="line">            name=<span class="string">&#x27;w&#x27;</span>, borrow=<span class="literal">True</span>)</span><br><span class="line">        <span class="variable language_">self</span>.b = theano.shared(</span><br><span class="line">            np.asarray(np.random.normal(loc=<span class="number">0.0</span>, scale=<span class="number">1.0</span>, size=(n_out,)),</span><br><span class="line">                       dtype=theano.config.floatX),</span><br><span class="line">            name=<span class="string">&#x27;b&#x27;</span>, borrow=<span class="literal">True</span>)</span><br><span class="line">        <span class="variable language_">self</span>.params = [<span class="variable language_">self</span>.w, <span class="variable language_">self</span>.b]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">set_inpt</span>(<span class="params">self, inpt, inpt_dropout, mini_batch_size</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        (1) inpt,output for validating and testing</span></span><br><span class="line"><span class="string">        (2) inpt_dropout,output_dropout for training (output_dropout---&gt;[cost]---&gt;grad---&gt;params)</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        以 ConvPoolLayer1(m,20,12,12),ConvPoolLayer2(m,40,4,4),[640,30,10]网络结构为例说明：</span></span><br><span class="line"><span class="string">        ************************************************************************************************</span></span><br><span class="line"><span class="string">        X(m,784),Y(m,)</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        ConvPoolLayer1:</span></span><br><span class="line"><span class="string">        当前层的inpt是前一层的output，因为是第一层，所以初始化为inpt = X(m,784)</span></span><br><span class="line"><span class="string">        inpt(m,784)---&gt;inpt(m,1,28,28)</span></span><br><span class="line"><span class="string">        conv_out(m,20,24,24)</span></span><br><span class="line"><span class="string">        pooled_out(m,20,12,12)</span></span><br><span class="line"><span class="string">        output(m,20,12,12)</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        ConvPoolLayer2:</span></span><br><span class="line"><span class="string">        inpt(m,20,12,12)</span></span><br><span class="line"><span class="string">        conv_out(m,40,8,8)</span></span><br><span class="line"><span class="string">        pooled_out(m,40,4,4)</span></span><br><span class="line"><span class="string">        output(m,40,4,4)</span></span><br><span class="line"><span class="string">        ************************************************************************************************</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        对于FullyConnectedLayer而言，inpt是ConvPoolLayer2的output=(m,40,4,4) </span></span><br><span class="line"><span class="string">        ================================================================================================</span></span><br><span class="line"><span class="string">        Layer1:</span></span><br><span class="line"><span class="string">        inpt=(m,40,4,4)---&gt;inpt(m,640)    a1(m,640)即：m个样本，每个样本640个neurons</span></span><br><span class="line"><span class="string">        output = sigmoid(input*w+b) ===&gt; a2 = sigmoid(a1*w+b)</span></span><br><span class="line"><span class="string">        a2(m,30) = sigmoid(  a1(m,640)* w(640,30)+ b(30,) ) </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Layer2:</span></span><br><span class="line"><span class="string">        当前层的inpt是前一层的output，即是FullyConnectedLayer1的output，包含30个hidden neurons输出 a2(m,30)</span></span><br><span class="line"><span class="string">        output = SOFTMAX(input*w+b) ===&gt; a3 = SOFTMAX(a2*w+b)</span></span><br><span class="line"><span class="string">        a3(m,10) = SOFTMAX(  a2(m,30)* w(30,10)+ b(10,) ) </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        output是m个样本对应的10个概率,y_out是m个样本对应的真实数值。</span></span><br><span class="line"><span class="string">        ================================================================================================</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.inpt = inpt.reshape((mini_batch_size, <span class="variable language_">self</span>.n_in))</span><br><span class="line">        <span class="comment">#self.output = self.activation_fn((1-self.p_dropout)*T.dot(self.inpt, self.w) + self.b)  </span></span><br><span class="line">        <span class="variable language_">self</span>.output = <span class="variable language_">self</span>.activation_fn(T.dot(<span class="variable language_">self</span>.inpt, <span class="variable language_">self</span>.w) + <span class="variable language_">self</span>.b)</span><br><span class="line">       </span><br><span class="line">        <span class="comment">#self.y_out = T.argmax(self.output, axis=1) # 暂时不用，只是用最后一层的y_out作为输出结果</span></span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.inpt_dropout = dropout_layer( inpt_dropout.reshape((mini_batch_size, <span class="variable language_">self</span>.n_in)), <span class="variable language_">self</span>.p_dropout)</span><br><span class="line">        <span class="variable language_">self</span>.output_dropout = <span class="variable language_">self</span>.activation_fn(T.dot(<span class="variable language_">self</span>.inpt_dropout, <span class="variable language_">self</span>.w) + <span class="variable language_">self</span>.b)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#def accuracy(self, y):</span></span><br><span class="line">    <span class="comment">#    &quot;Return the accuracy for the mini-batch.&quot;</span></span><br><span class="line">    <span class="comment">#    # 暂时不用，只是用最后一层</span></span><br><span class="line">    <span class="comment">#    return T.mean(T.eq(y, self.y_out))</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SoftmaxLayer</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_in, n_out, p_dropout=<span class="number">0.0</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.n_in = n_in</span><br><span class="line">        <span class="variable language_">self</span>.n_out = n_out</span><br><span class="line">        <span class="variable language_">self</span>.activation_fn = softmax <span class="comment"># default to softmax</span></span><br><span class="line">        <span class="variable language_">self</span>.p_dropout = p_dropout</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Initialize weights and biases</span></span><br><span class="line">        <span class="comment"># for sigmoid neurons,w---&gt;(0, 1/sqrt(n_in)) b---&gt;(0,1)</span></span><br><span class="line">        <span class="comment"># for softmax neurons,w = 0,b = 0, no need using suitably parameteried normal random variables</span></span><br><span class="line">        <span class="variable language_">self</span>.w = theano.shared(</span><br><span class="line">            np.zeros((n_in, n_out), dtype=theano.config.floatX),</span><br><span class="line">            name=<span class="string">&#x27;w&#x27;</span>, borrow=<span class="literal">True</span>)</span><br><span class="line">        <span class="variable language_">self</span>.b = theano.shared(</span><br><span class="line">            np.zeros((n_out,), dtype=theano.config.floatX),</span><br><span class="line">            name=<span class="string">&#x27;b&#x27;</span>, borrow=<span class="literal">True</span>)</span><br><span class="line">        <span class="variable language_">self</span>.params = [<span class="variable language_">self</span>.w, <span class="variable language_">self</span>.b]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">set_inpt</span>(<span class="params">self, inpt, inpt_dropout, mini_batch_size</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        (1) inpt,output for validating and testing</span></span><br><span class="line"><span class="string">        (2) inpt_dropout,output_dropout for training (output_dropout---&gt;[cost]---&gt;grad---&gt;params)</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        在Python中，a = sigmoid(w*a+b), w=(30,784),a=(784,1)一次使用一个样本参与计算。</span></span><br><span class="line"><span class="string">        在Theano中修改为,a = sigmoid(a*w+b) a=(m,784),w=(784,30)一次使用m个样本参与计算。</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        以[784,30,10]网络结构为例说明：</span></span><br><span class="line"><span class="string">        Layer1:</span></span><br><span class="line"><span class="string">        当前层的inpt是前一层的output，因为是第一层，所以初始化为a1 = X(m,784) Matrix，每一个样本包含784个输入neurons</span></span><br><span class="line"><span class="string">        output = sigmoid(input*w+b) ===&gt; a2 = sigmoid(a1*w+b)</span></span><br><span class="line"><span class="string">        a2(m,30) = sigmoid(  a1(m,784)* w(784,30)+ b(30,) ) </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Layer2:</span></span><br><span class="line"><span class="string">        当前层的inpt是前一层的output，即是FullyConnectedLayer的output，包含30个hidden neurons输出 a2(m,30)</span></span><br><span class="line"><span class="string">        output = SOFTMAX(input*w+b) ===&gt; a3 = SOFTMAX(a2*w+b)</span></span><br><span class="line"><span class="string">        a3(m,10) = SOFTMAX(  a2(m,30)* w(30,10)+ b(10,) ) </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        output是m个样本对应的10个概率,y_out是m个样本对应的真实数值。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.inpt = inpt.reshape((mini_batch_size, <span class="variable language_">self</span>.n_in))  <span class="comment"># tesorvariable Matrix(m,n_in)</span></span><br><span class="line">        <span class="comment">#self.output = self.activation_fn((1-self.p_dropout)*T.dot(self.inpt, self.w) + self.b)</span></span><br><span class="line">        <span class="variable language_">self</span>.output = <span class="variable language_">self</span>.activation_fn(T.dot(<span class="variable language_">self</span>.inpt, <span class="variable language_">self</span>.w) + <span class="variable language_">self</span>.b)</span><br><span class="line">        </span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        input--&gt;    output   ---&gt; y_out</span></span><br><span class="line"><span class="string">        X1---&gt; [y0,y1,...y9] ---&gt;  1</span></span><br><span class="line"><span class="string">        X2---&gt; [y0,y1,...y9] ---&gt;  0</span></span><br><span class="line"><span class="string">        ...</span></span><br><span class="line"><span class="string">        Xm---&gt; [y0,y1,...y9] ---&gt;  2</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        axis沿着row作为一个整体进行，y_out作为最终的输出=vector(m,)。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.y_out = T.argmax(<span class="variable language_">self</span>.output, axis=<span class="number">1</span>) <span class="comment"># 对应的数值 [2,1,...7]</span></span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.inpt_dropout = dropout_layer( inpt_dropout.reshape((mini_batch_size, <span class="variable language_">self</span>.n_in)), <span class="variable language_">self</span>.p_dropout)</span><br><span class="line">        <span class="variable language_">self</span>.output_dropout = <span class="variable language_">self</span>.activation_fn(T.dot(<span class="variable language_">self</span>.inpt_dropout, <span class="variable language_">self</span>.w) + <span class="variable language_">self</span>.b)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">cost</span>(<span class="params">self, net</span>):</span><br><span class="line">        <span class="string">&quot;Return the log-likelihood cost.&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        使用output_dropout用于train</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        (1) 一个样本对应的代价Cx</span></span><br><span class="line"><span class="string">        C = -log(a[i])</span></span><br><span class="line"><span class="string">        i = np.argmax(y)  # a(10,1) y(10,1)</span></span><br><span class="line"><span class="string">        return -np.log(a[i,0])</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        (2) m个样本的平均代价</span></span><br><span class="line"><span class="string">        计算代价的时候，传递Network作为参数，方便获取net.y</span></span><br><span class="line"><span class="string">       </span></span><br><span class="line"><span class="string">                output(m,10)     net.y   cost</span></span><br><span class="line"><span class="string">        X1---&gt; [y0,y1,...y9] ---&gt;  1     -log a[1,1]</span></span><br><span class="line"><span class="string">        X2---&gt; [y0,y1,...y9] ---&gt;  0     -log a[2,0]</span></span><br><span class="line"><span class="string">        Xm---&gt; [y0,y1,...y9] ---&gt;  2     -log a[m,2]</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        a = np.array([[0,   0.8, 0,   0,...],</span></span><br><span class="line"><span class="string">                      [0.9, 0,   0,   0,...],</span></span><br><span class="line"><span class="string">                      [0,   0,   0.7, 0...]])</span></span><br><span class="line"><span class="string">        y = [1,0,2]</span></span><br><span class="line"><span class="string">        a[[0,1,2],y]</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        &gt; array([ 0.8,  0.9,  0.7])</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        m = net.y.shape[<span class="number">0</span>]</span><br><span class="line">        rows = T.arange(m)</span><br><span class="line">        <span class="keyword">return</span> -T.mean(T.log( <span class="variable language_">self</span>.output_dropout[rows, net.y] ))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">self, y</span>):</span><br><span class="line">        <span class="string">&quot;Return the accuracy for the mini-batch.&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        使用output,y_out用于test</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        y(m,) 对应m个样本的真实结果</span></span><br><span class="line"><span class="string">        y_out(m,)　对应m个样本的预测结果</span></span><br><span class="line"><span class="string">        如果mini_batch_size = 5</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        y = np.array([2,1,7,8,9])</span></span><br><span class="line"><span class="string">        y_out = np.array([2,1,7,6,9])</span></span><br><span class="line"><span class="string">        np.mean(np.equal(y,y_out))  # [1,1,1,0,1] 0.80</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> T.mean(T.eq(y, <span class="variable language_">self</span>.y_out))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#### Miscellanea</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">size</span>(<span class="params">data</span>):</span><br><span class="line">    <span class="string">&quot;Return the size of the dataset `data`.&quot;</span></span><br><span class="line">    <span class="keyword">return</span> data[<span class="number">0</span>].get_value(borrow=<span class="literal">True</span>).shape[<span class="number">0</span>]  <span class="comment"># N = 50000</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dropout_layer</span>(<span class="params">layer, p_dropout</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    对于[784,30,10]</span></span><br><span class="line"><span class="string">    Layer1:</span></span><br><span class="line"><span class="string">    layer= float32 (m,784), p_dropout = 0.2,对每个节点以一定的概率进行drop</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    参考：http://www.jianshu.com/p/ba9ca3b07922</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Inverted Dropout </span></span><br><span class="line"><span class="string">    我们稍微将 Dropout 方法改进一下，使得我们只需要在训练阶段缩放激活函数的输出值，而不用在测试阶段改变什么。</span></span><br><span class="line"><span class="string">    这个改进的 Dropout 方法就被称之为 Inverted Dropout 。</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    在各种深度学习框架的实现中，我们都是用 Inverted Dropout 来代替 Dropout，因为这种方式有助于模型的完整性，</span></span><br><span class="line"><span class="string">    我们只需要修改一个参数（保留/丢弃概率），而整个模型都不用修改。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    srng = shared_randomstreams.RandomStreams( np.random.RandomState(<span class="number">0</span>).randint(<span class="number">999999</span>) )</span><br><span class="line">    retain_prob = <span class="number">1.</span> - p_dropout <span class="comment"># retain probility  theano.config.floatX</span></span><br><span class="line">    <span class="comment">#mask = srng.binomial(n=1, p=retain_prob, size=layer.shape,dtype=&#x27;int8&#x27;) # int8</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#mask: &lt;class &#x27;theano.tensor.var.TensorVariable&#x27;&gt; TensorType(float32, vector)</span></span><br><span class="line">    mask = srng.binomial(n=<span class="number">1</span>, p=retain_prob, size=layer.shape,dtype=theano.config.floatX)</span><br><span class="line">    mask_layer = layer*mask</span><br><span class="line">    <span class="keyword">return</span> mask_layer/retain_prob <span class="comment">#在train阶段除以retain_prob，以便test阶段每一个Layer的output形式保持不变。</span></span><br></pre></td></tr></table></figure>

<h2 id="Test-Network3"><a href="#Test-Network3" class="headerlink" title="Test Network3"></a>Test Network3</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">random.seed(<span class="number">12345678</span>)</span><br><span class="line">np.random.seed(<span class="number">12345678</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#from ke_network3 import *</span></span><br><span class="line">epochs = <span class="number">3</span></span><br><span class="line">training_set_size = <span class="number">100</span></span><br><span class="line">mini_batch_size = <span class="number">10</span></span><br><span class="line">train_data,val_data,test_data = load_data_shared(training_set_size=training_set_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># for conv pool layer</span></span><br><span class="line">image_shape=(mini_batch_size,<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line">filter_shape=(<span class="number">20</span>,<span class="number">1</span>,<span class="number">5</span>,<span class="number">5</span>)</span><br><span class="line">poolsize=(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">convpool_layer1 = ConvPoolLayer(image_shape=image_shape,filter_shape=filter_shape, poolsize=poolsize)</span><br><span class="line">n_in = <span class="number">20</span>*<span class="number">12</span>*<span class="number">12</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">image_shape=(mini_batch_size,<span class="number">20</span>,<span class="number">12</span>,<span class="number">12</span>)</span><br><span class="line">filter_shape=(<span class="number">40</span>,<span class="number">20</span>,<span class="number">5</span>,<span class="number">5</span>)</span><br><span class="line">poolsize=(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">n_in = <span class="number">40</span>*<span class="number">4</span>*<span class="number">4</span></span><br><span class="line">convpool_layer2 = ConvPoolLayer(image_shape=image_shape,filter_shape=filter_shape, poolsize=poolsize)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">full_layer = FullyConnectedLayer(n_in=n_in,n_out=<span class="number">30</span>)</span><br><span class="line">softmax_layer = SoftmaxLayer(n_in=<span class="number">30</span>,n_out=<span class="number">10</span>)</span><br><span class="line"><span class="comment">#net = Network([convpool_layer1,full_layer,softmax_layer],10)</span></span><br><span class="line">net = Network([convpool_layer1,convpool_layer2,full_layer,softmax_layer],<span class="number">10</span>)</span><br><span class="line">net.SGD(train_data,epochs,mini_batch_size,<span class="number">0.3</span>,val_data,test_data,lmbda=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<pre><code>updates TensorType(float32, 4D) TensorType(float32, 4D)
updates TensorType(float32, vector) TensorType(float32, vector)
updates TensorType(float32, 4D) TensorType(float32, 4D)
updates TensorType(float32, vector) TensorType(float32, vector)
updates TensorType(float32, matrix) TensorType(float32, matrix)
updates TensorType(float32, vector) TensorType(float32, vector)
updates TensorType(float32, matrix) TensorType(float32, matrix)
updates TensorType(float32, vector) TensorType(float32, vector)
Training mini-batch number 0
Epoch 0: validation accuracy 10.00%

This is the best validation accuracy to date.
The corresponding test accuracy is 8.00%
Epoch 1: validation accuracy 10.00%

This is the best validation accuracy to date.
The corresponding test accuracy is 8.00%
Epoch 2: validation accuracy 10.00%

This is the best validation accuracy to date.
The corresponding test accuracy is 8.00%

Finished training network.
Best validation accuracy of 10.00% obtained at iteration 29
Corresponding test accuracy of 8.00%





([2.2949765, 2.2951121, 2.2958748],
 [0.10000000000000001, 0.10000000000000001, 0.10000000000000001],
 [2.2682509, 2.2655275, 2.2644706],
 [0.13, 0.13, 0.13])
</code></pre>
<h2 id="Basic-Test-of-Network3-py"><a href="#Basic-Test-of-Network3-py" class="headerlink" title="Basic Test of Network3.py"></a>Basic Test of Network3.py</h2><h3 id="1-load-data"><a href="#1-load-data" class="headerlink" title="(1) load data"></a>(1) load data</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ke_network3 <span class="keyword">import</span> *</span><br><span class="line">filename=<span class="string">&quot;../data/mnist.pkl.gz&quot;</span></span><br><span class="line">filename=<span class="string">&quot;../data/mnist_expanded.pkl.gz&quot;</span></span><br><span class="line">f = gzip.<span class="built_in">open</span>(filename, <span class="string">&#x27;rb&#x27;</span>)</span><br><span class="line">training_data, validation_data, test_data = cPickle.load(f)</span><br><span class="line">f.close()</span><br><span class="line"></span><br><span class="line">x = training_data[<span class="number">0</span>] <span class="comment"># (m,784)</span></span><br><span class="line">y = training_data[<span class="number">1</span>] <span class="comment"># (m,)</span></span><br><span class="line"><span class="built_in">print</span> <span class="built_in">type</span>(x),<span class="built_in">type</span>(y)</span><br><span class="line"><span class="built_in">print</span> <span class="built_in">type</span>(x[<span class="number">0</span>]),<span class="built_in">type</span>(y[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span> x.shape,y.shape</span><br><span class="line"><span class="built_in">print</span> x[<span class="number">0</span>].shape,y[<span class="number">0</span>].shape</span><br><span class="line">x2 = x[:<span class="number">10</span>,]</span><br><span class="line"></span><br><span class="line">set_size = <span class="number">10</span></span><br><span class="line">x = training_data[<span class="number">0</span>] <span class="comment"># float32  (50000, 784)</span></span><br><span class="line">y = training_data[<span class="number">1</span>] <span class="comment"># int64  (50000,)</span></span><br><span class="line"></span><br><span class="line">training_x = theano.shared( training_data[<span class="number">0</span>][:set_size,],  borrow=<span class="literal">True</span>) <span class="comment">#float32</span></span><br><span class="line">training_y = theano.shared( np.asarray(training_data[<span class="number">0</span>][:set_size,],dtype=<span class="string">&#x27;int8&#x27;</span>),  borrow=<span class="literal">True</span>) <span class="comment"># int8</span></span><br><span class="line"><span class="comment">#training_x2 = theano.shared(np.asarray(training_data[0], dtype=theano.config.floatX), borrow=True) # float64</span></span><br><span class="line"><span class="built_in">print</span> training_x.<span class="built_in">type</span></span><br><span class="line"><span class="built_in">print</span> training_y.<span class="built_in">type</span></span><br><span class="line"><span class="comment">#print training_x2.type</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 乘法可能会改变TensorVariable的类型</span></span><br><span class="line">new_x = training_x*<span class="number">0.1</span> <span class="comment"># float32---&gt;float64</span></span><br><span class="line"><span class="built_in">print</span> training_x.<span class="built_in">type</span>,new_x.<span class="built_in">type</span></span><br></pre></td></tr></table></figure>

<pre><code>&lt;type &#39;numpy.ndarray&#39;&gt; &lt;type &#39;numpy.ndarray&#39;&gt;
&lt;type &#39;numpy.ndarray&#39;&gt; &lt;type &#39;numpy.int64&#39;&gt;
(50, 784) (50,)
(784,) ()
TensorType(float32, matrix)
TensorType(int8, matrix)
TensorType(float32, matrix) TensorType(float64, matrix)
</code></pre>
<h3 id="2-dimshuffle-b-to-match-pooled-out"><a href="#2-dimshuffle-b-to-match-pooled-out" class="headerlink" title="(2) dimshuffle b to match pooled_out"></a>(2) dimshuffle b to match pooled_out</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">pooled_out = np.arange(<span class="number">18</span>).reshape(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span> pooled_out</span><br><span class="line">b = np.array([<span class="number">0.0</span>,<span class="number">1.0</span>],dtype=<span class="string">&#x27;float32&#x27;</span>) <span class="comment"># [0,1]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># shuffle b to match pooled_out</span></span><br><span class="line">sb = theano.shared(np.asarray(b,dtype=<span class="string">&#x27;float32&#x27;</span>)) </span><br><span class="line">y = sb.dimshuffle(<span class="string">&#x27;x&#x27;</span>, <span class="number">0</span>, <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;x&#x27;</span>)  <span class="comment"># TensorVariable TensorType(float32, (True, False, True, True))</span></span><br><span class="line"><span class="comment"># 2个feature map分别增加b0,b1</span></span><br><span class="line"><span class="built_in">print</span> <span class="built_in">type</span>(y),y.<span class="built_in">type</span>,y.shape.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">b_value = y.<span class="built_in">eval</span>()</span><br><span class="line"><span class="built_in">print</span> b_value</span><br><span class="line">pooled_out + b_value</span><br></pre></td></tr></table></figure>

<pre><code>[[[[ 0  1  2]
   [ 3  4  5]
   [ 6  7  8]]

  [[ 9 10 11]
   [12 13 14]
   [15 16 17]]]]
&lt;class &#39;theano.tensor.var.TensorVariable&#39;&gt; TensorType(float32, (True, False, True, True)) [1 2 1 1]
[[[[ 0.]]

  [[ 1.]]]]





array([[[[  0.,   1.,   2.],
         [  3.,   4.,   5.],
         [  6.,   7.,   8.]],

        [[ 10.,  11.,  12.],
         [ 13.,  14.,  15.],
         [ 16.,  17.,  18.]]]])
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><h2 id="History"><a href="#History" class="headerlink" title="History"></a>History</h2><ul>
<li>20180807: created.</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kezunlin.me/blog/network2-py/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="kezunlin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kezunlin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/network2-py/" class="post-title-link" itemprop="url">network2.py</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-08-07 10:35:00" itemprop="dateCreated datePublished" datetime="2018-08-07T10:35:00+08:00">2018-08-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-10-14 13:39:28" itemprop="dateModified" datetime="2024-10-14T13:39:28+08:00">2024-10-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="network2-py"><a href="#network2-py" class="headerlink" title="network2.py"></a>network2.py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;network2.py</span></span><br><span class="line"><span class="string">~~~~~~~~~~~~~~</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">An improved version of network.py, implementing the stochastic</span></span><br><span class="line"><span class="string">gradient descent learning algorithm for a feedforward neural network.</span></span><br><span class="line"><span class="string">Improvements include the addition of the cross-entropy cost function,</span></span><br><span class="line"><span class="string">regularization, and better initialization of network weights.  Note</span></span><br><span class="line"><span class="string">that I have focused on making the code simple, easily readable, and</span></span><br><span class="line"><span class="string">easily modifiable.  It is not optimized, and omits many desirable</span></span><br><span class="line"><span class="string">features.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#### Libraries</span></span><br><span class="line"><span class="comment"># Standard library</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="comment"># Third-party libraries</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#### Define the quadratic and cross-entropy cost functions</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">QuadraticCost</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fn</span>(<span class="params">a, y</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Return the cost associated with an output ``a`` and desired output ``y``.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0.5</span>*np.linalg.norm(a-y)**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">delta</span>(<span class="params">z, a, y</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Return the error delta from the output layer.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> (a-y) * sigmoid_prime(z)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CrossEntropyCost</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fn</span>(<span class="params">a, y</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Return the cost associated with an output ``a`` and desired output</span></span><br><span class="line"><span class="string">        ``y``.  Note that np.nan_to_num is used to ensure numerical</span></span><br><span class="line"><span class="string">        stability.  In particular, if both ``a`` and ``y`` have a 1.0</span></span><br><span class="line"><span class="string">        in the same slot, then the expression (1-y)*np.log(1-a)</span></span><br><span class="line"><span class="string">        returns nan.  The np.nan_to_num ensures that that is converted</span></span><br><span class="line"><span class="string">        to the correct value (0.0).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> np.<span class="built_in">sum</span>(np.nan_to_num(-y*np.log(a)-(<span class="number">1</span>-y)*np.log(<span class="number">1</span>-a)))</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">delta</span>(<span class="params">z, a, y</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Return the error delta from the output layer.  Note that the</span></span><br><span class="line"><span class="string">        parameter ``z`` is not used by the method.  It is included in</span></span><br><span class="line"><span class="string">        the method&#x27;s parameters in order to make the interface</span></span><br><span class="line"><span class="string">        consistent with the delta method for other cost classes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> (a-y)</span><br><span class="line"></span><br><span class="line"><span class="comment">#********************************************************    </span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LogLikelihoodCost</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fn</span>(<span class="params">a, y</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        C = -log(a[i])</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        a(10,1) y(10,1)</span></span><br><span class="line"><span class="string">        y = [0,0,1,0,0,0,0,0,0,0,0] </span></span><br><span class="line"><span class="string">        i = 2</span></span><br><span class="line"><span class="string">        C = -log a[2,0]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        i = np.argmax(y)  </span><br><span class="line">        <span class="keyword">return</span> -np.log(a[i,<span class="number">0</span>])</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">delta</span>(<span class="params">z, a, y</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        delta_j = aj-yj</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> (a-y)</span><br><span class="line"><span class="comment">#********************************************************  </span></span><br><span class="line">    </span><br><span class="line"><span class="comment">#### Main Network class</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Network</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, sizes, cost=CrossEntropyCost</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;The list ``sizes`` contains the number of neurons in the respective</span></span><br><span class="line"><span class="string">        layers of the network.  For example, if the list was [2, 3, 1]</span></span><br><span class="line"><span class="string">        then it would be a three-layer network, with the first layer</span></span><br><span class="line"><span class="string">        containing 2 neurons, the second layer 3 neurons, and the</span></span><br><span class="line"><span class="string">        third layer 1 neuron.  The biases and weights for the network</span></span><br><span class="line"><span class="string">        are initialized randomly, using</span></span><br><span class="line"><span class="string">        ``self.default_weight_initializer`` (see docstring for that</span></span><br><span class="line"><span class="string">        method).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.num_layers = <span class="built_in">len</span>(sizes)</span><br><span class="line">        <span class="variable language_">self</span>.sizes = sizes</span><br><span class="line">        <span class="variable language_">self</span>.cost=cost</span><br><span class="line">        <span class="comment"># init use_softmax with cost type</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.cost == LogLikelihoodCost:</span><br><span class="line">            <span class="variable language_">self</span>.use_softmax = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.use_softmax = <span class="literal">False</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># init weight initializer and feedforward method</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.use_softmax:</span><br><span class="line">            <span class="variable language_">self</span>.default_weight_initializer = <span class="variable language_">self</span>.default_weight_initializer_with_softmax</span><br><span class="line">            <span class="variable language_">self</span>.feedforward = <span class="variable language_">self</span>.feedforward_with_softmax</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.default_weight_initializer = <span class="variable language_">self</span>.default_weight_initializer_1</span><br><span class="line">            <span class="variable language_">self</span>.feedforward = <span class="variable language_">self</span>.feedforward_1</span><br><span class="line">        <span class="comment"># init weights and biases</span></span><br><span class="line">        <span class="variable language_">self</span>.default_weight_initializer()</span><br><span class="line">        <span class="comment"># at least an input and output layers</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="variable language_">self</span>.num_layers&gt;=<span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">default_weight_initializer_with_softmax</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># sigmoid neurons: w(0,1/sqrt(n_in)) b(0,1)</span></span><br><span class="line">        <span class="comment"># softmax neurons: w = b = 0</span></span><br><span class="line">        <span class="comment"># len(sizes)&gt;=2</span></span><br><span class="line">        <span class="comment"># (1) for sigmoid neuros</span></span><br><span class="line">        <span class="variable language_">self</span>.biases = [np.random.randn(y, <span class="number">1</span>) <span class="keyword">for</span> y <span class="keyword">in</span> <span class="variable language_">self</span>.sizes[<span class="number">1</span>:-<span class="number">1</span>]]</span><br><span class="line">        <span class="variable language_">self</span>.weights = [np.random.randn(y, x)/np.sqrt(x)</span><br><span class="line">                        <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="variable language_">self</span>.sizes[:-<span class="number">1</span>], <span class="variable language_">self</span>.sizes[<span class="number">1</span>:-<span class="number">1</span>])]</span><br><span class="line">        <span class="comment">#(2) for last somtmax neurons</span></span><br><span class="line">        x = <span class="variable language_">self</span>.sizes[-<span class="number">2</span>]</span><br><span class="line">        y = <span class="variable language_">self</span>.sizes[-<span class="number">1</span>]</span><br><span class="line">        last_b = np.zeros((y, <span class="number">1</span>))</span><br><span class="line">        last_w = np.zeros((y, x))</span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.biases.append(last_b)</span><br><span class="line">        <span class="variable language_">self</span>.weights.append(last_w)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">default_weight_initializer_1</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Initialize each weight using a Gaussian distribution with mean 0</span></span><br><span class="line"><span class="string">        and standard deviation 1 over the square root of the number of</span></span><br><span class="line"><span class="string">        weights connecting to the same neuron.  Initialize the biases</span></span><br><span class="line"><span class="string">        using a Gaussian distribution with mean 0 and standard</span></span><br><span class="line"><span class="string">        deviation 1.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Note that the first layer is assumed to be an input layer, and</span></span><br><span class="line"><span class="string">        by convention we won&#x27;t set any biases for those neurons, since</span></span><br><span class="line"><span class="string">        biases are only ever used in computing the outputs from later</span></span><br><span class="line"><span class="string">        layers.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.biases = [np.random.randn(y, <span class="number">1</span>) <span class="keyword">for</span> y <span class="keyword">in</span> <span class="variable language_">self</span>.sizes[<span class="number">1</span>:]]</span><br><span class="line">        <span class="variable language_">self</span>.weights = [np.random.randn(y, x)/np.sqrt(x)</span><br><span class="line">                        <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="variable language_">self</span>.sizes[:-<span class="number">1</span>], <span class="variable language_">self</span>.sizes[<span class="number">1</span>:])]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">large_weight_initializer</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Initialize the weights using a Gaussian distribution with mean 0</span></span><br><span class="line"><span class="string">        and standard deviation 1.  Initialize the biases using a</span></span><br><span class="line"><span class="string">        Gaussian distribution with mean 0 and standard deviation 1.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Note that the first layer is assumed to be an input layer, and</span></span><br><span class="line"><span class="string">        by convention we won&#x27;t set any biases for those neurons, since</span></span><br><span class="line"><span class="string">        biases are only ever used in computing the outputs from later</span></span><br><span class="line"><span class="string">        layers.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        This weight and bias initializer uses the same approach as in</span></span><br><span class="line"><span class="string">        Chapter 1, and is included for purposes of comparison.  It</span></span><br><span class="line"><span class="string">        will usually be better to use the default weight initializer</span></span><br><span class="line"><span class="string">        instead.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.biases = [np.random.randn(y, <span class="number">1</span>) <span class="keyword">for</span> y <span class="keyword">in</span> <span class="variable language_">self</span>.sizes[<span class="number">1</span>:]]</span><br><span class="line">        <span class="variable language_">self</span>.weights = [np.random.randn(y, x)</span><br><span class="line">                        <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="variable language_">self</span>.sizes[:-<span class="number">1</span>], <span class="variable language_">self</span>.sizes[<span class="number">1</span>:])]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">feedforward_with_softmax</span>(<span class="params">self, a</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Return the output of the network if ``a`` is input.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="variable language_">self</span>.biases[:-<span class="number">1</span>], <span class="variable language_">self</span>.weights[:-<span class="number">1</span>]):</span><br><span class="line">            a = sigmoid(np.dot(w, a)+b)</span><br><span class="line">        <span class="comment"># last layer</span></span><br><span class="line">        b,w = <span class="variable language_">self</span>.biases[-<span class="number">1</span>],<span class="variable language_">self</span>.weights[-<span class="number">1</span>]</span><br><span class="line">        last_z = np.dot(w, a)+b</span><br><span class="line">        last_a = softmax(last_z)</span><br><span class="line">        <span class="keyword">return</span> last_a</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">feedforward_1</span>(<span class="params">self, a</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Return the output of the network if ``a`` is input.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="variable language_">self</span>.biases, <span class="variable language_">self</span>.weights):</span><br><span class="line">            a = sigmoid(np.dot(w, a)+b)</span><br><span class="line">        <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">SGD</span>(<span class="params">self, training_data, epochs, mini_batch_size, eta,</span></span><br><span class="line"><span class="params">            lmbda = <span class="number">0.0</span>,</span></span><br><span class="line"><span class="params">            evaluation_data=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">            monitor_evaluation_cost=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">            monitor_evaluation_accuracy=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">            monitor_training_cost=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">            monitor_training_accuracy=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Train the neural network using mini-batch stochastic gradient</span></span><br><span class="line"><span class="string">        descent.  The ``training_data`` is a list of tuples ``(x, y)``</span></span><br><span class="line"><span class="string">        representing the training inputs and the desired outputs.  The</span></span><br><span class="line"><span class="string">        other non-optional parameters are self-explanatory, as is the</span></span><br><span class="line"><span class="string">        regularization parameter ``lmbda``.  The method also accepts</span></span><br><span class="line"><span class="string">        ``evaluation_data``, usually either the validation or test</span></span><br><span class="line"><span class="string">        data.  We can monitor the cost and accuracy on either the</span></span><br><span class="line"><span class="string">        evaluation data or the training data, by setting the</span></span><br><span class="line"><span class="string">        appropriate flags.  The method returns a tuple containing four</span></span><br><span class="line"><span class="string">        lists: the (per-epoch) costs on the evaluation data, the</span></span><br><span class="line"><span class="string">        accuracies on the evaluation data, the costs on the training</span></span><br><span class="line"><span class="string">        data, and the accuracies on the training data.  All values are</span></span><br><span class="line"><span class="string">        evaluated at the end of each training epoch.  So, for example,</span></span><br><span class="line"><span class="string">        if we train for 30 epochs, then the first element of the tuple</span></span><br><span class="line"><span class="string">        will be a 30-element list containing the cost on the</span></span><br><span class="line"><span class="string">        evaluation data at the end of each epoch. Note that the lists</span></span><br><span class="line"><span class="string">        are empty if the corresponding flag is not set.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> evaluation_data: n_data = <span class="built_in">len</span>(evaluation_data)</span><br><span class="line">        n = <span class="built_in">len</span>(training_data)</span><br><span class="line">        num_batches = n/mini_batch_size</span><br><span class="line">        evaluation_cost, evaluation_accuracy = [], []</span><br><span class="line">        training_cost, training_accuracy = [], []</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> xrange(epochs):</span><br><span class="line">            random.shuffle(training_data)</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> xrange(<span class="number">0</span>,num_batches):</span><br><span class="line">                mini_batch = training_data[k*mini_batch_size : (k+<span class="number">1</span>)*mini_batch_size]</span><br><span class="line">                <span class="variable language_">self</span>.update_mini_batch(mini_batch, eta, lmbda, <span class="built_in">len</span>(training_data))</span><br><span class="line">            <span class="built_in">print</span> <span class="string">&quot;Epoch %s training complete&quot;</span> % j</span><br><span class="line">            <span class="keyword">if</span> monitor_training_cost:</span><br><span class="line">                cost = <span class="variable language_">self</span>.total_cost(training_data, lmbda)</span><br><span class="line">                training_cost.append(cost)</span><br><span class="line">                <span class="built_in">print</span> <span class="string">&quot;Cost on training data: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(cost)</span><br><span class="line">            <span class="keyword">if</span> monitor_training_accuracy:</span><br><span class="line">                accuracy = <span class="variable language_">self</span>.accuracy(training_data, convert=<span class="literal">True</span>)</span><br><span class="line">                training_accuracy.append(accuracy)</span><br><span class="line">                <span class="built_in">print</span> <span class="string">&quot;Accuracy on training data: &#123;&#125; / &#123;&#125;&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">                    accuracy, n)</span><br><span class="line">            <span class="keyword">if</span> monitor_evaluation_cost:</span><br><span class="line">                cost = <span class="variable language_">self</span>.total_cost(evaluation_data, lmbda, convert=<span class="literal">True</span>)</span><br><span class="line">                evaluation_cost.append(cost)</span><br><span class="line">                <span class="built_in">print</span> <span class="string">&quot;Cost on evaluation data: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(cost)</span><br><span class="line">            <span class="keyword">if</span> monitor_evaluation_accuracy:</span><br><span class="line">                accuracy = <span class="variable language_">self</span>.accuracy(evaluation_data)</span><br><span class="line">                evaluation_accuracy.append(accuracy)</span><br><span class="line">                <span class="built_in">print</span> <span class="string">&quot;Accuracy on evaluation data: &#123;&#125; / &#123;&#125;&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">                    <span class="variable language_">self</span>.accuracy(evaluation_data), n_data)</span><br><span class="line">            <span class="built_in">print</span></span><br><span class="line">        <span class="keyword">return</span> evaluation_cost, evaluation_accuracy, training_cost, training_accuracy</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">calculate_sum_derivatives_of_mini_batch</span>(<span class="params">self,mini_batch</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        计算m个样本的总梯度和。</span></span><br><span class="line"><span class="string">        利用反向传播计算每一个样本(x,y)对应的梯度。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> <span class="variable language_">self</span>.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> <span class="variable language_">self</span>.weights]</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> mini_batch:</span><br><span class="line">            <span class="comment"># 给定一个样本X,利用反向传播算法计算对应w,b的梯度</span></span><br><span class="line">            delta_nabla_b, delta_nabla_w = <span class="variable language_">self</span>.backprop(x, y)</span><br><span class="line">            <span class="comment"># 对m个样本的梯度进行累计求和</span></span><br><span class="line">            nabla_b = [nb+dnb <span class="keyword">for</span> nb, dnb <span class="keyword">in</span> <span class="built_in">zip</span>(nabla_b, delta_nabla_b)]</span><br><span class="line">            nabla_w = [nw+dnw <span class="keyword">for</span> nw, dnw <span class="keyword">in</span> <span class="built_in">zip</span>(nabla_w, delta_nabla_w)]</span><br><span class="line">        <span class="keyword">return</span> nabla_b,nabla_w</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update_mini_batch</span>(<span class="params">self, mini_batch, eta, lmbda, n</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Update the network&#x27;s weights and biases by applying gradient</span></span><br><span class="line"><span class="string">        descent using backpropagation to a single mini batch.  The</span></span><br><span class="line"><span class="string">        ``mini_batch`` is a list of tuples ``(x, y)``, ``eta`` is the</span></span><br><span class="line"><span class="string">        learning rate, ``lmbda`` is the regularization parameter, and</span></span><br><span class="line"><span class="string">        ``n`` is the total size of the training data set.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        m = <span class="built_in">len</span>(mini_batch)</span><br><span class="line">        nabla_b,nabla_w = <span class="variable language_">self</span>.calculate_sum_derivatives_of_mini_batch(mini_batch)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#self.weights = [w-(eta/m)*nw for w, nw in zip(self.weights, nabla_w)]</span></span><br><span class="line">        <span class="comment">#self.biases =  [b-(eta/m)*nb for b, nb in zip(self.biases,  nabla_b)]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># L2 regularization</span></span><br><span class="line">        weight_decay = <span class="number">1</span>-eta*(lmbda/n)</span><br><span class="line">        <span class="variable language_">self</span>.weights = [weight_decay*w-(eta/m)*nw</span><br><span class="line">                        <span class="keyword">for</span> w, nw <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="variable language_">self</span>.weights, nabla_w)]</span><br><span class="line">        <span class="variable language_">self</span>.biases = [b-(eta/m)*nb</span><br><span class="line">                       <span class="keyword">for</span> b, nb <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="variable language_">self</span>.biases, nabla_b)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backprop</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Return a tuple &quot;(nabla_b, nabla_w)&quot; representing the</span></span><br><span class="line"><span class="string">        gradient for the cost function C_x.  &quot;nabla_b&quot; and</span></span><br><span class="line"><span class="string">        &quot;nabla_w&quot; are layer-by-layer lists of numpy arrays, similar</span></span><br><span class="line"><span class="string">        to &quot;self.biases&quot; and &quot;self.weights&quot;.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 初始化nb,nw,结构和b,w一样</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> <span class="variable language_">self</span>.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> <span class="variable language_">self</span>.weights]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># feedforward</span></span><br><span class="line">        <span class="comment"># 执行算法的feedforward阶段</span></span><br><span class="line">        <span class="comment">#　(1)初始化x作为a_1</span></span><br><span class="line">        activation = x</span><br><span class="line">        activations = [x] <span class="comment"># list to store all the activations, layer by layer</span></span><br><span class="line">        zs = [] <span class="comment"># list to store all the z vectors, layer by layer</span></span><br><span class="line">        <span class="comment"># (2)l=2,....L层，分别计算z_l,a_l并且保存下来。</span></span><br><span class="line">        <span class="comment">#*************************************************************</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.use_softmax:</span><br><span class="line">            <span class="keyword">for</span> b, w <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="variable language_">self</span>.biases[:-<span class="number">1</span>], <span class="variable language_">self</span>.weights[:-<span class="number">1</span>]):</span><br><span class="line">                z = np.dot(w, activation)+b</span><br><span class="line">                zs.append(z)</span><br><span class="line">                activation = sigmoid(z)</span><br><span class="line">                activations.append(activation)</span><br><span class="line">            <span class="comment">#last layer</span></span><br><span class="line">            b,w = <span class="variable language_">self</span>.biases[-<span class="number">1</span>],<span class="variable language_">self</span>.weights[-<span class="number">1</span>]</span><br><span class="line">            last_z = np.dot(w, activation)+b</span><br><span class="line">            last_a = softmax(last_z)</span><br><span class="line">            zs.append(last_z)</span><br><span class="line">            activations.append(last_a)</span><br><span class="line">        <span class="comment">#*************************************************************</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> b, w <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="variable language_">self</span>.biases, <span class="variable language_">self</span>.weights):</span><br><span class="line">                z = np.dot(w, activation)+b</span><br><span class="line">                zs.append(z)</span><br><span class="line">                activation = sigmoid(z)</span><br><span class="line">                activations.append(activation)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#========================================================================</span></span><br><span class="line">        <span class="comment"># 先计算所有的误差delta，最后计算所有层的梯度nb,nw，代码可读性更高一些</span></span><br><span class="line">        <span class="comment">#========================================================================</span></span><br><span class="line">        <span class="comment"># method2</span></span><br><span class="line">        <span class="comment"># backward pass</span></span><br><span class="line">        <span class="comment"># 执行算法的backward阶段</span></span><br><span class="line">        <span class="comment"># (3)初始化第L层的误差,delta_L　= cost(z,a,y)</span></span><br><span class="line">        l = -<span class="number">1</span></span><br><span class="line">        <span class="comment">#***************************************************</span></span><br><span class="line">        delta = <span class="variable language_">self</span>.cost.delta(zs[l],activations[l], y)</span><br><span class="line">        <span class="comment">#***************************************************</span></span><br><span class="line">        deltas = [delta] <span class="comment"># list to store all the errors,layer by layer</span></span><br><span class="line">        <span class="comment"># (4)初始化l=L-1,....2层的误差,delta_l = np.dot(w_l+1^T,delta_l+1)* sigmoid_prime(z_l)</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">2</span>, <span class="variable language_">self</span>.num_layers):</span><br><span class="line">            l = -i <span class="comment">#(-２代表L-1,-3代表L-2,-(L-1)代表2)</span></span><br><span class="line">            delta = np.dot(<span class="variable language_">self</span>.weights[l+<span class="number">1</span>].transpose(), deltas[l+<span class="number">1</span>]) * sigmoid_prime(zs[l])</span><br><span class="line">            deltas.insert(<span class="number">0</span>,delta) <span class="comment"># 确保误差的顺序，从后往前计算，所以需要insert在数组的最前面</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#(5)l=L,L-1,....2层，计算所有的梯度向量nb,nw</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">1</span>, <span class="variable language_">self</span>.num_layers):</span><br><span class="line">            l = -i <span class="comment">#(-1,-2,....-(L-1))</span></span><br><span class="line">            nabla_b[l] = deltas[l]</span><br><span class="line">            nabla_w[l] = np.dot(deltas[l], activations[l-<span class="number">1</span>].transpose())</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> (nabla_b, nabla_w)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">self, data, convert=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Return the number of inputs in ``data`` for which the neural</span></span><br><span class="line"><span class="string">        network outputs the correct result. The neural network&#x27;s</span></span><br><span class="line"><span class="string">        output is assumed to be the index of whichever neuron in the</span></span><br><span class="line"><span class="string">        final layer has the highest activation.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        The flag ``convert`` should be set to False if the data set is</span></span><br><span class="line"><span class="string">        validation or test data (the usual case), and to True if the</span></span><br><span class="line"><span class="string">        data set is the training data. The need for this flag arises</span></span><br><span class="line"><span class="string">        due to differences in the way the results ``y`` are</span></span><br><span class="line"><span class="string">        represented in the different data sets.  In particular, it</span></span><br><span class="line"><span class="string">        flags whether we need to convert between the different</span></span><br><span class="line"><span class="string">        representations.  It may seem strange to use different</span></span><br><span class="line"><span class="string">        representations for the different data sets.  Why not use the</span></span><br><span class="line"><span class="string">        same representation for all three data sets?  It&#x27;s done for</span></span><br><span class="line"><span class="string">        efficiency reasons -- the program usually evaluates the cost</span></span><br><span class="line"><span class="string">        on the training data and the accuracy on other data sets.</span></span><br><span class="line"><span class="string">        These are different types of computations, and using different</span></span><br><span class="line"><span class="string">        representations speeds things up.  More details on the</span></span><br><span class="line"><span class="string">        representations can be found in</span></span><br><span class="line"><span class="string">        mnist_loader.load_data_wrapper.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> convert: <span class="comment"># train data</span></span><br><span class="line">            results = [(np.argmax(<span class="variable language_">self</span>.feedforward(x)), np.argmax(y))</span><br><span class="line">                       <span class="keyword">for</span> (x, y) <span class="keyword">in</span> data]</span><br><span class="line">        <span class="keyword">else</span>: <span class="comment"># val/test data</span></span><br><span class="line">            results = [(np.argmax(<span class="variable language_">self</span>.feedforward(x)), y)</span><br><span class="line">                        <span class="keyword">for</span> (x, y) <span class="keyword">in</span> data]</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span>(<span class="built_in">int</span>(x == y) <span class="keyword">for</span> (x, y) <span class="keyword">in</span> results)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">total_cost</span>(<span class="params">self, data, lmbda, convert=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Return the total cost for the data set ``data``.  The flag</span></span><br><span class="line"><span class="string">        ``convert`` should be set to False if the data set is the</span></span><br><span class="line"><span class="string">        training data (the usual case), and to True if the data set is</span></span><br><span class="line"><span class="string">        the validation or test data.  See comments on the similar (but</span></span><br><span class="line"><span class="string">        reversed) convention for the ``accuracy`` method, above.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        cost = <span class="number">0.0</span></span><br><span class="line">        n = <span class="built_in">len</span>(data)</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> data:</span><br><span class="line">            a = <span class="variable language_">self</span>.feedforward(x)</span><br><span class="line">            <span class="keyword">if</span> convert: <span class="comment"># val/test data</span></span><br><span class="line">                y = vectorized_result(y)</span><br><span class="line">            cost += <span class="variable language_">self</span>.cost.fn(a, y)/n</span><br><span class="line">        <span class="comment"># L2 term = lmbda/(2n)*sum(w**2)</span></span><br><span class="line">        l2_term = <span class="number">0.5</span>*(lmbda/n)*<span class="built_in">sum</span>(np.linalg.norm(w)**<span class="number">2</span> <span class="keyword">for</span> w <span class="keyword">in</span> <span class="variable language_">self</span>.weights)</span><br><span class="line">        cost += l2_term</span><br><span class="line">        <span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">save</span>(<span class="params">self, filename</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Save the neural network to the file ``filename``.&quot;&quot;&quot;</span></span><br><span class="line">        data = &#123;<span class="string">&quot;sizes&quot;</span>: <span class="variable language_">self</span>.sizes,</span><br><span class="line">                <span class="string">&quot;weights&quot;</span>: [w.tolist() <span class="keyword">for</span> w <span class="keyword">in</span> <span class="variable language_">self</span>.weights],</span><br><span class="line">                <span class="string">&quot;biases&quot;</span>: [b.tolist() <span class="keyword">for</span> b <span class="keyword">in</span> <span class="variable language_">self</span>.biases],</span><br><span class="line">                <span class="string">&quot;cost&quot;</span>: <span class="built_in">str</span>(<span class="variable language_">self</span>.cost.__name__)&#125;</span><br><span class="line">        f = <span class="built_in">open</span>(filename, <span class="string">&quot;w&quot;</span>)</span><br><span class="line">        json.dump(data, f)</span><br><span class="line">        f.close()</span><br><span class="line"></span><br><span class="line"><span class="comment">#### Loading a Network</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load</span>(<span class="params">filename</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Load a neural network from the file ``filename``.  Returns an</span></span><br><span class="line"><span class="string">    instance of Network.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    f = <span class="built_in">open</span>(filename, <span class="string">&quot;r&quot;</span>)</span><br><span class="line">    data = json.load(f)</span><br><span class="line">    f.close()</span><br><span class="line">    name = sys.modules[__name__]</span><br><span class="line">    cost = <span class="built_in">getattr</span>(name, data[<span class="string">&quot;cost&quot;</span>])</span><br><span class="line">    net = Network(data[<span class="string">&quot;sizes&quot;</span>], cost=cost)</span><br><span class="line">    net.weights = [np.array(w) <span class="keyword">for</span> w <span class="keyword">in</span> data[<span class="string">&quot;weights&quot;</span>]]</span><br><span class="line">    net.biases = [np.array(b) <span class="keyword">for</span> b <span class="keyword">in</span> data[<span class="string">&quot;biases&quot;</span>]]</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line"><span class="comment">#### Miscellaneous functions</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vectorized_result</span>(<span class="params">j</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Return a 10-dimensional unit vector with a 1.0 in the j&#x27;th position</span></span><br><span class="line"><span class="string">    and zeroes elsewhere.  This is used to convert a digit (0...9)</span></span><br><span class="line"><span class="string">    into a corresponding desired output from the neural network.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    e = np.zeros((<span class="number">10</span>, <span class="number">1</span>))</span><br><span class="line">    e[j] = <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">return</span> e</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;The sigmoid function.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1.0</span>+np.exp(-z))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid_prime</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Derivative of the sigmoid function.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> sigmoid(z)*(<span class="number">1</span>-sigmoid(z))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="comment">#e^z/ sum(e^z)</span></span><br><span class="line">    ez = np.exp(z)</span><br><span class="line">    sum_ez = <span class="built_in">sum</span>(ez)</span><br><span class="line">    <span class="keyword">return</span> ez/sum_ez</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mnist_loader</span><br><span class="line">training_data, validation_data, test_data = mnist_loader.load_data_wrapper()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = load(<span class="string">&quot;./1.json&quot;</span>)</span><br><span class="line">net.accuracy(validation_data)</span><br></pre></td></tr></table></figure>




<pre><code>9385
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = load(<span class="string">&quot;./0.json&quot;</span>)</span><br><span class="line">net.accuracy(validation_data)</span><br></pre></td></tr></table></figure>




<pre><code>8904
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><h2 id="History"><a href="#History" class="headerlink" title="History"></a>History</h2><ul>
<li>20180807: created.</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kezunlin.me/blog/network1-py/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="kezunlin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kezunlin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/network1-py/" class="post-title-link" itemprop="url">network1.py</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-08-07 10:29:00" itemprop="dateCreated datePublished" datetime="2018-08-07T10:29:00+08:00">2018-08-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-10-14 13:39:28" itemprop="dateModified" datetime="2024-10-14T13:39:28+08:00">2024-10-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="network1-py"><a href="#network1-py" class="headerlink" title="network1.py"></a>network1.py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">network.py</span></span><br><span class="line"><span class="string">~~~~~~~~~~</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">A module to implement the stochastic gradient descent learning</span></span><br><span class="line"><span class="string">algorithm for a feedforward neural network.  Gradients are calculated</span></span><br><span class="line"><span class="string">using backpropagation.  Note that I have focused on making the code</span></span><br><span class="line"><span class="string">simple, easily readable, and easily modifiable.  It is not optimized,</span></span><br><span class="line"><span class="string">and omits many desirable features.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#### Libraries</span></span><br><span class="line"><span class="comment"># Standard library</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># Third-party libraries</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Network</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, sizes</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;The list ``sizes`` contains the number of neurons in the</span></span><br><span class="line"><span class="string">        respective layers of the network.  For example, if the list</span></span><br><span class="line"><span class="string">        was [2, 3, 1] then it would be a three-layer network, with the</span></span><br><span class="line"><span class="string">        first layer containing 2 neurons, the second layer 3 neurons,</span></span><br><span class="line"><span class="string">        and the third layer 1 neuron.  The biases and weights for the</span></span><br><span class="line"><span class="string">        network are initialized randomly, using a Gaussian</span></span><br><span class="line"><span class="string">        distribution with mean 0, and variance 1.  Note that the first</span></span><br><span class="line"><span class="string">        layer is assumed to be an input layer, and by convention we</span></span><br><span class="line"><span class="string">        won&#x27;t set any biases for those neurons, since biases are only</span></span><br><span class="line"><span class="string">        ever used in computing the outputs from later layers.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.num_layers = <span class="built_in">len</span>(sizes)</span><br><span class="line">        <span class="variable language_">self</span>.sizes = sizes</span><br><span class="line">        <span class="variable language_">self</span>.biases = [np.random.randn(y, <span class="number">1</span>) <span class="keyword">for</span> y <span class="keyword">in</span> sizes[<span class="number">1</span>:]]</span><br><span class="line">        <span class="variable language_">self</span>.weights = [np.random.randn(y, x)</span><br><span class="line">                        <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(sizes[:-<span class="number">1</span>], sizes[<span class="number">1</span>:])]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">feedforward</span>(<span class="params">self, a</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Return the output of the network if ``a`` is input.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="variable language_">self</span>.biases, <span class="variable language_">self</span>.weights):</span><br><span class="line">            a = sigmoid(np.dot(w, a)+b)</span><br><span class="line">        <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">SGD</span>(<span class="params">self, training_data, epochs, mini_batch_size, eta,</span></span><br><span class="line"><span class="params">            test_data=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Train the neural network using mini-batch stochastic</span></span><br><span class="line"><span class="string">        gradient descent.  The ``training_data`` is a list of tuples</span></span><br><span class="line"><span class="string">        ``(x, y)`` representing the training inputs and the desired</span></span><br><span class="line"><span class="string">        outputs.  The other non-optional parameters are</span></span><br><span class="line"><span class="string">        self-explanatory.  If ``test_data`` is provided then the</span></span><br><span class="line"><span class="string">        network will be evaluated against the test data after each</span></span><br><span class="line"><span class="string">        epoch, and partial progress printed out.  This is useful for</span></span><br><span class="line"><span class="string">        tracking progress, but slows things down substantially.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> test_data: n_test = <span class="built_in">len</span>(test_data)</span><br><span class="line">        n = <span class="built_in">len</span>(training_data)</span><br><span class="line">        num_batches = n/mini_batch_size</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> xrange(epochs):</span><br><span class="line">            random.shuffle(training_data)</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> xrange(<span class="number">0</span>,num_batches):</span><br><span class="line">                mini_batch = training_data[k*mini_batch_size : (k+<span class="number">1</span>)*mini_batch_size]</span><br><span class="line">                <span class="variable language_">self</span>.update_mini_batch(mini_batch, eta)</span><br><span class="line">            <span class="keyword">if</span> test_data:</span><br><span class="line">                <span class="built_in">print</span> <span class="string">&quot;Epoch &#123;0&#125;: &#123;1&#125; / &#123;2&#125;&quot;</span>.<span class="built_in">format</span>(j, <span class="variable language_">self</span>.evaluate(test_data), n_test)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="built_in">print</span> <span class="string">&quot;Epoch &#123;0&#125; complete&quot;</span>.<span class="built_in">format</span>(j)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">calculate_sum_derivatives_of_mini_batch</span>(<span class="params">self,mini_batch</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        计算m个样本的总梯度和。</span></span><br><span class="line"><span class="string">        利用反向传播计算每一个样本(x,y)对应的梯度。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> <span class="variable language_">self</span>.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> <span class="variable language_">self</span>.weights]</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> mini_batch:</span><br><span class="line">            <span class="comment"># 给定一个样本X,利用反向传播算法计算对应w,b的梯度</span></span><br><span class="line">            delta_nabla_b, delta_nabla_w = <span class="variable language_">self</span>.backprop(x, y)</span><br><span class="line">            <span class="comment"># 对m个样本的梯度进行累计求和</span></span><br><span class="line">            nabla_b = [nb+dnb <span class="keyword">for</span> nb, dnb <span class="keyword">in</span> <span class="built_in">zip</span>(nabla_b, delta_nabla_b)]</span><br><span class="line">            nabla_w = [nw+dnw <span class="keyword">for</span> nw, dnw <span class="keyword">in</span> <span class="built_in">zip</span>(nabla_w, delta_nabla_w)]</span><br><span class="line">        <span class="keyword">return</span> nabla_b,nabla_w</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update_mini_batch</span>(<span class="params">self, mini_batch, eta</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Update the network&#x27;s weights and biases by applying</span></span><br><span class="line"><span class="string">        gradient descent using backpropagation to a single mini batch.</span></span><br><span class="line"><span class="string">        The &quot;mini_batch&quot; is a list of tuples &quot;(x, y)&quot;, and &quot;eta&quot;</span></span><br><span class="line"><span class="string">        is the learning rate.&quot;&quot;&quot;</span></span><br><span class="line">        m = <span class="built_in">len</span>(mini_batch)</span><br><span class="line">        nabla_b,nabla_w = <span class="variable language_">self</span>.calculate_sum_derivatives_of_mini_batch(mini_batch)</span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.weights = [w-(eta/m)*nw <span class="keyword">for</span> w, nw <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="variable language_">self</span>.weights, nabla_w)]</span><br><span class="line">        <span class="variable language_">self</span>.biases =  [b-(eta/m)*nb <span class="keyword">for</span> b, nb <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="variable language_">self</span>.biases,  nabla_b)]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backprop</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Return a tuple &quot;(nabla_b, nabla_w)&quot; representing the</span></span><br><span class="line"><span class="string">        gradient for the cost function C_x.  &quot;nabla_b&quot; and</span></span><br><span class="line"><span class="string">        &quot;nabla_w&quot; are layer-by-layer lists of numpy arrays, similar</span></span><br><span class="line"><span class="string">        to &quot;self.biases&quot; and &quot;self.weights&quot;.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 初始化nb,nw,结构和b,w一样</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> <span class="variable language_">self</span>.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> <span class="variable language_">self</span>.weights]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># feedforward</span></span><br><span class="line">        <span class="comment"># 执行算法的feedforward阶段</span></span><br><span class="line">        <span class="comment">#　(1)初始化x作为a_1</span></span><br><span class="line">        activation = x</span><br><span class="line">        activations = [x] <span class="comment"># list to store all the activations, layer by layer</span></span><br><span class="line">        zs = [] <span class="comment"># list to store all the z vectors, layer by layer</span></span><br><span class="line">        <span class="comment"># (2)l=2,....L层，分别计算z_l,a_l并且保存下来。</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="variable language_">self</span>.biases, <span class="variable language_">self</span>.weights):</span><br><span class="line">            z = np.dot(w, activation)+b</span><br><span class="line">            zs.append(z)</span><br><span class="line">            activation = sigmoid(z)</span><br><span class="line">            activations.append(activation)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#========================================================================</span></span><br><span class="line">        <span class="comment"># 先计算所有的误差delta，最后计算所有层的梯度nb,nw，代码可读性更高一些</span></span><br><span class="line">        <span class="comment">#========================================================================</span></span><br><span class="line">        <span class="comment"># method2</span></span><br><span class="line">        <span class="comment"># backward pass</span></span><br><span class="line">        <span class="comment"># 执行算法的backward阶段</span></span><br><span class="line">        <span class="comment"># (3)初始化第L层的误差,delta_L　= cost(a_L,y) * sigmoid_prime(z_L)</span></span><br><span class="line">        l = -<span class="number">1</span></span><br><span class="line">        delta = <span class="variable language_">self</span>.cost_derivative_of_a_L(activations[l], y) * sigmoid_prime(zs[l])</span><br><span class="line">        deltas = [delta] <span class="comment"># list to store all the errors,layer by layer</span></span><br><span class="line">        <span class="comment"># (4)初始化l=L-1,....2层的误差,delta_l = np.dot(w_l+1^T,delta_l+1)* sigmoid_prime(z_l)</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">2</span>, <span class="variable language_">self</span>.num_layers):</span><br><span class="line">            l = -i <span class="comment">#(-２代表L-1,-3代表L-2,-(L-1)代表2)</span></span><br><span class="line">            delta = np.dot(<span class="variable language_">self</span>.weights[l+<span class="number">1</span>].transpose(), deltas[l+<span class="number">1</span>]) * sigmoid_prime(zs[l])</span><br><span class="line">            deltas.insert(<span class="number">0</span>,delta) <span class="comment"># 确保误差的顺序，从后往前计算，所以需要insert在数组的最前面</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#(5)l=L,L-1,....2层，计算所有的梯度向量nb,nw</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">1</span>, <span class="variable language_">self</span>.num_layers):</span><br><span class="line">            l = -i <span class="comment">#(-1,-2,....-(L-1))</span></span><br><span class="line">            nabla_b[l] = deltas[l]</span><br><span class="line">            nabla_w[l] = np.dot(deltas[l], activations[l-<span class="number">1</span>].transpose())</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> (nabla_b, nabla_w)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">self, test_data</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Return the number of test inputs for which the neural</span></span><br><span class="line"><span class="string">        network outputs the correct result. Note that the neural</span></span><br><span class="line"><span class="string">        network&#x27;s output is assumed to be the index of whichever</span></span><br><span class="line"><span class="string">        neuron in the final layer has the highest activation.&quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        l = [0,1,0,0,0,0,0,0,0,0]</span></span><br><span class="line"><span class="string">        a = np.array(l).reshape(10,1)</span></span><br><span class="line"><span class="string">        np.argmax(a) #输出向量对应的数字１</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        test_results = [(1,1),(2,2),(3,3),(1,9)]</span></span><br><span class="line"><span class="string">        [int(x == y) for (x, y) in test_results] </span></span><br><span class="line"><span class="string">        #[1, 1, 1, 0]</span></span><br><span class="line"><span class="string">        sum([int(x == y) for (x, y) in test_results])</span></span><br><span class="line"><span class="string">        #3</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        test_results = [(np.argmax(<span class="variable language_">self</span>.feedforward(x)), y)</span><br><span class="line">                        <span class="keyword">for</span> (x, y) <span class="keyword">in</span> test_data]</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span>(<span class="built_in">int</span>(x == y) <span class="keyword">for</span> (x, y) <span class="keyword">in</span> test_results)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">cost_derivative_of_a_L</span>(<span class="params">self, output_activations, y</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Return the vector of partial derivatives \partial C_x /</span></span><br><span class="line"><span class="string">        \partial a for the output activations.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> (output_activations-y)</span><br><span class="line"></span><br><span class="line"><span class="comment">#### Miscellaneous functions</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;The sigmoid function.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1.0</span>+np.exp(-z))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid_prime</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Derivative of the sigmoid function.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> sigmoid(z)*(<span class="number">1</span>-sigmoid(z))</span><br></pre></td></tr></table></figure>

<h1 id="mnist-loader"><a href="#mnist-loader" class="headerlink" title="mnist_loader"></a>mnist_loader</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">mnist_loader</span></span><br><span class="line"><span class="string">~~~~~~~~~~~~</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">A library to load the MNIST image data.  For details of the data</span></span><br><span class="line"><span class="string">structures that are returned, see the doc strings for ``load_data``</span></span><br><span class="line"><span class="string">and ``load_data_wrapper``.  In practice, ``load_data_wrapper`` is the</span></span><br><span class="line"><span class="string">function usually called by our neural network code.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#### Libraries</span></span><br><span class="line"><span class="comment"># Standard library</span></span><br><span class="line"><span class="keyword">import</span> cPickle</span><br><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"></span><br><span class="line"><span class="comment"># Third-party libraries</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Return the MNIST data as a tuple containing the training data,</span></span><br><span class="line"><span class="string">    the validation data, and the test data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The ``training_data`` is returned as a tuple with two entries.</span></span><br><span class="line"><span class="string">    The first entry contains the actual training images.  This is a</span></span><br><span class="line"><span class="string">    numpy ndarray with 50,000 entries.  Each entry is, in turn, a</span></span><br><span class="line"><span class="string">    numpy ndarray with 784 values, representing the 28 * 28 = 784</span></span><br><span class="line"><span class="string">    pixels in a single MNIST image.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The second entry in the ``training_data`` tuple is a numpy ndarray</span></span><br><span class="line"><span class="string">    containing 50,000 entries.  Those entries are just the digit</span></span><br><span class="line"><span class="string">    values (0...9) for the corresponding images contained in the first</span></span><br><span class="line"><span class="string">    entry of the tuple.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The ``validation_data`` and ``test_data`` are similar, except</span></span><br><span class="line"><span class="string">    each contains only 10,000 images.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    This is a nice data format, but for use in neural networks it&#x27;s</span></span><br><span class="line"><span class="string">    helpful to modify the format of the ``training_data`` a little.</span></span><br><span class="line"><span class="string">    That&#x27;s done in the wrapper function ``load_data_wrapper()``, see</span></span><br><span class="line"><span class="string">    below.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    f = gzip.<span class="built_in">open</span>(<span class="string">&#x27;../data/mnist.pkl.gz&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>)</span><br><span class="line">    training_data, validation_data, test_data = cPickle.load(f)</span><br><span class="line">    f.close()</span><br><span class="line">    <span class="keyword">return</span> (training_data, validation_data, test_data)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_wrapper</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Return a tuple containing ``(training_data, validation_data,</span></span><br><span class="line"><span class="string">    test_data)``. Based on ``load_data``, but the format is more</span></span><br><span class="line"><span class="string">    convenient for use in our implementation of neural networks.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    In particular, ``training_data`` is a list containing 50,000</span></span><br><span class="line"><span class="string">    2-tuples ``(x, y)``.  ``x`` is a 784-dimensional numpy.ndarray</span></span><br><span class="line"><span class="string">    containing the input image.  ``y`` is a 10-dimensional</span></span><br><span class="line"><span class="string">    numpy.ndarray representing the unit vector corresponding to the</span></span><br><span class="line"><span class="string">    correct digit for ``x``.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ``validation_data`` and ``test_data`` are lists containing 10,000</span></span><br><span class="line"><span class="string">    2-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensional</span></span><br><span class="line"><span class="string">    numpy.ndarry containing the input image, and ``y`` is the</span></span><br><span class="line"><span class="string">    corresponding classification, i.e., the digit values (integers)</span></span><br><span class="line"><span class="string">    corresponding to ``x``.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Obviously, this means we&#x27;re using slightly different formats for</span></span><br><span class="line"><span class="string">    the training data and the validation / test data.  These formats</span></span><br><span class="line"><span class="string">    turn out to be the most convenient for use in our neural network</span></span><br><span class="line"><span class="string">    code.&quot;&quot;&quot;</span></span><br><span class="line">    tr_d, va_d, te_d = load_data()</span><br><span class="line">    <span class="comment"># train</span></span><br><span class="line">    training_inputs = [np.reshape(x, (<span class="number">784</span>, <span class="number">1</span>)) <span class="keyword">for</span> x <span class="keyword">in</span> tr_d[<span class="number">0</span>]]</span><br><span class="line">    <span class="comment"># vector train_y, while val and test y are integers.</span></span><br><span class="line">    training_results = [vectorized_result(y) <span class="keyword">for</span> y <span class="keyword">in</span> tr_d[<span class="number">1</span>]] </span><br><span class="line">    training_data = <span class="built_in">zip</span>(training_inputs, training_results)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># val</span></span><br><span class="line">    validation_inputs = [np.reshape(x, (<span class="number">784</span>, <span class="number">1</span>)) <span class="keyword">for</span> x <span class="keyword">in</span> va_d[<span class="number">0</span>]]</span><br><span class="line">    validation_data = <span class="built_in">zip</span>(validation_inputs, va_d[<span class="number">1</span>])</span><br><span class="line">    <span class="comment"># test</span></span><br><span class="line">    test_inputs = [np.reshape(x, (<span class="number">784</span>, <span class="number">1</span>)) <span class="keyword">for</span> x <span class="keyword">in</span> te_d[<span class="number">0</span>]]</span><br><span class="line">    test_data = <span class="built_in">zip</span>(test_inputs, te_d[<span class="number">1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> (training_data, validation_data, test_data)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vectorized_result</span>(<span class="params">j</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Return a 10-dimensional unit vector with a 1.0 in the jth</span></span><br><span class="line"><span class="string">    position and zeroes elsewhere.  This is used to convert a digit</span></span><br><span class="line"><span class="string">    (0...9) into a corresponding desired output from the neural</span></span><br><span class="line"><span class="string">    network.&quot;&quot;&quot;</span></span><br><span class="line">    e = np.zeros((<span class="number">10</span>, <span class="number">1</span>))</span><br><span class="line">    e[j] = <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">return</span> e</span><br></pre></td></tr></table></figure>

<h1 id="test"><a href="#test" class="headerlink" title="test"></a>test</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> mnist_loader</span><br><span class="line"><span class="keyword">from</span> ke_network <span class="keyword">import</span> Network</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>():</span><br><span class="line">    train,val,test = mnist_loader.load_data_wrapper()</span><br><span class="line">    epoch = <span class="number">30</span></span><br><span class="line">    mini_batch_size  = <span class="number">10</span></span><br><span class="line">    eta = <span class="number">3.0</span></span><br><span class="line">    net = Network([<span class="number">784</span>,<span class="number">30</span>,<span class="number">10</span>])</span><br><span class="line">    net.SGD(train,epoch,mini_batch_size,eta,test_data=test)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    test()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>

<pre><code>Epoch 0: 9135 / 10000
Epoch 1: 9238 / 10000
Epoch 2: 9337 / 10000
Epoch 3: 9345 / 10000
Epoch 4: 9393 / 10000
Epoch 5: 9389 / 10000
Epoch 6: 9419 / 10000
Epoch 7: 9410 / 10000
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><h2 id="History"><a href="#History" class="headerlink" title="History"></a>History</h2><ul>
<li>20180807: created.</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kezunlin.me/blog/numpy-random-shuffle-and-theano-shuffle/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="kezunlin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kezunlin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/numpy-random-shuffle-and-theano-shuffle/" class="post-title-link" itemprop="url">python numpy random shuffle and theano shuffle</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-08-07 10:14:00" itemprop="dateCreated datePublished" datetime="2018-08-07T10:14:00+08:00">2018-08-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-10-14 13:39:28" itemprop="dateModified" datetime="2024-10-14T13:39:28+08:00">2024-10-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/python/" itemprop="url" rel="index"><span itemprop="name">python</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Examples-of-shuffle"><a href="#Examples-of-shuffle" class="headerlink" title="Examples of shuffle"></a>Examples of shuffle</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">numpy.random.shuffle(x)</span></span><br><span class="line"><span class="string">Modify a sequence in-place by shuffling its contents.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">random.shuffle(list)只能对list进行随机打乱。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">x : array_like</span></span><br><span class="line"><span class="string">    The array or list to be shuffled.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns: None</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">This function only shuffles the array along the first index of a multi-dimensional array</span></span><br><span class="line"><span class="string">（多维矩阵中，只对第一维（行）做打乱顺序操作）</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">arr = np.arange(<span class="number">10</span>)</span><br><span class="line">arr</span><br></pre></td></tr></table></figure>




<pre><code>array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">np.random.shuffle(arr)</span><br><span class="line">arr</span><br></pre></td></tr></table></figure>




<pre><code>array([1, 4, 7, 3, 0, 9, 5, 8, 2, 6])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">arr = np.arange(<span class="number">9</span>).reshape((<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">arr</span><br></pre></td></tr></table></figure>




<pre><code>array([[0, 1, 2],
       [3, 4, 5],
       [6, 7, 8]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对于矩阵，按照row进行打乱。</span></span><br><span class="line">np.random.shuffle(arr)</span><br><span class="line">arr</span><br></pre></td></tr></table></figure>




<pre><code>array([[3, 4, 5],
       [0, 1, 2],
       [6, 7, 8]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对于矩阵，按照row进行打乱。</span></span><br><span class="line">np.random.shuffle(arr)</span><br><span class="line">arr</span><br></pre></td></tr></table></figure>




<pre><code>array([[3, 4, 5],
       [6, 7, 8],
       [0, 1, 2]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># random.shuffle(list)</span></span><br><span class="line">l = [i <span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">0</span>,<span class="number">10</span>)]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line">random.shuffle(l)</span><br><span class="line">l</span><br></pre></td></tr></table></figure>




<pre><code>[2, 9, 5, 8, 6, 3, 7, 1, 4, 0]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">random.shuffle(l)</span><br><span class="line">l</span><br></pre></td></tr></table></figure>




<pre><code>[3, 9, 2, 1, 5, 7, 8, 4, 0, 6]
</code></pre>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- kzl in-article ad -->
<p><ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-5653382914441020"
     data-ad-slot="7925631830"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>




<h2 id="Shuffe-two-sequences-at-the-same-time"><a href="#Shuffe-two-sequences-at-the-same-time" class="headerlink" title="Shuffe two sequences at the same time"></a>Shuffe two sequences at the same time</h2><p>shuffle的状态依赖于random.seed,np.random.seed，可以使用time作为seed</p>
<p>random.seed(time.time())</p>
<h3 id="2次shuffle的seed不一样，结果不一致"><a href="#2次shuffle的seed不一样，结果不一致" class="headerlink" title="2次shuffle的seed不一样，结果不一致"></a>2次shuffle的seed不一样，结果不一致</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">train_data = (np.array([[<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">                       [<span class="number">2</span>,<span class="number">2</span>],</span><br><span class="line">                       [<span class="number">3</span>,<span class="number">3</span>],</span><br><span class="line">                       [<span class="number">4</span>,<span class="number">4</span>]]), np.array([<span class="number">11</span>,<span class="number">22</span>,<span class="number">33</span>,<span class="number">44</span>]))</span><br><span class="line">x = train_data[<span class="number">0</span>]</span><br><span class="line">y = train_data[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># shuffle x,y的时候，只要保证seed相同，那么shuffle之后的x,y对应元素的顺序保持一样</span></span><br><span class="line"><span class="comment">#np.random.seed(1)</span></span><br><span class="line">np.random.shuffle(x)</span><br><span class="line"><span class="built_in">print</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment">#np.random.seed(1)</span></span><br><span class="line">np.random.shuffle(y)</span><br><span class="line"><span class="built_in">print</span> y</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span></span><br><span class="line"><span class="built_in">print</span> train_data</span><br></pre></td></tr></table></figure>

<pre><code>[[3 3]
 [1 1]
 [2 2]
 [4 4]]
[22 11 33 44]

(array([[3, 3],
       [1, 1],
       [2, 2],
       [4, 4]]), array([22, 11, 33, 44]))
</code></pre>
<h3 id="2次shuffle的seed一样，结果保持一致"><a href="#2次shuffle的seed一样，结果保持一致" class="headerlink" title="2次shuffle的seed一样，结果保持一致"></a>2次shuffle的seed一样，结果保持一致</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">train_data = (np.array([[<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">                       [<span class="number">2</span>,<span class="number">2</span>],</span><br><span class="line">                       [<span class="number">3</span>,<span class="number">3</span>],</span><br><span class="line">                       [<span class="number">4</span>,<span class="number">4</span>]]), np.array([<span class="number">11</span>,<span class="number">22</span>,<span class="number">33</span>,<span class="number">44</span>]))</span><br><span class="line">x = train_data[<span class="number">0</span>]</span><br><span class="line">y = train_data[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># shuffle x,y的时候，只要保证seed相同，那么shuffle之后的x,y对应元素的顺序保持一样</span></span><br><span class="line">seed = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">np.random.seed(seed)</span><br><span class="line">np.random.shuffle(x)</span><br><span class="line"><span class="built_in">print</span> x</span><br><span class="line"></span><br><span class="line">np.random.seed(seed)</span><br><span class="line">np.random.shuffle(y)</span><br><span class="line"><span class="built_in">print</span> y</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span></span><br><span class="line"><span class="built_in">print</span> train_data</span><br></pre></td></tr></table></figure>

<pre><code>[[4 4]
 [3 3]
 [1 1]
 [2 2]]
[44 33 11 22]

(array([[4, 4],
       [3, 3],
       [1, 1],
       [2, 2]]), array([44, 33, 11, 22]))
</code></pre>
<h3 id="shuffle-data-x-y"><a href="#shuffle-data-x-y" class="headerlink" title="shuffle_data(x,y)"></a>shuffle_data(x,y)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">train_data = (np.array([[<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">                       [<span class="number">2</span>,<span class="number">2</span>],</span><br><span class="line">                       [<span class="number">3</span>,<span class="number">3</span>],</span><br><span class="line">                       [<span class="number">4</span>,<span class="number">4</span>]]), np.array([<span class="number">11</span>,<span class="number">22</span>,<span class="number">33</span>,<span class="number">44</span>]))</span><br><span class="line">x = train_data[<span class="number">0</span>]</span><br><span class="line">y = train_data[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">shuffle_data</span>(<span class="params">x,y</span>):</span><br><span class="line">    <span class="comment">#seed = int(time.time()) </span></span><br><span class="line">    </span><br><span class="line">    seed = <span class="number">1</span></span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    np.random.shuffle(x)</span><br><span class="line"></span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    np.random.shuffle(y)</span><br><span class="line"></span><br><span class="line">shuffle_data(x,y)</span><br><span class="line"><span class="built_in">print</span> x</span><br><span class="line"><span class="built_in">print</span> y</span><br><span class="line"><span class="built_in">print</span> </span><br><span class="line"><span class="built_in">print</span> train_data</span><br></pre></td></tr></table></figure>

<pre><code>[[4 4]
 [3 3]
 [1 1]
 [2 2]]
[44 33 11 22]

(array([[4, 4],
       [3, 3],
       [1, 1],
       [2, 2]]), array([44, 33, 11, 22]))
</code></pre>
<h2 id="Shuffle-in-theano-with-TensorVariable"><a href="#Shuffle-in-theano-with-TensorVariable" class="headerlink" title="Shuffle in theano with TensorVariable"></a>Shuffle in theano with TensorVariable</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> theano</span><br><span class="line"><span class="keyword">import</span> theano.tensor <span class="keyword">as</span> T</span><br><span class="line"></span><br><span class="line">train_data = (np.array([[<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">                       [<span class="number">2</span>,<span class="number">2</span>],</span><br><span class="line">                       [<span class="number">3</span>,<span class="number">3</span>],</span><br><span class="line">                       [<span class="number">4</span>,<span class="number">4</span>]],dtype=<span class="string">&#x27;float64&#x27;</span>), np.array([<span class="number">11</span>,<span class="number">22</span>,<span class="number">33</span>,<span class="number">44</span>],dtype=<span class="string">&#x27;int32&#x27;</span>))</span><br><span class="line">x = train_data[<span class="number">0</span>]</span><br><span class="line">y = train_data[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">shuffle_data</span>(<span class="params">x,y</span>):</span><br><span class="line">    <span class="comment">#seed = int(time.time()) </span></span><br><span class="line">    </span><br><span class="line">    seed = <span class="number">1</span></span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    np.random.shuffle(x)</span><br><span class="line"></span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    np.random.shuffle(y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># OK because of shared&lt;----&gt;train data</span></span><br><span class="line">shared_x = theano.shared(train_data[<span class="number">0</span>],  borrow=<span class="literal">True</span>) </span><br><span class="line">shared_y = theano.shared(train_data[<span class="number">1</span>],  borrow=<span class="literal">True</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment">#shared_x = theano.shared(np.asarray(train_data[0], dtype=theano.config.floatX), borrow=True)  # no copy</span></span><br><span class="line"><span class="comment">#shared_y = theano.shared(np.asarray(train_data[1], dtype=theano.config.floatX), borrow=True)  # no copy</span></span><br><span class="line"></span><br><span class="line">y_cast = T.cast(shared_y,<span class="string">&quot;int32&quot;</span>) </span><br><span class="line"><span class="comment"># shared_y dtype int32, no copy, y_cast is  TensorSharedVariable(int32,vector)</span></span><br><span class="line"><span class="comment"># shared_y dtype float64, copy,  y_cast is  TensorVariable(int32,vector)</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> shared_x.<span class="built_in">type</span>,shared_y.<span class="built_in">type</span>,y_cast.<span class="built_in">type</span></span><br><span class="line"><span class="built_in">print</span> <span class="built_in">type</span>(shared_x),<span class="built_in">type</span>(shared_y),<span class="built_in">type</span>(y_cast)</span><br><span class="line"><span class="built_in">print</span> shared_y <span class="keyword">is</span> y_cast</span><br><span class="line">    </span><br><span class="line"><span class="built_in">print</span> <span class="string">&#x27;old train&#x27;</span></span><br><span class="line"><span class="built_in">print</span> train_data</span><br><span class="line"></span><br><span class="line"><span class="comment">#print &#x27;\nupdate train&#x27;</span></span><br><span class="line"><span class="comment">#x[0] = 100</span></span><br><span class="line"><span class="comment">#y[0] = 100</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> <span class="string">&#x27;\nshuffle train&#x27;</span></span><br><span class="line"><span class="comment">#shuffle_data(train_data[0],train_data[1])</span></span><br><span class="line"></span><br><span class="line">originX = shared_x.get_value(borrow=<span class="literal">True</span>)</span><br><span class="line">originY = shared_y.get_value(borrow=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span> originX <span class="keyword">is</span> x <span class="comment"># true </span></span><br><span class="line"><span class="built_in">print</span> originY <span class="keyword">is</span> y <span class="comment"># true</span></span><br><span class="line">shuffle_data(originX,originY)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> train_data</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> <span class="string">&#x27;\nshared x and y&#x27;</span></span><br><span class="line"><span class="built_in">print</span> shared_x.get_value() </span><br><span class="line"><span class="built_in">print</span> shared_y.get_value()</span><br></pre></td></tr></table></figure>

<pre><code>WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:
 https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29

WARNING (theano.sandbox.cuda): CUDA is installed, but device gpu is not available  (error: Unable to get the number of gpus available: no CUDA-capable device is detected)


TensorType(float64, matrix) TensorType(int32, vector) TensorType(int32, vector)
&lt;class &#39;theano.tensor.sharedvar.TensorSharedVariable&#39;&gt; &lt;class &#39;theano.tensor.sharedvar.TensorSharedVariable&#39;&gt; &lt;class &#39;theano.tensor.sharedvar.TensorSharedVariable&#39;&gt;
True
old train
(array([[ 1.,  1.],
       [ 2.,  2.],
       [ 3.,  3.],
       [ 4.,  4.]]), array([11, 22, 33, 44], dtype=int32))

shuffle train
True
True
(array([[ 4.,  4.],
       [ 3.,  3.],
       [ 1.,  1.],
       [ 2.,  2.]]), array([44, 33, 11, 22], dtype=int32))

shared x and y
[[ 4.  4.]
 [ 3.  3.]
 [ 1.  1.]
 [ 2.  2.]]
[44 33 11 22]
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><h2 id="History"><a href="#History" class="headerlink" title="History"></a>History</h2><ul>
<li>20180807: created.</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kezunlin.me/blog/Example-3-A-Real-Example-Logistic-Regression-with-theano/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="kezunlin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kezunlin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/Example-3-A-Real-Example-Logistic-Regression-with-theano/" class="post-title-link" itemprop="url">Example 3- A Real Example Logistic Regression with theano</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-08-07 09:45:00" itemprop="dateCreated datePublished" datetime="2018-08-07T09:45:00+08:00">2018-08-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-10-14 13:39:28" itemprop="dateModified" datetime="2024-10-14T13:39:28+08:00">2024-10-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Series"><a href="#Series" class="headerlink" title="Series"></a>Series</h2><ul>
<li><a href="https://kezunlin.me/post/3bee8fac/#more">Example 1-theano linear regression with gradient descent</a></li>
<li><a href="https://kezunlin.me/post/8d4707b6/#more">Example 2 - Linear Regression Example with Python and theano from MSDN</a></li>
<li><a href="https://kezunlin.me/post/8ce3f979/#more"><strong>Example 3- A Real Example Logistic Regression with theano</strong></a></li>
</ul>
<h2 id="code-example"><a href="#code-example" class="headerlink" title="code example"></a>code example</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> theano</span><br><span class="line"><span class="keyword">import</span> theano.tensor <span class="keyword">as</span> T</span><br><span class="line">rng = numpy.random</span><br><span class="line"></span><br><span class="line">N = <span class="number">400</span>                                   <span class="comment"># training sample size</span></span><br><span class="line">feats = <span class="number">784</span>                               <span class="comment"># number of input variables</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># generate a dataset: D = (input_values, target_class)</span></span><br><span class="line">D = (rng.randn(N, feats), rng.randint(size=N, low=<span class="number">0</span>, high=<span class="number">2</span>))</span><br><span class="line"><span class="comment"># (400, 784) (400,)</span></span><br><span class="line">training_steps = <span class="number">10000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Declare Theano symbolic variables</span></span><br><span class="line">x = T.dmatrix(<span class="string">&quot;x&quot;</span>) <span class="comment"># (400,784)</span></span><br><span class="line">y = T.dvector(<span class="string">&quot;y&quot;</span>) <span class="comment"># (400,)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># initialize the weight vector w randomly</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># this and the following bias variable b</span></span><br><span class="line"><span class="comment"># are shared so they keep their values</span></span><br><span class="line"><span class="comment"># between training iterations (updates)</span></span><br><span class="line">w = theano.shared(rng.randn(feats), name=<span class="string">&quot;w&quot;</span>) <span class="comment">#vector (784,)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># initialize the bias term</span></span><br><span class="line">b = theano.shared(<span class="number">0.</span>, name=<span class="string">&quot;b&quot;</span>) <span class="comment">#scalar ()</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Initial model:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(w.get_value().shape)</span><br><span class="line"><span class="built_in">print</span>(b.get_value().shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct Theano expression graph</span></span><br><span class="line">p_1 = <span class="number">1</span> / (<span class="number">1</span> + T.exp(-T.dot(x, w) - b))   <span class="comment"># Probability that target = 1        = vector(400,)</span></span><br><span class="line">prediction = p_1 &gt; <span class="number">0.5</span>                    <span class="comment"># The prediction thresholded         = vector(400,) of bool</span></span><br><span class="line">xent = -y * T.log(p_1) - (<span class="number">1</span>-y) * T.log(<span class="number">1</span>-p_1) <span class="comment"># Cross-entropy loss function    = vector(400,) of Cx for given x</span></span><br><span class="line">cost = xent.mean() + <span class="number">0.01</span> * (w ** <span class="number">2</span>).<span class="built_in">sum</span>()<span class="comment"># The cost to minimize               = scalar   f(x,y,w,b)</span></span><br><span class="line">gw, gb = T.grad(cost, [w, b])             <span class="comment"># Compute the gradient of the cost</span></span><br><span class="line">                                          <span class="comment"># w.r.t weight vector w and</span></span><br><span class="line">                                          <span class="comment"># bias term b</span></span><br><span class="line">                                          <span class="comment"># (we shall return to this in a</span></span><br><span class="line">                                          <span class="comment"># following section of this tutorial)</span></span><br><span class="line"></span><br><span class="line">updates = [(w, w - <span class="number">0.1</span> * gw),(b, b - <span class="number">0.1</span> * gb)]                </span><br><span class="line"><span class="comment"># Compile</span></span><br><span class="line">train = theano.function(</span><br><span class="line">          inputs=[x,y],</span><br><span class="line">          outputs=[prediction, xent,cost],</span><br><span class="line">          updates= updates</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">predict = theano.function(inputs=[x], outputs=prediction)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(training_steps):</span><br><span class="line">    pred, xcent,cost_t = train(D[<span class="number">0</span>], D[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">prediction = predict(D[<span class="number">0</span>])  <span class="comment"># vector(400,) of bool</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#a = np.array([1,2,3,4])</span></span><br><span class="line"><span class="comment">#b = np.array([2,2,4,3])</span></span><br><span class="line"><span class="comment">#np.mean( np.equal(a,b) ) #[0,1,0,0] 0.25</span></span><br><span class="line"></span><br><span class="line">accuracy = <span class="number">100</span>* np.mean( np.equal(prediction,D[<span class="number">1</span>]) )</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Final model:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(w.get_value().shape)</span><br><span class="line"><span class="built_in">print</span>(b.get_value().shape)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;\nCost = &quot;</span>,cost_t</span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;Accuracy = &quot;</span>, accuracy,<span class="string">&quot;%&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\ntarget values for D:&quot;</span>)</span><br><span class="line"><span class="comment">#print(D[1])</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;prediction on D:&quot;</span>)</span><br><span class="line"><span class="comment">#print(prediction)</span></span><br></pre></td></tr></table></figure>

<pre><code>Initial model:
(784,)
()
Final model:
(784,)
()

Cost =  0.121553645129
Accuracy =  100.0 %

target values for D:
prediction on D:
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><h2 id="History"><a href="#History" class="headerlink" title="History"></a>History</h2><ul>
<li>20180807: created.</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/12/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/12/">12</a><span class="page-number current">13</span><a class="page-number" href="/page/14/">14</a><span class="space">&hellip;</span><a class="page-number" href="/page/19/">19</a><a class="extend next" rel="next" href="/page/14/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">kezunlin</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">181</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">213</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">kezunlin</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
